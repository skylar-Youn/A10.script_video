"""FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜: AI ì‡¼ì¸  ì œì‘ ì›¹ UI ë° API."""
from __future__ import annotations

import base64
import json
import logging
import re
import shutil
import subprocess
from datetime import datetime
from pathlib import Path
from functools import lru_cache
from typing import Any, Dict, List, Optional, Literal, Tuple
from uuid import uuid4

try:
    import cv2
    CV2_IMPORT_ERROR: Optional[Exception] = None
except ImportError as exc:  # pragma: no cover - optional at runtime
    cv2 = None  # type: ignore[assignment]
    CV2_IMPORT_ERROR = exc

try:
    import torch  # type: ignore[import-untyped]
    TORCH_IMPORT_ERROR: Optional[Exception] = None
except Exception as exc:  # pragma: no cover - optional at runtime
    torch = None  # type: ignore[assignment]
    TORCH_IMPORT_ERROR = exc

from dotenv import load_dotenv
from fastapi import (
    APIRouter,
    Body,
    FastAPI,
    File,
    Form,
    HTTPException,
    Request,
    Response,
    UploadFile,
    status,
)
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, JSONResponse, FileResponse, StreamingResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from starlette.concurrency import run_in_threadpool

from pydantic import BaseModel
from PIL import ImageColor

# ì´ëª¨ì§€ ë Œë”ë§ ëª¨ë“ˆ
from web_app.emoji_renderer import render_emoji_text_to_png

# AI Shorts Maker imports - optional module
try:
    from ai_shorts_maker.generator import GenerationOptions, generate_short
    from ai_shorts_maker.models import (
        ProjectMetadata,
        ProjectSummary,
        ProjectVersionInfo,
        SubtitleCreate,
        SubtitleUpdate,
        TimelineUpdate,
    )
    from ai_shorts_maker.repository import (
        clone_project,
        delete_project as repository_delete_project,
        list_projects,
        load_project,
        metadata_path,
    )
    from ai_shorts_maker.services import (
        add_subtitle,
        delete_subtitle_line,
        list_versions,
        render_project,
        replace_timeline,
        restore_project_version,
        update_audio_settings,
        update_subtitle_style,
        update_subtitle,
    )
    import ai_shorts_maker.translator as translator_module
    from ai_shorts_maker.translator import (
        TranslatorProject,
        TranslatorProjectCreate,
        TranslatorProjectUpdate,
    aggregate_dashboard_projects,
    clone_translator_project,
    create_project as translator_create_project,
    delete_project as translator_delete_project,
    downloads_listing,
    ensure_directories as ensure_translator_directories,
    list_projects as translator_list_projects,
    load_project as translator_load_project,
    update_project as translator_update_project,
    translate_project_segments,
    synthesize_voice_for_project,
    render_translated_project,
    list_translation_versions,
    load_translation_version,
    apply_saved_result_to_project,
    delete_project_segment,
    UPLOADS_DIR,
)
    AI_SHORTS_MAKER_AVAILABLE = True
except ImportError as e:
    AI_SHORTS_MAKER_AVAILABLE = False
    logging.warning(f"AI Shorts Maker module not available: {e}")
    # Define placeholder variables for missing imports
    TranslatorProject = None
    TranslatorProjectCreate = None
    TranslatorProjectUpdate = None

from videoanalysis.config import SAVED_RESULTS_DIR
from youtube.ytdl import download_with_options, parse_sub_langs
try:
    from youtube.ytcompareinspector import compare_two, compare_dir
    SIMILARITY_IMPORT_ERROR: Optional[Exception] = None
except Exception as exc:  # pylint: disable=broad-except
    compare_two = None  # type: ignore[assignment]
    compare_dir = None  # type: ignore[assignment]
    SIMILARITY_IMPORT_ERROR = exc
from youtube.ytcopyrightinspector import run_precheck as run_copyright_precheck

from .utils.text_removal import (
    BackgroundFillConfig,
    DiffusionUnavailableError,
    RemovalConfig,
    TrackerUnavailableError,
    trim_video_clip,
    prepare_video_preview,
    split_media_components,
    run_background_fill,
    run_text_removal,
)


class RenderRequest(BaseModel):
    burn_subs: Optional[bool] = False


class SubtitleStyleRequest(BaseModel):
    font_size: Optional[int] = None
    y_offset: Optional[int] = None
    stroke_width: Optional[int] = None
    font_path: Optional[str] = None
    animation: Optional[str] = None
    template: Optional[str] = None
    banner_primary_text: Optional[str] = None
    banner_secondary_text: Optional[str] = None
    banner_primary_font_size: Optional[int] = None
    banner_secondary_font_size: Optional[int] = None
    banner_line_spacing: Optional[int] = None


class DashboardProject(BaseModel):
    id: str
    title: str
    project_type: Literal["shorts", "translator"]
    status: Literal["draft", "segmenting", "translating", "voice_ready", "voice_complete", "rendering", "rendered", "failed"]
    completed_steps: int = 1
    total_steps: int = 5
    topic: Optional[str] = None
    language: Optional[str] = None
    thumbnail: Optional[str] = None
    updated_at: Optional[str] = None
    source_origin: Optional[str] = None


class TextRemovalBox(BaseModel):
    x: float
    y: float
    width: float
    height: float


class TextRemovalProcessRequest(BaseModel):
    session_id: str
    prompt: str = "clean smooth surface, natural background, seamless texture"
    negative_prompt: str = "text, watermark, caption, letters, words, characters"
    model_id: str = "stabilityai/stable-diffusion-2-inpainting"
    strength: float = 0.9
    guidance_scale: float = 7.5
    num_inference_steps: int = 30
    dilate: int = 10
    device: str = "cuda"
    dtype: Literal["float16", "float32"] = "float16"
    seed: Optional[int] = None
    boxes: List[TextRemovalBox]
    max_frames: Optional[int] = None
    fps: Optional[float] = None


class TextRemovalFillRequest(BaseModel):
    session_id: str
    boxes: List[TextRemovalBox]
    dilate: int = 4
    fps: Optional[float] = None
    max_frames: Optional[int] = None


class TextRemovalSplitRequest(BaseModel):
    session_id: str


class TextRemovalTrimRequest(BaseModel):
    session_id: str
    start: float = 0.0
    duration: float = 4.0


logger = logging.getLogger(__name__)


def open_video_with_transcode_fallback(video_path: Path) -> tuple[Any, Optional[Path]]:
    """
    OpenCVë¡œ ë¹„ë””ì˜¤ë¥¼ ì—´ë˜, AV1 ë“±ìœ¼ë¡œ ì‹¤íŒ¨ ì‹œ H.264ë¡œ íŠ¸ëœìŠ¤ì½”ë”©í•˜ì—¬ ì¬ì‹œë„.

    Returns:
        (cv2.VideoCapture, transcoded_file_path or None)
    """
    import subprocess
    import tempfile

    if cv2 is None:
        raise RuntimeError("OpenCVê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    def _create_capture(path: Path):
        api_preference = getattr(cv2, "CAP_FFMPEG", 0)
        capture = cv2.VideoCapture()

        if hasattr(cv2, "CAP_PROP_HW_ACCELERATION") and hasattr(cv2, "VIDEO_ACCELERATION_NONE"):
            capture.set(cv2.CAP_PROP_HW_ACCELERATION, cv2.VIDEO_ACCELERATION_NONE)

        opened = False
        try:
            opened = capture.open(str(path), api_preference)
        except TypeError:  # pragma: no cover - older OpenCV fallback
            opened = capture.open(str(path))

        if not opened:
            capture.release()
            try:
                capture = cv2.VideoCapture(str(path), api_preference)
            except TypeError:  # pragma: no cover - older OpenCV fallback
                capture = cv2.VideoCapture(str(path))
            if hasattr(cv2, "CAP_PROP_HW_ACCELERATION") and hasattr(cv2, "VIDEO_ACCELERATION_NONE"):
                capture.set(cv2.CAP_PROP_HW_ACCELERATION, cv2.VIDEO_ACCELERATION_NONE)
        return capture

    # ë¨¼ì € ì§ì ‘ ì—´ê¸° ì‹œë„
    cap = _create_capture(video_path)
    if cap.isOpened():
        return cap, None

    # ì‹¤íŒ¨ ì‹œ íŠ¸ëœìŠ¤ì½”ë”©
    logger.info(f"Failed to open video directly, attempting transcode: {video_path}")

    temp_dir = Path(tempfile.gettempdir())
    transcoded_file = temp_dir / f"transcoded_{uuid4().hex}.mp4"

    transcode_cmd = [
        "ffmpeg",
        "-y",
        "-hwaccel", "none",  # í•˜ë“œì›¨ì–´ ê°€ì† ë¹„í™œì„±í™”
        "-i", str(video_path),
        "-c:v", "libx264",
        "-pix_fmt", "yuv420p",
        "-preset", "ultrafast",
        "-vf", "scale=trunc(iw/2)*2:trunc(ih/2)*2",  # í•´ìƒë„ ìœ ì§€ (ì§ìˆ˜ë¡œ ë³´ì •)
        "-avoid_negative_ts", "make_zero",
        str(transcoded_file)
    ]

    proc = subprocess.run(transcode_cmd, capture_output=True, text=True)
    if proc.returncode != 0 or not transcoded_file.exists():
        error_msg = proc.stderr.strip() or proc.stdout.strip() or "FFmpeg transcoding failed"
        if transcoded_file.exists():
            transcoded_file.unlink()
        raise RuntimeError(f"Failed to transcode video: {error_msg}")

    cap = _create_capture(transcoded_file)
    if not cap.isOpened():
        if transcoded_file.exists():
            transcoded_file.unlink()
        raise RuntimeError("Failed to open transcoded video")

    return cap, transcoded_file


BASE_DIR = Path(__file__).resolve().parent
PACKAGE_DIR = BASE_DIR.parent / "ai_shorts_maker"
ASSETS_DIR = PACKAGE_DIR / "assets"
OUTPUT_DIR = PACKAGE_DIR / "outputs"
TEMPLATES_DIR = BASE_DIR / "templates"
STATIC_DIR = BASE_DIR / "static"
YTDL_SETTINGS_PATH = BASE_DIR / "ytdl_settings.json"
YTDL_HISTORY_PATH = BASE_DIR / "ytdl_history.json"
DEFAULT_YTDL_OUTPUT_DIR = (BASE_DIR.parent / "youtube" / "download").resolve()
COPYRIGHT_UPLOAD_DIR = BASE_DIR / "uploads" / "copyright"
TEXT_REMOVAL_UPLOAD_DIR = BASE_DIR / "uploads" / "text_removal"
YTDL_UPLOAD_DIR = BASE_DIR / "uploads" / "ytdl"
COPYRIGHT_REPORT_DIR = BASE_DIR / "reports" / "copyright"
SIMILARITY_SETTINGS_PATH = BASE_DIR / "similarity_settings.json"
COPYRIGHT_SETTINGS_PATH = BASE_DIR / "copyright_settings.json"
DEFAULT_YTDL_SETTINGS: Dict[str, Any] = {
    "output_dir": str(DEFAULT_YTDL_OUTPUT_DIR),
    "sub_langs": "ko",
    "sub_format": "srt/best",
    "download_subs": True,
    "auto_subs": True,
    "dry_run": False,
    "extract_vocal": False,
    "extract_instruments": False,
}

SIMILARITY_DEFAULTS: Dict[str, Any] = {
    "video_a": "",
    "video_b": "",
    "ref_dir": "",
    "fps": "1.0",
    "resize": "320x320",
    "max_frames": "200",
    "phash_th": "12",
    "weight_phash": "0.25",
    "weight_orb": "0.25",
    "weight_hist": "0.25",
    "weight_psnr": "0.25",
    "top_n": "5",
}

TEXT_REMOVAL_UPLOAD_DIR.mkdir(parents=True, exist_ok=True)
YTDL_UPLOAD_DIR.mkdir(parents=True, exist_ok=True)


def load_similarity_settings() -> Dict[str, Any]:
    settings = SIMILARITY_DEFAULTS.copy()
    if SIMILARITY_SETTINGS_PATH.exists():
        try:
            data = json.loads(SIMILARITY_SETTINGS_PATH.read_text(encoding="utf-8"))
            if isinstance(data, dict):
                for key in settings:
                    if key in data:
                        value = data[key]
                        settings[key] = "" if value is None else str(value)
        except (OSError, json.JSONDecodeError) as exc:
            logger.warning("Failed to load similarity settings: %s", exc)
    return settings


def save_similarity_settings(values: Dict[str, Any]) -> None:
    payload: Dict[str, Any] = {}
    for key in SIMILARITY_DEFAULTS:
        if key in values:
            value = values[key]
            payload[key] = "" if value is None else str(value)
    try:
        SIMILARITY_SETTINGS_PATH.write_text(
            json.dumps(payload, ensure_ascii=False, indent=2),
            encoding="utf-8",
        )
    except OSError as exc:
        logger.error("Failed to save similarity settings: %s", exc)
        raise


def default_similarity_form() -> Dict[str, Any]:
    return load_similarity_settings()


COPYRIGHT_DEFAULTS: Dict[str, Any] = {
    "reference_dir": "",
    "fps": "1.0",
    "hash": "phash",
    "resize": "256x256",
    "max_frames": "",
    "hamming_th": "12",
    "high_th": "0.30",
    "med_th": "0.15",
    "audio_only": False,
    "report_dir": "",
}


def load_copyright_settings() -> Dict[str, Any]:
    settings = COPYRIGHT_DEFAULTS.copy()
    if COPYRIGHT_SETTINGS_PATH.exists():
        try:
            data = json.loads(COPYRIGHT_SETTINGS_PATH.read_text(encoding="utf-8"))
            if isinstance(data, dict):
                for key, default_value in settings.items():
                    if key not in data:
                        continue
                    value = data[key]
                    if isinstance(default_value, bool):
                        settings[key] = bool(value)
                    else:
                        settings[key] = "" if value is None else str(value)
        except (OSError, json.JSONDecodeError) as exc:
            logger.warning("Failed to load copyright settings: %s", exc)
    return settings


def save_copyright_settings(values: Dict[str, Any]) -> None:
    payload: Dict[str, Any] = {}
    for key, default_value in COPYRIGHT_DEFAULTS.items():
        if isinstance(default_value, bool):
            raw = values.get(key)
            if isinstance(raw, str):
                payload[key] = raw.lower() in {"1", "true", "yes", "on"}
            else:
                payload[key] = bool(raw)
        else:
            if key not in values:
                continue
            value = values[key]
            payload[key] = "" if value is None else str(value)
    try:
        COPYRIGHT_SETTINGS_PATH.write_text(
            json.dumps(payload, ensure_ascii=False, indent=2),
            encoding="utf-8",
        )
    except OSError as exc:
        logger.error("Failed to save copyright settings: %s", exc)
        raise


def default_copyright_form() -> Dict[str, Any]:
    return load_copyright_settings()


LANG_OPTIONS: List[tuple[str, str]] = [
    ("ko", "í•œêµ­ì–´"),
    ("en", "English"),
    ("ja", "æ—¥æœ¬èª"),
]
LANG_OPTION_SET = {code for code, _ in LANG_OPTIONS}


def sanitize_lang(value: Optional[str]) -> str:
    if not value:
        return "ko"
    value_lower = value.lower()
    return value_lower if value_lower in LANG_OPTION_SET else "ko"


def _path_exists(value: Optional[str]) -> bool:
    if not value:
        return False
    try:
        return Path(value).exists()
    except OSError:
        return False


def ensure_directory(path: Path) -> None:
    path.mkdir(parents=True, exist_ok=True)


def parse_resize_form(value: str) -> Optional[tuple[int, int]]:
    try:
        width_str, height_str = value.lower().split("x")
        width = int(width_str)
        height = int(height_str)
        if width <= 0 or height <= 0:
            raise ValueError
        return width, height
    except Exception:  # pylint: disable=broad-except
        return None


# ---------------------------------------------------------------------------
# Styling helpers for video overlay rendering
# ---------------------------------------------------------------------------

FONT_SEARCH_DIRECTORIES: Tuple[Path, ...] = tuple(
    directory
    for directory in (
        Path(__file__).resolve().parent / "static" / "fonts",
        Path(__file__).resolve().parent.parent / "ai_shorts_maker" / "assets" / "fonts",
        Path("/usr/share/fonts/truetype/noto"),
        Path("/usr/share/fonts/truetype/nanum"),
        Path("/usr/share/fonts/truetype/dejavu"),
        Path("/usr/share/fonts/truetype/liberation"),
    )
    if directory.exists()
)


@lru_cache(maxsize=1)
def _collect_font_inventory() -> Dict[str, Path]:
    """Scan known directories for font files and cache the results."""
    inventory: Dict[str, Path] = {}
    for base_dir in FONT_SEARCH_DIRECTORIES:
        try:
            for font_file in base_dir.rglob("*"):
                if font_file.is_file() and font_file.suffix.lower() in {".ttf", ".otf", ".ttc"}:
                    inventory.setdefault(font_file.name.lower(), font_file)
        except (OSError, PermissionError):
            continue
    return inventory


@lru_cache(maxsize=None)
def _resolve_font_file(font_family: Optional[str], font_weight: Optional[str]) -> Optional[str]:
    """Find a matching font file for the requested family/weight."""
    inventory = _collect_font_inventory()
    if not inventory:
        return None

    normalized_family = (font_family or "").lower()
    normalized_weight = (font_weight or "").lower()
    prefer_bold = any(
        normalized_weight.startswith(prefix) for prefix in ("bold", "600", "700", "800", "900")
    )

    candidate_names: List[str] = []

    def add_candidate(name: str) -> None:
        lower = name.lower()
        if lower not in candidate_names:
            candidate_names.append(lower)

    if normalized_family:
        if "pretendard" in normalized_family:
            if prefer_bold:
                add_candidate("pretendard-semibold.ttf")
                add_candidate("pretendard-bold.ttf")
            add_candidate("pretendard-regular.ttf")
            add_candidate("pretendard.ttf")
        if "nanum" in normalized_family:
            if prefer_bold:
                add_candidate("nanumgothicbold.ttf")
                add_candidate("nanumsquareb.ttf")
                add_candidate("nanumbarungothicbold.ttf")
            add_candidate("nanumgothic.ttf")
            add_candidate("nanumsquarer.ttf")
            add_candidate("nanumbarungothic.ttf")
        if "noto" in normalized_family:
            # ì¼ë³¸ì–´ í°íŠ¸ ìš°ì„  ì²˜ë¦¬ (JP ë˜ëŠ” Japaneseê°€ ëª…ì‹œëœ ê²½ìš°)
            if "jp" in normalized_family or "japanese" in normalized_family:
                if prefer_bold:
                    add_candidate("notosansjp-bold.ttf")
                    add_candidate("notosanscjk-regular.ttc")  # CJK í†µí•© í°íŠ¸
                add_candidate("notosansjp-regular.ttf")
                add_candidate("notosanscjk-regular.ttc")
            # í•œêµ­ì–´ í°íŠ¸ ì²˜ë¦¬
            elif "kr" in normalized_family or "korean" in normalized_family:
                if prefer_bold:
                    add_candidate("notosanskr-bold.ttf")
                    add_candidate("notosans-bold.ttf")
                    add_candidate("notosanscjkkr-bold.otf")
                add_candidate("notosanskr-regular.ttf")
                add_candidate("notosans-regular.ttf")
                add_candidate("notosanscjkkr-regular.otf")
            # ì¼ë°˜ Noto í°íŠ¸ (í•œêµ­ì–´ ê¸°ë³¸)
            else:
                if prefer_bold:
                    add_candidate("notosans-bold.ttf")
                    add_candidate("notosanscjkk-bold.otf")
                    add_candidate("notosanscjkkr-bold.otf")
                    add_candidate("notosanskr-bold.ttf")
                    add_candidate("notosansjp-bold.ttf")
                add_candidate("notosans-regular.ttf")
                add_candidate("notosanscjkkr-regular.otf")
                add_candidate("notosanscjk-regular.ttc")
                add_candidate("notosanskr-regular.ttf")
                add_candidate("notosansjp-regular.ttf")
        if "arial" in normalized_family:
            if prefer_bold:
                add_candidate("arialbd.ttf")
            add_candidate("arial.ttf")
        if "segoe" in normalized_family:
            if prefer_bold:
                add_candidate("segoeuib.ttf")
            add_candidate("segoeui.ttf")

    # ê¸°ë³¸ í°íŠ¸ fallback ìˆœì„œ (í•œê¸€ ì§€ì› ìš°ì„ )
    if prefer_bold:
        add_candidate("pretendard-semibold.ttf")
        add_candidate("nanumgothicbold.ttf")
        add_candidate("nanumsquareb.ttf")
        add_candidate("nanumbarungothicbold.ttf")
        add_candidate("notosans-bold.ttf")
        add_candidate("arialbd.ttf")

    add_candidate("pretendard-regular.ttf")
    add_candidate("nanumgothic.ttf")
    add_candidate("nanumsquarer.ttf")
    add_candidate("nanumbarungothic.ttf")
    add_candidate("notosans-regular.ttf")
    add_candidate("dejavusans.ttf")
    add_candidate("arial.ttf")

    for candidate in candidate_names:
        if candidate in inventory:
            return str(inventory[candidate])
        for file_name, path in inventory.items():
            if candidate in file_name:
                return str(path)

    return None


@lru_cache(maxsize=256)
def _parse_css_color(value: Optional[str], default_hex: str = "0xFFFFFF") -> Tuple[str, Optional[float]]:
    """Convert CSS color expressions into FFmpeg-compatible hex + alpha."""
    if not value:
        return default_hex, None
    try:
        r, g, b, a = ImageColor.getcolor(value.strip(), "RGBA")
        hex_value = f"0x{r:02X}{g:02X}{b:02X}"
        if a < 255:
            return hex_value, round(a / 255.0, 4)
        return hex_value, None
    except Exception:  # pylint: disable=broad-except
        return default_hex, None


def _format_color_for_ffmpeg(color_hex: str, alpha: Optional[float]) -> str:
    """Format a color + alpha for drawtext."""
    if alpha is None:
        return color_hex
    alpha_clamped = max(0.0, min(1.0, float(alpha)))
    if alpha_clamped >= 0.9999:
        return color_hex
    alpha_str = f"{alpha_clamped:.4f}".rstrip("0").rstrip(".")
    return f"{color_hex}@{alpha_str}"


def _parse_css_length(value: Optional[str], font_size: float) -> Optional[float]:
    """Parse CSS length values (px, em, rem, %) into pixel units."""
    if not value:
        return None
    lowered = value.strip().lower()
    if not lowered or lowered in {"auto", "normal"}:
        return None
    try:
        if lowered.endswith("px"):
            return float(lowered[:-2])
        if lowered.endswith("em"):
            return float(lowered[:-2]) * font_size
        if lowered.endswith("rem"):
            return float(lowered[:-3]) * font_size
        if lowered.endswith("%"):
            return float(lowered[:-1]) * font_size / 100.0
        return float(lowered)
    except ValueError:
        return None


def _parse_text_shadow(shadow_value: Optional[str], font_size: float) -> Optional[Dict[str, Any]]:
    """Extract the first text-shadow entry and convert it to pixel offsets."""
    if not shadow_value:
        return None
    shadow_str = shadow_value.strip()
    if not shadow_str or shadow_str.lower() == "none":
        return None

    entries: List[str] = []
    buffer = ""
    depth = 0
    for char in shadow_str:
        if char == "(":
            depth += 1
        elif char == ")":
            depth -= 1
        if char == "," and depth == 0:
            if buffer.strip():
                entries.append(buffer.strip())
            buffer = ""
        else:
            buffer += char
    if buffer.strip():
        entries.append(buffer.strip())

    if not entries:
        return None

    first_entry = entries[0]
    color_match = re.search(r'(rgba?\([^)]*\)|#[0-9a-fA-F]{3,8}|[a-zA-Z]+)\s*$', first_entry)
    color = color_match.group(1) if color_match else None
    remainder = first_entry[: color_match.start()].strip() if color_match else first_entry

    length_tokens = [token for token in re.split(r"\s+", remainder) if token]
    dx = _parse_css_length(length_tokens[0], font_size) if len(length_tokens) > 0 else 0.0
    dy = _parse_css_length(length_tokens[1], font_size) if len(length_tokens) > 1 else 0.0

    return {
        "color": color,
        "dx": float(dx or 0.0),
        "dy": float(dy or 0.0),
    }


def escape_ffmpeg_text(text: str) -> str:
    """Escape characters that would break FFmpeg drawtext filters."""
    escaped = text.replace("\\", "\\\\")
    escaped = escaped.replace("'", "'\\''")
    escaped = escaped.replace(":", "\\:")
    escaped = escaped.replace("[", "\\[")
    escaped = escaped.replace("]", "\\]")
    return escaped


def _contains_emoji(text: str) -> bool:
    """
    í…ìŠ¤íŠ¸ì— ì´ëª¨ì§€ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸

    Args:
        text: í™•ì¸í•  í…ìŠ¤íŠ¸

    Returns:
        bool: ì´ëª¨ì§€ê°€ ìˆìœ¼ë©´ True
    """
    # ì´ëª¨ì§€ ìœ ë‹ˆì½”ë“œ ë²”ìœ„
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"  # ì´ëª¨í‹°ì½˜
        "\U0001F300-\U0001F5FF"  # ì‹¬ë³¼ & í”½í† ê·¸ë¨
        "\U0001F680-\U0001F6FF"  # êµí†µ & ì§€ë„ ì‹¬ë³¼
        "\U0001F1E0-\U0001F1FF"  # êµ­ê¸°
        "\U00002702-\U000027B0"  # Dingbats
        "\U000024C2-\U0001F251"  # ê¸°íƒ€ ì‹¬ë³¼
        "\U0001F900-\U0001F9FF"  # ì¶”ê°€ ì´ëª¨ì§€
        "\U0001FA70-\U0001FAFF"  # í™•ì¥ ì´ëª¨ì§€
        "\u2600-\u26FF"          # ê¸°íƒ€ ì‹¬ë³¼
        "\u2700-\u27BF"          # Dingbats
        "]+",
        flags=re.UNICODE
    )
    return bool(emoji_pattern.search(text))


async def _render_subtitle_with_emoji(
    text: str,
    output_path: str,
    video_width: int,
    video_height: int,
    font_size: int = 40,
    font_color: str = "white",
    outline_color: str = "black",
    outline_width: int = 3,
    y_position_percent: float = 0.85,
    font_family: str = '"Noto Sans CJK JP", "Noto Color Emoji", sans-serif',
) -> bool:
    """
    ì´ëª¨ì§€ê°€ í¬í•¨ëœ ìë§‰ì„ PNGë¡œ ë Œë”ë§

    Args:
        text: ìë§‰ í…ìŠ¤íŠ¸
        output_path: PNG ì €ì¥ ê²½ë¡œ
        video_width: ë¹„ë””ì˜¤ ë„ˆë¹„
        video_height: ë¹„ë””ì˜¤ ë†’ì´
        font_size: í°íŠ¸ í¬ê¸°
        font_color: í…ìŠ¤íŠ¸ ìƒ‰ìƒ
        outline_color: ì™¸ê³½ì„  ìƒ‰ìƒ
        outline_width: ì™¸ê³½ì„  ë„ˆë¹„
        y_position_percent: Y ìœ„ì¹˜ (0.0-1.0)
        font_family: í°íŠ¸ íŒ¨ë°€ë¦¬

    Returns:
        bool: ì„±ê³µ ì—¬ë¶€
    """
    try:
        # PNG ë†’ì´ëŠ” í°íŠ¸ í¬ê¸°ì˜ 2.5ë°° ì •ë„
        png_height = int(font_size * 2.5)
        y_pixel = int(video_height * y_position_percent)

        success = await render_emoji_text_to_png(
            text=text,
            output_path=output_path,
            width=video_width,
            height=png_height,
            font_size=font_size,
            font_color=font_color,
            outline_color=outline_color,
            outline_width=outline_width,
            x_position="center",
            y_position="center",
            font_family=font_family,
        )

        if success:
            logging.info(f"âœ… ì´ëª¨ì§€ ìë§‰ PNG ìƒì„±: {output_path}")
            logging.info(f"   í…ìŠ¤íŠ¸: {text}")
            logging.info(f"   í¬ê¸°: {video_width}x{png_height}px")
        else:
            logging.error(f"âŒ ì´ëª¨ì§€ ìë§‰ PNG ìƒì„± ì‹¤íŒ¨: {output_path}")

        return success

    except Exception as e:
        logging.error(f"âŒ ì´ëª¨ì§€ ìë§‰ ë Œë”ë§ ì˜¤ë¥˜: {e}", exc_info=True)
        return False


def _contains_emoji(text: str) -> bool:
    """í…ìŠ¤íŠ¸ì— ì´ëª¨ì§€ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
    if not text:
        return False

    return any(
        '\U0001F300' <= char <= '\U0001F9FF' or  # ì´ëª¨ì§€ ë° ê¸°í˜¸
        '\U0001F600' <= char <= '\U0001F64F' or  # ê°ì • í‘œí˜„
        '\U0001F680' <= char <= '\U0001F6FF' or  # êµí†µ/ì§€ë„
        '\U00002600' <= char <= '\U000026FF' or  # Miscellaneous Symbols (âš” í¬í•¨)
        '\U00002700' <= char <= '\U000027BF' or  # Dingbats
        '\U0001F900' <= char <= '\U0001F9FF' or  # ì¶”ê°€ ì´ëª¨ì§€
        '\U0001FA00' <= char <= '\U0001FAFF'     # Extended-A
        for char in text
    )


async def _create_overlay_png_filter(
    overlay: Dict[str, Any],
    video_width: int,
    video_height: int,
    temp_dir: Path,
) -> Optional[Tuple[str, Dict[str, Any]]]:
    """ì´ëª¨ì§€ê°€ í¬í•¨ëœ ì˜¤ë²„ë ˆì´ë¥¼ PNGë¡œ ë Œë”ë§í•˜ê³  overlay í•„í„° ìƒì„±"""
    if not overlay or not overlay.get("text"):
        return None

    text = str(overlay.get("text", ""))
    if not text:
        return None

    try:
        font_size = max(12, int(round(float(overlay.get("fontSize", 48)))))
    except (TypeError, ValueError):
        font_size = 48

    # í°íŠ¸ ìƒ‰ìƒ íŒŒì‹±
    font_color_raw = overlay.get("color", "#FFFFFF")
    try:
        if font_color_raw.startswith("#"):
            # CSS hex color
            font_color = font_color_raw
        elif font_color_raw.startswith("rgb"):
            # rgb(a) -> hex ë³€í™˜
            import re
            rgb_match = re.match(r"rgba?\((\d+),\s*(\d+),\s*(\d+)", font_color_raw)
            if rgb_match:
                r, g, b = rgb_match.groups()
                font_color = f"#{int(r):02x}{int(g):02x}{int(b):02x}"
            else:
                font_color = "#FFFFFF"
        else:
            font_color = "#FFFFFF"
    except Exception:
        font_color = "#FFFFFF"

    # ì™¸ê³½ì„  ìƒ‰ìƒ (ê¸°ë³¸ê°’: ê²€ì€ìƒ‰)
    outline_color = "#000000"
    outline_width = 3

    # ìœ„ì¹˜ ê³„ì‚°
    def _coerce_position(value: Any, fallback: float) -> float:
        try:
            if value is None:
                raise TypeError
            return float(value)
        except (TypeError, ValueError):
            return fallback

    x_value = _coerce_position(overlay.get("x"), video_width / 2)
    y_value = _coerce_position(overlay.get("y"), video_height / 2)

    # y_position_percent ê³„ì‚° (0.0 ~ 1.0)
    y_position_percent = y_value / video_height if video_height > 0 else 0.5

    # PNG íŒŒì¼ ê²½ë¡œ
    png_filename = f"emoji_overlay_{uuid4().hex[:8]}.png"
    png_path = temp_dir / png_filename

    # í°íŠ¸ íŒ¨ë°€ë¦¬ ê²°ì •
    overlay_type = (overlay.get("type") or "").lower()
    if overlay_type == "japanese" or any('\u3040' <= c <= '\u309F' or '\u30A0' <= c <= '\u30FF' for c in text):
        font_family = '"Noto Sans CJK JP", "Noto Color Emoji", sans-serif'
    else:
        font_family = '"Noto Sans CJK KR", "Noto Color Emoji", sans-serif'

    # PNG ë Œë”ë§
    success = await _render_subtitle_with_emoji(
        text=text,
        output_path=str(png_path),
        video_width=video_width,
        video_height=video_height,
        font_size=font_size,
        font_color=font_color,
        outline_color=outline_color,
        outline_width=outline_width,
        y_position_percent=y_position_percent,
        font_family=font_family,
    )

    if not success or not png_path.exists():
        logging.error(f"âŒ PNG ë Œë”ë§ ì‹¤íŒ¨: {png_path}")
        return None

    # overlay í•„í„° ìƒì„±
    # PNGë¥¼ ë¹„ë””ì˜¤ ì¤‘ì•™ì— ë°°ì¹˜ (y ìœ„ì¹˜ëŠ” y_position_percent ê¸°ë°˜)
    y_pixel = int(video_height * y_position_percent)

    # PNG ì´ë¯¸ì§€ ë†’ì´ ê³„ì‚° (í°íŠ¸ í¬ê¸°ì˜ 2.5ë°°)
    png_height = int(font_size * 2.5)

    # PNGë¥¼ ì´ë¯¸ì§€ë¡œ ë¡œë“œí•˜ê³  overlay í•„í„° ìƒì„±
    # í˜•ì‹: movie=filename:loop=0,setpts=N/FRAME_RATE/TB[png];[in][png]overlay=x:y[out]
    # ë” ê°„ë‹¨í•œ ë°©ë²•: overlay í•„í„°ì— ì§ì ‘ ê²½ë¡œ ì§€ì •
    # ì°¸ê³ : FFmpeg overlay í•„í„°ëŠ” ë‘ ê°œì˜ ì…ë ¥ ìŠ¤íŠ¸ë¦¼ì´ í•„ìš”í•˜ë¯€ë¡œ,
    # PNGë¥¼ ë³„ë„ ì…ë ¥ìœ¼ë¡œ ì¶”ê°€í•˜ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤

    # PNG ê²½ë¡œë¥¼ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
    escaped_png_path = str(png_path).replace("\\", "\\\\").replace(":", "\\:")

    meta = {
        "font_size": font_size,
        "png_path": str(png_path),
        "font_color": font_color,
        "outline_color": outline_color,
        "outline_width": outline_width,
        "is_png_overlay": True,  # PNG ì˜¤ë²„ë ˆì´ì„ì„ í‘œì‹œ
        "y_pixel": y_pixel,
        "png_height": png_height,
    }

    logging.info(f"âœ… ì´ëª¨ì§€ PNG ì˜¤ë²„ë ˆì´ ìƒì„±: {png_filename}")
    logging.info(f"   í…ìŠ¤íŠ¸: {text}")
    logging.info(f"   í¬ê¸°: {font_size}px")
    logging.info(f"   ìœ„ì¹˜: ({x_value}, {y_value})")

    # PNG ì˜¤ë²„ë ˆì´ëŠ” ë©”íƒ€ë°ì´í„°ë§Œ ë°˜í™˜ (í•„í„°ëŠ” ë‚˜ì¤‘ì— êµ¬ì„±)
    return None, meta


def _build_drawtext_filter(
    overlay: Dict[str, Any],
    video_width: int,
    video_height: int,
) -> Optional[Tuple[str, Dict[str, Any]]]:
    """Construct a drawtext filter string that mirrors the web overlay styling."""
    if not overlay or not overlay.get("text"):
        return None

    text_raw = str(overlay.get("text", ""))
    if not text_raw:
        return None
    text = escape_ffmpeg_text(text_raw)

    overlay_type = (overlay.get("type") or "").lower()

    try:
        font_size = max(12, int(round(float(overlay.get("fontSize", 48)))))
    except (TypeError, ValueError):
        font_size = 48

    # âš ï¸ ì£¼ì˜: ì›¹ ë¯¸ë¦¬ë³´ê¸°ì™€ ë™ê¸°í™”ë¥¼ ìœ„í•´ í°íŠ¸ í¬ê¸° ì¶•ì†Œë¥¼ ì œê±°í•¨
    # if overlay_type in {"korean", "english"}:
    #     font_size = max(12, int(round(font_size / 5.0)))

    def _coerce_position(value: Any, fallback: float) -> float:
        try:
            if value is None:
                raise TypeError
            return float(value)
        except (TypeError, ValueError):
            return fallback

    default_positions = {
        "title": {"x": video_width / 2, "y": video_height * 0.82},
        "subtitle": {"x": video_width / 2, "y": video_height * 0.9},
        "korean": {"x": video_width * 0.898026, "y": video_height * 1.0},
        "english": {"x": video_width * 1.0, "y": video_height * 0.886635},
        "source": {"x": video_width / 2, "y": video_height * 0.97},
    }

    fallback_position = default_positions.get(overlay_type, {"x": video_width / 2, "y": video_height / 2})
    x_value = _coerce_position(overlay.get("x"), fallback_position.get("x", video_width / 2))
    y_value = _coerce_position(overlay.get("y"), fallback_position.get("y", video_height / 2))

    if overlay_type in {"title", "subtitle"} and overlay.get("y") in {None, ""}:
        # ì œëª©/ë¶€ì œëª©ì´ ìœ„ì¹˜ ê°’ì„ ê°–ì§€ ì•ŠëŠ” ê²½ìš° ì£¼/ë³´ì¡° ìë§‰ ê¸°ë³¸ ìœ„ì¹˜ì™€ ë§ì¶˜ë‹¤.
        anchor_key = "korean" if overlay_type == "title" else "english"
        anchor_fallback = default_positions.get(anchor_key)
        if anchor_fallback and anchor_fallback.get("y") is not None:
            y_value = anchor_fallback["y"]

    # drawtext ì¢Œí‘œëŠ” í…ìŠ¤íŠ¸ ìƒë‹¨/ì¢Œì¸¡ ê¸°ì¤€ìœ¼ë¡œ ë‹¤ë£¨ë©°, CSSì˜ translateX(-50%) íš¨ê³¼ë¥¼ ëª¨ì‚¬í•œë‹¤.
    x_expr = f"clip({x_value}-text_w/2,0,{video_width}-text_w)"

    # âš ï¸ Canvas textBaseline='middle'ê³¼ ë™ê¸°í™”: ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì¤‘ì•™ ì •ë ¬
    # CanvasëŠ” ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ textBaseline='middle'ë¡œ ë Œë”ë§í•˜ë¯€ë¡œ,
    # FFmpegë„ ë™ì¼í•˜ê²Œ y ì¢Œí‘œë¥¼ í…ìŠ¤íŠ¸ ì¤‘ì•™ìœ¼ë¡œ ì²˜ë¦¬
    y_expr_base = f"{y_value}-text_h/2"

    y_expr = f"clip({y_expr_base},0,{video_height}-text_h)"

    font_family = overlay.get("fontFamily")
    font_weight = overlay.get("fontWeight")

    # CJK ë¬¸ì ê°ì§€
    has_korean = any('\uac00' <= char <= '\ud7a3' or '\u3131' <= char <= '\u318e' for char in text_raw)
    has_japanese = any(
        '\u3040' <= char <= '\u309f' or  # íˆë¼ê°€ë‚˜
        '\u30a0' <= char <= '\u30ff' or  # ê°€íƒ€ì¹´ë‚˜
        '\u31f0' <= char <= '\u31ff' or  # ê°€íƒ€ì¹´ë‚˜ í™•ì¥
        '\uff61' <= char <= '\uff9f' or  # ë°˜ê° ê°€íƒ€ì¹´ë‚˜ ë° êµ¬ë‘ì 
        '\u4e00' <= char <= '\u9faf'     # CJK í•œì
        for char in text_raw
    )
    has_cjk = has_korean or has_japanese

    # CJK ë¬¸ìì— ë§ëŠ” í°íŠ¸ ì„ íƒ
    normalized_font_family = (font_family or "").lower()

    if has_cjk:
        if has_japanese:
            # ì¼ë³¸ì–´ í…ìŠ¤íŠ¸ì—ëŠ” ì¼ë³¸ì–´ ì§€ì› í°íŠ¸ë¥¼ ê°•ì œë¡œ ì§€ì • (KR í°íŠ¸ ì‚¬ìš© ì‹œ â–¡ í‘œì‹œ ë°©ì§€)
            if not normalized_font_family or ("jp" not in normalized_font_family and "cjk" not in normalized_font_family):
                font_family = "NotoSansJP"
                normalized_font_family = font_family.lower()
        else:
            if not normalized_font_family or (
                "nanum" not in normalized_font_family
                and "pretendard" not in normalized_font_family
                and "noto" not in normalized_font_family
            ):
                font_family = "NanumGothic"
                normalized_font_family = font_family.lower()

    font_path = _resolve_font_file(font_family, font_weight)

    # CJK í°íŠ¸ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°, ì‹œìŠ¤í…œ í°íŠ¸ ì§ì ‘ ì§€ì •
    if has_cjk and not font_path:
        if has_japanese:
            # ì¼ë³¸ì–´ê°€ í¬í•¨ëœ ê²½ìš°: CJK í†µí•© í°íŠ¸ ìš°ì„  ì‚¬ìš©
            fallback_fonts = [
                "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc",  # CJK í†µí•© (ì¼ë³¸ì–´ í¬í•¨)
                "/home/sk/ws/youtubeanalysis/web_app/static/fonts/NotoSansJP-Regular.ttf",  # ì¼ë³¸ì–´ ì „ìš©
                "/usr/share/fonts/truetype/noto/NotoSans-Regular.ttf",     # í•œêµ­ì–´ í°íŠ¸ (fallback)
                "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
            ]
        else:
            # í•œêµ­ì–´ë§Œ ìˆëŠ” ê²½ìš°: í•œêµ­ì–´ í°íŠ¸ ìš°ì„ 
            fallback_fonts = [
                "/usr/share/fonts/truetype/noto/NotoSans-Regular.ttf",
                "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
                "/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf",
                "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc",  # CJK í†µí•© (fallback)
            ]
        for fallback in fallback_fonts:
            if Path(fallback).exists():
                font_path = fallback
                logging.info(f"âœ… CJK í°íŠ¸ ì„ íƒ: {font_path} (ì¼ë³¸ì–´: {has_japanese}, í•œêµ­ì–´: {has_korean})")
                break

    # ì˜¤ë²„ë ˆì´ íƒ€ì…ì— ë”°ë¼ ìƒ‰ìƒ ê°•ì œ ì„¤ì •
    overlay_opacity = overlay.get("opacity")

    if overlay_type in ["korean", "english"]:
        # korean/english ì˜¤ë²„ë ˆì´ëŠ” í•­ìƒ í°ìƒ‰ìœ¼ë¡œ ê³ ì • (ê²€ì • ë°°ê²½ì— í‘œì‹œë˜ë¯€ë¡œ)
        font_hex = "FFFFFF"
        font_alpha = 1.0
    else:
        # title, subtitle ë“±ì€ ê¸°ì¡´ ìƒ‰ìƒ ìœ ì§€
        font_hex, font_alpha = _parse_css_color(overlay.get("color"))
        if overlay_opacity is not None:
            try:
                opacity_value = float(overlay_opacity)
                opacity_value = max(0.0, min(1.0, opacity_value))
                if font_alpha is None:
                    font_alpha = opacity_value
                else:
                    font_alpha = max(0.0, min(1.0, font_alpha * opacity_value))
            except (TypeError, ValueError):
                pass
    font_color = _format_color_for_ffmpeg(font_hex, font_alpha)

    outline_hex, outline_alpha = _parse_css_color(
        overlay.get("outlineColor"),
        default_hex="0x000000",
    )
    outline_color = _format_color_for_ffmpeg(outline_hex, outline_alpha)

    if overlay_type in {"korean", "english"}:
        border_width = max(2, int(round(font_size * 0.08)))
    else:
        border_width = max(1, int(round(font_size * 0.06)))

    letter_spacing = overlay.get("letterSpacing")
    spacing_value = _parse_css_length(letter_spacing, font_size) if letter_spacing else None

    shadow_info = _parse_text_shadow(overlay.get("textShadow"), font_size)

    background_color_raw = overlay.get("backgroundColor")
    box_color_hex, box_alpha = _parse_css_color(background_color_raw, default_hex="0x000000")

    padding = overlay.get("padding") or {}
    padding_candidates = [
        padding.get("top"),
        padding.get("right"),
        padding.get("bottom"),
        padding.get("left"),
    ]
    padding_pixels = [
        float(value)
        for value in padding_candidates
        if isinstance(value, (int, float)) and value > 0
    ]
    if not padding_pixels and background_color_raw:
        padding_pixels = [font_size * 0.25]
    padding_amount = int(round(max(padding_pixels))) if padding_pixels else 0

    drawtext_params = [
        f"text='{text}'",
        f"x='{x_expr}'",
        f"y='{y_expr}'",
        f"fontsize={font_size}",
        "text_shaping=1",
        f"fontcolor={font_color}",
    ]

    # ì´ëª¨ì§€ ê°ì§€ (ì¼ë°˜ì ì¸ ì´ëª¨ì§€ ìœ ë‹ˆì½”ë“œ ë²”ìœ„)
    has_emoji = any(
        '\U0001F300' <= char <= '\U0001F9FF' or  # ì´ëª¨ì§€ ë° ê¸°í˜¸
        '\U0001F600' <= char <= '\U0001F64F' or  # ê°ì • í‘œí˜„
        '\U0001F680' <= char <= '\U0001F6FF' or  # êµí†µ/ì§€ë„
        '\U00002702' <= char <= '\U000027B0' or  # ê¸°íƒ€ ê¸°í˜¸
        '\U0001F900' <= char <= '\U0001F9FF'     # ì¶”ê°€ ì´ëª¨ì§€
        for char in text_raw
    )

    # ì´ëª¨ì§€ê°€ ìˆìœ¼ë©´ fontconfigë¥¼ ì‚¬ìš© (ìë™ fallback)
    # ì—†ìœ¼ë©´ ê¸°ì¡´ëŒ€ë¡œ fontfile ì‚¬ìš©
    if has_emoji and has_japanese:
        # ì¼ë³¸ì–´ + ì´ëª¨ì§€: fontconfig ì‚¬ìš©í•˜ì—¬ Noto Sans CJK JP + Color Emoji fallback
        drawtext_params.append("font='Noto Sans CJK JP'")
        logging.info(f"ğŸ¨ ì´ëª¨ì§€ ê°ì§€: fontconfig ì‚¬ìš© (Noto Sans CJK JP + emoji fallback)")
    elif has_emoji:
        # ì´ëª¨ì§€ë§Œ: ê¸°ë³¸ í°íŠ¸ + emoji fallback
        drawtext_params.append("font='Noto Sans'")
        logging.info(f"ğŸ¨ ì´ëª¨ì§€ ê°ì§€: fontconfig ì‚¬ìš© (Noto Sans + emoji fallback)")
    elif font_path:
        # ì´ëª¨ì§€ ì—†ìŒ: ê¸°ì¡´ëŒ€ë¡œ fontfile ì‚¬ìš©
        sanitized_font = font_path.replace("\\", "\\\\").replace(":", "\\:")
        drawtext_params.append(f"fontfile={sanitized_font}")

    if spacing_value:
        drawtext_params.append(f"spacing={spacing_value:.2f}")

    if border_width > 0:
        drawtext_params.append(f"borderw={border_width}")
        drawtext_params.append(f"bordercolor={outline_color}")

    if shadow_info and shadow_info.get("color"):
        shadow_hex, shadow_alpha = _parse_css_color(shadow_info["color"], default_hex="0x000000")
        shadow_color = _format_color_for_ffmpeg(shadow_hex, shadow_alpha)
        drawtext_params.append(f"shadowcolor={shadow_color}")
        drawtext_params.append(f"shadowx={int(round(shadow_info.get('dx', 0.0)))}")
        drawtext_params.append(f"shadowy={int(round(shadow_info.get('dy', 0.0)))}")

    if background_color_raw and (box_alpha is None or box_alpha > 0):
        drawtext_params.append("box=1")
        drawtext_params.append(f"boxcolor={_format_color_for_ffmpeg(box_color_hex, box_alpha)}")
        drawtext_params.append(f"boxborderw={max(1, padding_amount)}")

    filter_str = "drawtext=" + ":".join(drawtext_params)
    meta = {
        "font_size": font_size,
        "fontfile": font_path,
        "font_color": font_color,
        "outline_color": outline_color,
        "border_width": border_width,
        "letter_spacing": spacing_value,
        "shadow": shadow_info,
        "background": background_color_raw,
        "padding": padding_amount,
        "opacity": overlay_opacity,
    }

    return filter_str, meta


def build_dashboard_project(summary: ProjectSummary) -> DashboardProject:
    audio_ready = _path_exists(summary.audio_path)
    video_ready = _path_exists(summary.video_path)

    completed_steps = 1
    status: Literal["draft", "translating", "voice_ready", "rendering", "rendered", "failed"] = "draft"

    if audio_ready:
        completed_steps = max(completed_steps, 3)
        status = "voice_ready"
    if video_ready:
        completed_steps = 4
        status = "rendered"

    updated_at = summary.updated_at.isoformat() if summary.updated_at else None

    return DashboardProject(
        id=summary.base_name,
        title=summary.topic or summary.base_name,
        topic=summary.topic,
        language=summary.language,
        status=status,
        completed_steps=completed_steps,
        thumbnail=summary.video_path,
        updated_at=updated_at,
    )


load_dotenv()
ASSETS_DIR.mkdir(parents=True, exist_ok=True)
(ASSETS_DIR / "broll").mkdir(exist_ok=True)
(ASSETS_DIR / "music").mkdir(exist_ok=True)
OUTPUT_DIR.mkdir(exist_ok=True)
COPYRIGHT_UPLOAD_DIR.mkdir(parents=True, exist_ok=True)
COPYRIGHT_REPORT_DIR.mkdir(parents=True, exist_ok=True)

app = FastAPI(title="AI Shorts Maker")

# ë‹¤ìš´ë¡œë“œ ì§„í–‰ ìƒíƒœ ì¶”ì 
download_tasks: Dict[str, Dict[str, Any]] = {}

# ì˜¤ë””ì˜¤ ì¶”ì¶œ ì§„í–‰ ìƒíƒœ ì¶”ì 
extraction_tasks: Dict[str, Dict[str, Any]] = {}

# ì˜ìƒ ìë¥´ê¸° ì§„í–‰ ìƒíƒœ ì¶”ì 
cut_video_tasks: Dict[str, Dict[str, Any]] = {}

# í”„ë¦¬ë·° ì˜ìƒ ìƒì„± ì§„í–‰ ìƒíƒœ ì¶”ì 
preview_video_tasks: Dict[str, Dict[str, Any]] = {}

# CORS ì„¤ì • ì¶”ê°€
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ëª¨ë“  origin í—ˆìš© (ê°œë°œìš©)
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ê°œë°œ í™˜ê²½: ì •ì  íŒŒì¼ ìºì‹œ ë¹„í™œì„±í™” ë¯¸ë“¤ì›¨ì–´
@app.middleware("http")
async def disable_cache_middleware(request: Request, call_next):
    """ê°œë°œ í™˜ê²½ì—ì„œ ì •ì  íŒŒì¼ ìºì‹œë¥¼ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤."""
    response = await call_next(request)

    # ì •ì  íŒŒì¼ì— ëŒ€í•´ì„œë§Œ ìºì‹œ ë¹„í™œì„±í™”
    if request.url.path.startswith("/static/") or request.url.path.startswith("/outputs/"):
        response.headers["Cache-Control"] = "no-cache, no-store, must-revalidate"
        response.headers["Pragma"] = "no-cache"
        response.headers["Expires"] = "0"

    return response

app.mount("/static", StaticFiles(directory=STATIC_DIR), name="static")
app.mount("/outputs", StaticFiles(directory=OUTPUT_DIR), name="outputs")
app.mount("/youtube/download", StaticFiles(directory=DEFAULT_YTDL_OUTPUT_DIR), name="youtube_download")

templates = Jinja2Templates(directory=str(TEMPLATES_DIR))

api_router = APIRouter(prefix="/api", tags=["projects"])


@api_router.get("/projects", response_model=List[ProjectSummary])
def api_list_projects() -> List[ProjectSummary]:
    return list_projects(OUTPUT_DIR)


@api_router.get("/projects/{base_name}", response_model=ProjectMetadata)
def api_get_project(base_name: str) -> ProjectMetadata:
    try:
        return load_project(base_name, OUTPUT_DIR)
    except FileNotFoundError as exc:  # pragma: no cover - handled as HTTP 404
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=str(exc)) from exc


@api_router.post("/projects/{base_name}/subtitles", response_model=ProjectMetadata)
def api_add_subtitle(base_name: str, payload: SubtitleCreate) -> ProjectMetadata:
    try:
        return add_subtitle(base_name, payload)
    except FileNotFoundError as exc:
        raise HTTPException(status_code=404, detail=str(exc)) from exc
    except ValueError as exc:
        raise HTTPException(status_code=400, detail=str(exc)) from exc


@api_router.patch("/projects/{base_name}/subtitles/{subtitle_id}", response_model=ProjectMetadata)
def api_update_subtitle(base_name: str, subtitle_id: str, payload: SubtitleUpdate) -> ProjectMetadata:
    try:
        return update_subtitle(base_name, subtitle_id, payload)
    except FileNotFoundError as exc:
        raise HTTPException(status_code=404, detail=str(exc)) from exc
    except KeyError as exc:
        raise HTTPException(status_code=404, detail=str(exc)) from exc
    except ValueError as exc:
        raise HTTPException(status_code=400, detail=str(exc)) from exc


@api_router.delete("/projects/{base_name}/subtitles/{subtitle_id}", response_model=ProjectMetadata)
def api_delete_subtitle(base_name: str, subtitle_id: str) -> ProjectMetadata:
    try:
        return delete_subtitle_line(base_name, subtitle_id)
    except FileNotFoundError as exc:
        raise HTTPException(status_code=404, detail=str(exc)) from exc
    except KeyError as exc:
        raise HTTPException(status_code=404, detail=str(exc)) from exc


@api_router.patch("/projects/{base_name}/timeline", response_model=ProjectMetadata)
def api_update_timeline(base_name: str, payload: TimelineUpdate) -> ProjectMetadata:
    try:
        return replace_timeline(base_name, payload)
    except FileNotFoundError as exc:
        raise HTTPException(status_code=404, detail=str(exc)) from exc


@api_router.patch("/projects/{base_name}/audio", response_model=ProjectMetadata)
def api_update_audio(
    base_name: str,
    music_enabled: Optional[bool] = Body(None),
    music_volume: Optional[float] = Body(None),
    ducking: Optional[float] = Body(None),
    music_track: Optional[str] = Body(None),
) -> ProjectMetadata:
    try:
        return update_audio_settings(
            base_name,
            music_enabled=music_enabled,
            music_volume=music_volume,
            ducking=ducking,
            music_track=music_track,
        )
    except FileNotFoundError as exc:
        raise HTTPException(status_code=404, detail=str(exc)) from exc


@api_router.delete("/projects/{base_name}", status_code=status.HTTP_204_NO_CONTENT)
def api_delete_project(base_name: str) -> JSONResponse:
    try:
        repository_delete_project(base_name, OUTPUT_DIR)
    except FileNotFoundError as exc:
        raise HTTPException(status_code=404, detail=str(exc)) from exc
    return JSONResponse(status_code=status.HTTP_204_NO_CONTENT, content=None)


@api_router.post("/projects/{base_name}/clone", response_model=ProjectMetadata)
def api_clone_project(base_name: str) -> ProjectMetadata:
    """í”„ë¡œì íŠ¸ë¥¼ ë³µì œí•˜ì—¬ ë°±ì—…ë³¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        return clone_project(base_name)
    except FileNotFoundError as exc:
        raise HTTPException(status_code=404, detail=str(exc)) from exc
    except Exception as exc:
        raise HTTPException(status_code=500, detail=str(exc)) from exc


@api_router.post("/projects/{base_name}/render", response_model=ProjectMetadata)
def api_render_project(base_name: str, payload: Optional[RenderRequest] = Body(None)) -> ProjectMetadata:
    try:
        burn = payload.burn_subs if payload is not None else False
        return render_project(base_name, burn_subs=bool(burn))
    except FileNotFoundError as exc:
        raise HTTPException(status_code=404, detail=str(exc)) from exc
    except RuntimeError as exc:
        raise HTTPException(status_code=400, detail=str(exc)) from exc


@api_router.patch("/projects/{base_name}/subtitle-style", response_model=ProjectMetadata)
def api_update_subtitle_style_route(base_name: str, payload: SubtitleStyleRequest) -> ProjectMetadata:
    try:
        data = payload.model_dump(exclude_unset=True)
        return update_subtitle_style(base_name, **data)
    except FileNotFoundError as exc:
        raise HTTPException(status_code=404, detail=str(exc)) from exc


@api_router.get("/projects/{base_name}/versions", response_model=List[ProjectVersionInfo])
def api_list_versions(base_name: str) -> List[ProjectVersionInfo]:
    return list_versions(base_name)


@api_router.post("/projects/{base_name}/versions/{version}/restore", response_model=ProjectMetadata)
def api_restore_version(base_name: str, version: int) -> ProjectMetadata:
    try:
        return restore_project_version(base_name, version)
    except FileNotFoundError as exc:
        raise HTTPException(status_code=404, detail=str(exc)) from exc


app.include_router(api_router)


translator_router = APIRouter(prefix="/api/translator", tags=["translator"])


def _audio_path_to_url(audio_path: Optional[str]) -> Optional[str]:
    """Convert filesystem paths inside outputs/ to web URLs."""
    if audio_path is None:
        return None

    if isinstance(audio_path, (int, float)):
        return audio_path

    value = str(audio_path)
    if not value:
        return value

    if value.startswith("http://") or value.startswith("https://"):
        return value

    if value.startswith("/outputs/"):
        return value

    try:
        base_output_dir = translator_module.TRANSLATOR_DIR.parent
        path_obj = Path(value)
        relative_path = path_obj.relative_to(base_output_dir)
        return f"/outputs/{relative_path.as_posix()}"
    except ValueError:
        return value


def _convert_audio_paths(project: Optional[TranslatorProject]) -> Optional[TranslatorProject]:
    """Normalize audio paths in a translator project for web delivery."""
    if project is None:
        return None

    project.source_video = _audio_path_to_url(project.source_video)
    project.source_subtitle = _audio_path_to_url(project.source_subtitle)

    for segment in project.segments:
        segment.audio_path = _audio_path_to_url(segment.audio_path)

    if isinstance(project.extra, dict):
        if isinstance(project.extra.get("voice_path"), str):
            project.extra["voice_path"] = _audio_path_to_url(project.extra["voice_path"])
        if isinstance(project.extra.get("rendered_video_path"), str):
            project.extra["rendered_video_path"] = _audio_path_to_url(project.extra["rendered_video_path"])
        if isinstance(project.extra.get("audio_files"), dict):
            project.extra["audio_files"] = {
                key: _audio_path_to_url(value) if isinstance(value, str) else value
                for key, value in project.extra["audio_files"].items()
            }

    return project


def _convert_project_list(projects: List[TranslatorProject]) -> List[TranslatorProject]:
    return [_convert_audio_paths(project) for project in projects]


@translator_router.get("/downloads")
async def api_list_downloads() -> List[Dict[str, str]]:
    return downloads_listing()


@translator_router.get("/settings")
def api_get_translator_settings() -> Dict[str, Any]:
    return load_translator_settings()


@translator_router.post("/settings")
def api_save_translator_settings(payload: Dict[str, Any] = Body(...)) -> Dict[str, Any]:
    try:
        save_translator_settings(payload)
        return {"success": True, "message": "Settings saved successfully"}
    except Exception as exc:
        logger.exception("Failed to save translator settings")
        raise HTTPException(status_code=500, detail=str(exc)) from exc


@translator_router.get("/projects")
async def api_list_translator_projects():
    try:
        from ai_shorts_maker.translator import list_projects
        projects = await run_in_threadpool(list_projects)
        return _convert_project_list(projects)
    except Exception as exc:
        logger.exception("Failed to list translator projects")
        raise HTTPException(status_code=500, detail=str(exc)) from exc


@translator_router.post("/projects", response_model=TranslatorProject)
async def api_create_translator_project(payload: TranslatorProjectCreate) -> TranslatorProject:
    try:
        settings_to_save = {
            "target_lang": payload.target_lang,
            "translation_mode": payload.translation_mode,
            "tone_hint": payload.tone_hint,
        }
        save_translator_settings(settings_to_save)
        project = await run_in_threadpool(translator_create_project, payload)
        return _convert_audio_paths(project)
    except Exception as exc:
        logger.exception("Failed to create translator project")
        raise HTTPException(status_code=500, detail=str(exc)) from exc


@translator_router.get("/projects/{project_id}", response_model=TranslatorProject)
async def api_get_translator_project(project_id: str) -> TranslatorProject:
    try:
        project = await run_in_threadpool(translator_load_project, project_id)
        return _convert_audio_paths(project)
    except FileNotFoundError as exc:
        raise HTTPException(status_code=404, detail=str(exc)) from exc
    except ValueError as exc:
        raise HTTPException(status_code=400, detail=str(exc)) from exc


@translator_router.patch("/projects/{project_id}", response_model=TranslatorProject)
async def api_update_translator_project(
    project_id: str, payload: TranslatorProjectUpdate
) -> TranslatorProject:
    try:
        project = await run_in_threadpool(translator_update_project, project_id, payload)
        return _convert_audio_paths(project)
    except FileNotFoundError as exc:
        raise HTTPException(status_code=404, detail=str(exc)) from exc
    except ValueError as exc:
        raise HTTPException(status_code=400, detail=str(exc)) from exc


@translator_router.delete(
    "/projects/{project_id}",
    status_code=status.HTTP_204_NO_CONTENT,
    response_class=Response,
)
async def api_delete_translator_project(project_id: str) -> Response:
    try:
        await run_in_threadpool(translator_delete_project, project_id)
    except FileNotFoundError:
        pass  # Idempotent delete
    except Exception as exc:
        logger.exception("Failed to delete translator project %s", project_id)
        raise HTTPException(status_code=500, detail=str(exc)) from exc
    return Response(status_code=status.HTTP_204_NO_CONTENT)


@translator_router.post("/projects/{project_id}/add-subtitle-text")
async def api_add_subtitle_text(project_id: str, payload: Dict[str, Any] = Body(...)) -> Dict[str, Any]:
    """ìë§‰ í…ìŠ¤íŠ¸ë¥¼ íŒŒì‹±í•˜ì—¬ í”„ë¡œì íŠ¸ì— ì¶”ê°€"""
    try:
        subtitle_text = payload.get("subtitle_text", "")
        subtitle_format = payload.get("subtitle_format", "srt")  # srt ë˜ëŠ” vtt
        target_field = payload.get("target_field", "source_text")  # source_text ë˜ëŠ” commentary_korean
        selected_speakers = payload.get("selected_speakers", None)  # ì„ íƒëœ í™”ì ë¦¬ìŠ¤íŠ¸

        if not subtitle_text:
            raise ValueError("subtitle_text is required")

        # ìë§‰ íŒŒì‹±
        from ai_shorts_maker.translator import parse_subtitle_text_and_add_to_project

        result = await run_in_threadpool(
            parse_subtitle_text_and_add_to_project,
            project_id,
            subtitle_text,
            subtitle_format,
            target_field,
            selected_speakers
        )

        return {
            "success": True,
            "project_id": project_id,
            "segments_count": len(result["project"].segments) if result["project"] else 0,
            "added_count": result.get("added_count", 0),
            "replaced_count": result.get("replaced_count", 0),
            "removed_count": result.get("removed_count", 0),
            "target_field": target_field,
            "selected_speakers": selected_speakers
        }
    except Exception as exc:
        logger.exception("Failed to add subtitle text to project %s", project_id)
        raise HTTPException(status_code=500, detail=str(exc)) from exc


@translator_router.post("/projects/{project_id}/generate-commentary", response_model=TranslatorProject)
async def api_generate_ai_commentary(project_id: str) -> TranslatorProject:
    try:
        from ai_shorts_maker.translator import generate_ai_commentary_for_project
        project = await run_in_threadpool(generate_ai_commentary_for_project, project_id)
        return _convert_audio_paths(project)
    except Exception as exc:
        logger.exception("Failed to generate AI commentary for project %s", project_id)
        raise HTTPException(status_code=500, detail=str(exc)) from exc


@translator_router.post("/projects/{project_id}/generate-korean-commentary", response_model=TranslatorProject)
async def api_generate_korean_ai_commentary(project_id: str) -> TranslatorProject:
    try:
        from ai_shorts_maker.translator import generate_korean_ai_commentary_for_project
        project = await run_in_threadpool(generate_korean_ai_commentary_for_project, project_id)
        return _convert_audio_paths(project)
    except Exception as exc:
        logger.exception("Failed to generate Korean AI commentary for project %s", project_id)
        raise HTTPException(status_code=500, detail=str(exc)) from exc


@translator_router.post("/projects/{project_id}/translate", response_model=TranslatorProject)
async def api_translate_project(project_id: str) -> TranslatorProject:
    try:
        project = await run_in_threadpool(translate_project_segments, project_id)
        return _convert_audio_paths(project)
    except Exception as exc:
        logger.exception("Failed to run translation for project %s", project_id)
        raise HTTPException(status_code=500, detail=str(exc)) from exc


@translator_router.post("/projects/{project_id}/translate-segments", response_model=TranslatorProject)
async def api_translate_segments(project_id: str, payload: Dict[str, Any] = Body(...)) -> TranslatorProject:
    try:
        segment_ids = payload.get("segment_ids", [])
        target_lang = payload.get("target_lang", "ja")
        translation_mode = payload.get("translation_mode", "reinterpret")
        tone_hint = payload.get("tone_hint")

        from ai_shorts_maker.translator import translate_selected_segments

        project = await run_in_threadpool(
            translate_selected_segments,
            project_id,
            segment_ids,
            target_lang,
            translation_mode,
            tone_hint
        )
        return _convert_audio_paths(project)
    except Exception as exc:
        logger.exception("Failed to translate segments for project %s", project_id)
        raise HTTPException(status_code=500, detail=str(exc)) from exc


@translator_router.post("/projects/{project_id}/voice", response_model=TranslatorProject)
async def api_synthesize_voice(project_id: str) -> TranslatorProject:
    try:
        project = await run_in_threadpool(synthesize_voice_for_project, project_id)
        return _convert_audio_paths(project)
    except Exception as exc:
        logger.exception("Failed to run voice synthesis for project %s", project_id)
        raise HTTPException(status_code=500, detail=str(exc)) from exc


@translator_router.post("/projects/{project_id}/render", response_model=TranslatorProject)
async def api_render_project(project_id: str) -> TranslatorProject:
    try:
        project = await run_in_threadpool(render_translated_project, project_id)
        return _convert_audio_paths(project)
    except Exception as exc:
        logger.exception("Failed to run render for project %s", project_id)
        raise HTTPException(status_code=500, detail=str(exc)) from exc


@translator_router.get("/projects/{project_id}/versions")
async def api_list_translation_versions(project_id: str):
    try:
        return await run_in_threadpool(list_translation_versions, project_id)
    except Exception as exc:
        logger.exception("Failed to list translation versions for project %s", project_id)
        raise HTTPException(status_code=500, detail=str(exc)) from exc


@translator_router.get("/projects/{project_id}/versions/{version}")
async def api_get_translation_version(project_id: str, version: int):
    try:
        result = await run_in_threadpool(load_translation_version, project_id, version)
        if result is None:
            raise HTTPException(status_code=404, detail=f"Version {version} not found")
        return result
    except HTTPException:
        raise
    except Exception as exc:
        logger.exception("Failed to load translation version %s for project %s", version, project_id)
        raise HTTPException(status_code=500, detail=str(exc)) from exc


@translator_router.post("/projects/{project_id}/reverse-translate")
async def api_reverse_translate(project_id: str, payload: Dict[str, Any] = Body(...)):
    try:
        segment_id = payload.get("segment_id")
        japanese_text = payload.get("japanese_text", "").strip()

        if not japanese_text:
            raise HTTPException(status_code=400, detail="Japanese text is required")

        # Use the existing translate_project_segments function with reverse parameters
        from ai_shorts_maker.translator import translate_text, update_segment_text

        korean_text = await run_in_threadpool(
            translate_text,
            japanese_text,
            target_lang="ko",  # Japanese to Korean
            translation_mode="literal",
            tone_hint=None
        )

        # Update segment with reverse translated text
        if segment_id:
            await run_in_threadpool(
                update_segment_text,
                project_id,
                segment_id,
                "reverse_translated",
                korean_text
            )

        return {"korean_text": korean_text}

    except Exception as exc:
        logger.exception("Failed to reverse translate text for project %s", project_id)
        raise HTTPException(status_code=500, detail=str(exc)) from exc


@translator_router.post("/projects/{project_id}/reverse-translate-segments", response_model=TranslatorProject)
async def api_reverse_translate_segments(project_id: str, payload: Dict[str, Any] = Body(...)) -> TranslatorProject:
    """ì„ íƒëœ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì¼ë³¸ì–´ì—ì„œ í•œêµ­ì–´ë¡œ ì—­ë²ˆì—­í•©ë‹ˆë‹¤."""
    try:
        from ai_shorts_maker.translator import translate_selected_segments

        segment_ids = payload.get("segment_ids", [])

        updated_project = await run_in_threadpool(
            translate_selected_segments,
            project_id=project_id,
            segment_ids=segment_ids,
            target_lang="ko",
            translation_mode="literal",
            tone_hint=None
        )

        return _convert_audio_paths(updated_project)

    except FileNotFoundError as exc:
        logger.exception("Project %s not found", project_id)
        raise HTTPException(status_code=404, detail=f"Translator project '{project_id}' not found.") from exc
    except Exception as exc:
        logger.exception("Failed to reverse translate segments for project %s", project_id)
        raise HTTPException(status_code=500, detail=f"Failed to reverse translate segments: {str(exc)}") from exc


@translator_router.patch("/projects/{project_id}/segments")
async def api_update_segment_text(project_id: str, payload: Dict[str, Any] = Body(...)):
    try:
        segment_id = payload.get("segment_id")
        text_type = payload.get("text_type")
        text_value = payload.get("text_value", "")

        if not segment_id or not text_type:
            raise HTTPException(status_code=400, detail="segment_id and text_type are required")

        from ai_shorts_maker.translator import update_segment_text

        await run_in_threadpool(update_segment_text, project_id, segment_id, text_type, text_value)

        return {"success": True}

    except Exception as exc:
        logger.exception("Failed to update segment text for project %s", project_id)
        raise HTTPException(status_code=500, detail=str(exc)) from exc


@translator_router.post("/projects/{project_id}/reorder-segments")
async def api_reorder_segments(project_id: str, payload: Dict[str, Any] = Body(...)):
    try:
        segment_orders = payload.get("segment_orders")

        if not segment_orders:
            raise HTTPException(status_code=400, detail="segment_orders is required")

        from ai_shorts_maker.translator import reorder_project_segments

        project = await run_in_threadpool(reorder_project_segments, project_id, segment_orders)
        return _convert_audio_paths(project)

    except Exception as exc:
        logger.exception("Failed to reorder segments for project %s", project_id)
        raise HTTPException(status_code=500, detail=str(exc)) from exc


@translator_router.post("/projects/{project_id}/apply-saved-result", response_model=TranslatorProject)
async def api_apply_saved_result(project_id: str, payload: Dict[str, Any] = Body(...)) -> TranslatorProject:
    saved_result = payload.get("saved_result")
    if not isinstance(saved_result, dict):
        raise HTTPException(status_code=400, detail="saved_result ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤")

    try:
        project = await run_in_threadpool(apply_saved_result_to_project, project_id, saved_result)
        return _convert_audio_paths(project)
    except FileNotFoundError as exc:
        raise HTTPException(status_code=404, detail=str(exc)) from exc
    except ValueError as exc:
        raise HTTPException(status_code=400, detail=str(exc)) from exc
    except Exception as exc:  # pylint: disable=broad-except
        logger.exception("Failed to apply saved result to project %s", project_id)
        raise HTTPException(status_code=500, detail="ì¬í•´ì„ ê²°ê³¼ë¥¼ ì ìš©í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤") from exc


@translator_router.delete("/projects/{project_id}/segments/{segment_id}", response_model=TranslatorProject)
async def api_delete_segment(project_id: str, segment_id: str) -> TranslatorProject:
    try:
        project = await run_in_threadpool(delete_project_segment, project_id, segment_id)
        return _convert_audio_paths(project)
    except FileNotFoundError as exc:
        raise HTTPException(status_code=404, detail=str(exc)) from exc
    except ValueError as exc:
        raise HTTPException(status_code=404, detail=str(exc)) from exc
    except Exception as exc:  # pylint: disable=broad-except
        logger.exception("Failed to delete segment %s for project %s", segment_id, project_id)
        raise HTTPException(status_code=500, detail="ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì‚­ì œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤") from exc


@translator_router.get("/saved-results")
def api_list_saved_results() -> Dict[str, Any]:
    entries: List[Dict[str, Any]] = []
    for path in sorted(SAVED_RESULTS_DIR.glob("*.json")):
        try:
            data = json.loads(path.read_text(encoding="utf-8"))
        except Exception as exc:  # pylint: disable=broad-except
            logger.warning("Failed to read saved result %s: %s", path, exc)
            continue

        entries.append({
            "id": path.stem,
            "name": data.get("name"),
            "saved_at": data.get("saved_at"),
        })

    return {"results": entries}


@translator_router.get("/saved-results/{result_id}")
def api_get_saved_result(result_id: str) -> Dict[str, Any]:
    safe_id = ''.join(ch for ch in result_id if ch.isalnum() or ch in {'-', '_'})
    if not safe_id:
        raise HTTPException(status_code=400, detail="ì˜ëª»ëœ ì €ì¥ë³¸ ID ì…ë‹ˆë‹¤")

    path = SAVED_RESULTS_DIR / f"{safe_id}.json"
    if not path.exists():
        raise HTTPException(status_code=404, detail="ì €ì¥ë³¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    try:
        data = json.loads(path.read_text(encoding="utf-8"))
    except Exception as exc:  # pylint: disable=broad-except
        logger.exception("Failed to load saved result %s", path)
        raise HTTPException(status_code=500, detail="ì €ì¥ë³¸ì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤") from exc

    return {"result": data}


@translator_router.post("/projects/{project_id}/reset-to-translated")
async def api_reset_to_translated(project_id: str):
    try:
        from ai_shorts_maker.translator import reset_project_to_translated

        project = await run_in_threadpool(reset_project_to_translated, project_id)
        return _convert_audio_paths(project)

    except Exception as exc:
        logger.exception("Failed to reset project %s to translated state", project_id)
        raise HTTPException(status_code=500, detail=str(exc)) from exc


@translator_router.patch("/projects/{project_id}/segments/time")
async def api_update_segment_time(project_id: str, payload: Dict[str, Any] = Body(...)):
    try:
        segment_id = payload.get("segment_id")
        start_time = payload.get("start_time")
        end_time = payload.get("end_time")

        if not segment_id or start_time is None or end_time is None:
            raise HTTPException(status_code=400, detail="segment_id, start_time, and end_time are required")

        if start_time >= end_time:
            raise HTTPException(status_code=400, detail="start_time must be less than end_time")

        if start_time < 0 or end_time < 0:
            raise HTTPException(status_code=400, detail="times must be non-negative")

        from ai_shorts_maker.translator import update_segment_time

        await run_in_threadpool(update_segment_time, project_id, segment_id, float(start_time), float(end_time))

        return {"success": True}

    except Exception as exc:
        logger.exception("Failed to update segment time for project %s", project_id)
        raise HTTPException(status_code=500, detail=str(exc)) from exc


@translator_router.patch("/projects/{project_id}/voice-synthesis-mode")
async def api_update_voice_synthesis_mode(project_id: str, payload: Dict[str, Any] = Body(...)):
    try:
        voice_synthesis_mode = payload.get("voice_synthesis_mode")

        if not voice_synthesis_mode:
            raise HTTPException(status_code=400, detail="voice_synthesis_mode is required")

        if voice_synthesis_mode not in ["subtitle", "commentary", "both"]:
            raise HTTPException(status_code=400, detail="voice_synthesis_mode must be one of: subtitle, commentary, both")

        from ai_shorts_maker.translator import load_project, save_project

        project = await run_in_threadpool(load_project, project_id)
        project.voice_synthesis_mode = voice_synthesis_mode
        await run_in_threadpool(save_project, project)

        return {"success": True, "voice_synthesis_mode": voice_synthesis_mode}

    except Exception as exc:
        logger.exception("Failed to update voice synthesis mode for project %s", project_id)
        raise HTTPException(status_code=500, detail=str(exc)) from exc


@translator_router.post("/projects/{project_id}/clone")
async def api_clone_translator_project(project_id: str, payload: Dict[str, Any] = Body(default=None)):
    """ë²ˆì—­ê¸° í”„ë¡œì íŠ¸ë¥¼ ë³µì œí•˜ì—¬ ë°±ì—…ë³¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        new_name = None
        if payload:
            new_name = payload.get("new_name")

        from ai_shorts_maker.translator import clone_translator_project_with_name

        if new_name:
            project = await run_in_threadpool(clone_translator_project_with_name, project_id, new_name)
        else:
            project = await run_in_threadpool(clone_translator_project, project_id)
        return _convert_audio_paths(project)
    except FileNotFoundError as exc:
        raise HTTPException(status_code=404, detail=str(exc)) from exc
    except Exception as exc:
        logger.exception("Failed to clone translator project %s", project_id)
        raise HTTPException(status_code=500, detail=str(exc)) from exc


@translator_router.post("/projects/{project_id}/generate-selected-audio")
async def api_generate_selected_audio(project_id: str, payload: dict):
    """ì„ íƒëœ ì„¸ê·¸ë¨¼íŠ¸ë“¤ì˜ ìŒì„±ì„ ìƒì„±í•˜ê³ , ìë§‰ ì‹œê°„ì— ë§ì¶° ë¬´ìŒì„ í¬í•¨í•œ ìŒì„± íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    segment_ids = payload.get("segment_ids", [])
    voice = payload.get("voice", "nova")
    audio_format = payload.get("audio_format", "wav")
    task_id = payload.get("task_id")  # ì„ íƒì  task_id

    if not segment_ids:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="segment_ids is required"
        )

    # task_idê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„±
    if not task_id:
        task_id = f"{project_id}_selected_{len(segment_ids)}"

    try:
        from ai_shorts_maker.translator import generate_selected_audio_with_silence
        audio_path = await run_in_threadpool(generate_selected_audio_with_silence, project_id, segment_ids, voice, audio_format, task_id)
        return {"audio_path": _audio_path_to_url(audio_path), "task_id": task_id}
    except FileNotFoundError:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Translator project '{project_id}' not found."
        )
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )
    except Exception as e:
        logger.exception("Failed to generate selected audio for project %s", project_id)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to generate selected audio: {str(e)}"
        )


@translator_router.post("/projects/{project_id}/generate-all-audio", response_model=TranslatorProject)
async def api_generate_all_audio(project_id: str, payload: dict) -> TranslatorProject:
    """ëª¨ë“  ì„¸ê·¸ë¨¼íŠ¸ì— ëŒ€í•´ ìŒì„± íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    voice = payload.get("voice", "nova")
    audio_format = payload.get("audio_format", "wav")
    task_id = payload.get("task_id")  # ì„ íƒì  task_id

    try:
        from ai_shorts_maker.translator import generate_all_audio
        updated_project = await run_in_threadpool(generate_all_audio, project_id, voice, audio_format, task_id)
        return _convert_audio_paths(updated_project)
    except FileNotFoundError:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Translator project '{project_id}' not found."
        )
    except Exception as e:
        logger.exception("Failed to generate all audio for project %s", project_id)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to generate all audio: {str(e)}"
        )


@translator_router.get("/audio-progress/{task_id}")
async def api_get_audio_progress(task_id: str):
    """ìŒì„± ìƒì„± ì§„í–‰ë¥ ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
    try:
        from ai_shorts_maker.translator import get_audio_generation_progress
        progress = get_audio_generation_progress(task_id)
        if progress is None:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Task '{task_id}' not found."
            )
        return progress
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("Failed to get audio progress for task %s", task_id)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get audio progress: {str(e)}"
        )


@translator_router.get("/projects/{project_id}/load-generated-tracks")
async def api_load_generated_tracks(project_id: str):
    """ìƒì„±ëœ ìŒì„± íŠ¸ë™ íŒŒì¼ë“¤ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""
    try:
        from ai_shorts_maker.translator import TRANSLATOR_DIR
        import re

        project = await run_in_threadpool(translator_load_project, project_id)
        if not project:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Translator project '{project_id}' not found."
            )

        audio_dir = TRANSLATOR_DIR / project_id / "audio"
        if not audio_dir.exists():
            return {"tracks": []}

        # ìƒì„±ëœ ìŒì„± íŒŒì¼ ì°¾ê¸°
        tracks = []

        # 1. ì„ íƒ ì„¸ê·¸ë¨¼íŠ¸ ìŒì„± íŒŒì¼ë“¤ ì°¾ê¸° (selected_audio_*_segments.*)
        selected_audio_files = sorted(audio_dir.glob("selected_audio_*_segments.*"))
        for audio_file in selected_audio_files:
            try:
                output_dir = TRANSLATOR_DIR.parent
                relative_path = audio_file.relative_to(output_dir)
                audio_url = f"/outputs/{relative_path.as_posix()}"

                # íŒŒì¼ëª…ì—ì„œ ì„¸ê·¸ë¨¼íŠ¸ ê°œìˆ˜ ì¶”ì¶œ
                match = re.search(r"selected_audio_(\d+)_segments", audio_file.name)
                segment_count = int(match.group(1)) if match else 0

                tracks.append({
                    "type": "selected",
                    "path": audio_url,
                    "filename": audio_file.name,
                    "segment_count": segment_count,
                    "created_at": audio_file.stat().st_mtime,
                    "file_path": str(audio_file)  # ì‚­ì œë¥¼ ìœ„í•œ ì „ì²´ ê²½ë¡œ
                })
            except Exception as e:
                logger.warning(f"Failed to process audio file {audio_file}: {e}")

        return {"tracks": tracks}

    except HTTPException:
        raise
    except FileNotFoundError:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Translator project '{project_id}' not found."
        )
    except Exception as e:
        logger.exception("Failed to load generated tracks for project %s", project_id)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to load generated tracks: {str(e)}"
        )


@translator_router.delete("/projects/{project_id}/tracks")
async def api_delete_track(project_id: str, file_path: str = Body(..., embed=True)):
    """íŠ¸ë™ íŒŒì¼ì„ ì‚­ì œí•©ë‹ˆë‹¤."""
    try:
        from ai_shorts_maker.translator import TRANSLATOR_DIR
        from pathlib import Path

        # ë³´ì•ˆ: í”„ë¡œì íŠ¸ ë””ë ‰í„°ë¦¬ ë‚´ì˜ íŒŒì¼ë§Œ ì‚­ì œ ê°€ëŠ¥
        track_path = Path(file_path)
        project_audio_dir = TRANSLATOR_DIR / project_id / "audio"

        if not track_path.exists():
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Track file not found."
            )

        # ë³´ì•ˆ ì²´í¬: íŒŒì¼ì´ í”„ë¡œì íŠ¸ audio ë””ë ‰í„°ë¦¬ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸
        try:
            track_path.relative_to(project_audio_dir)
        except ValueError:
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="Cannot delete files outside project directory."
            )

        # íŒŒì¼ ì‚­ì œ
        track_path.unlink()
        logger.info(f"Deleted track file: {track_path}")

        return {"success": True, "message": "Track deleted successfully"}

    except HTTPException:
        raise
    except Exception as e:
        logger.exception("Failed to delete track for project %s", project_id)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to delete track: {str(e)}"
        )


app.include_router(translator_router)

# Import and include additional translator routes
from web_app.routers import translator as additional_translator_router
app.include_router(additional_translator_router.router)

# Video Editor Router
video_editor_router = APIRouter(prefix="/api/video-editor", tags=["video-editor"])


class VideoProcessRequest(BaseModel):
    project_id: str
    video_path: str
    template: str
    subtitle_type: str = "translated"  # translated, original, reverse, external
    external_subtitle_path: Optional[str] = None  # ì™¸ë¶€ ìë§‰ íŒŒì¼ ê²½ë¡œ


@video_editor_router.post("/process")
async def api_process_video(payload: VideoProcessRequest) -> Dict[str, Any]:
    try:
        # í”„ë¡œì íŠ¸ ë¡œë“œ
        project = await run_in_threadpool(translator_load_project, payload.project_id)

        # ì˜ìƒ ì²˜ë¦¬ ì‹¤í–‰
        result = await run_in_threadpool(
            process_video_with_subtitles,
            payload.project_id,
            payload.video_path,
            payload.template,
            payload.subtitle_type,
            project,
            payload.external_subtitle_path
        )

        return {"success": True, "result": result}
    except Exception as exc:
        logger.exception("Failed to process video for project %s", payload.project_id)
        return {"success": False, "error": str(exc)}


@video_editor_router.get("/subtitle-preview")
async def api_subtitle_preview(path: str) -> Dict[str, Any]:
    """ì™¸ë¶€ ìë§‰ íŒŒì¼ì˜ ë¯¸ë¦¬ë³´ê¸°ë¥¼ ì œê³µ"""
    try:
        subtitle_path = Path(path)

        if not subtitle_path.exists():
            return {"success": False, "error": "íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}

        if not subtitle_path.suffix.lower() in ['.srt', '.vtt', '.ass']:
            return {"success": False, "error": "ì§€ì›í•˜ì§€ ì•ŠëŠ” ìë§‰ íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤"}

        # íŒŒì¼ í¬ê¸° ì²´í¬ (10MB ì œí•œ)
        if subtitle_path.stat().st_size > 10 * 1024 * 1024:
            return {"success": False, "error": "íŒŒì¼ì´ ë„ˆë¬´ í½ë‹ˆë‹¤"}

        # íŒŒì¼ ë‚´ìš© ì½ê¸° (ì²˜ìŒ 1000ìë§Œ)
        with open(subtitle_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read(1000)

        # ì¤„ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ì²˜ìŒ 10ì¤„ë§Œ ë³´ì—¬ì£¼ê¸°
        lines = content.split('\n')[:10]
        preview = '\n'.join(lines)

        if len(content) >= 1000:
            preview += "\n\n... (ë” ë§ì€ ë‚´ìš©ì´ ìˆìŠµë‹ˆë‹¤)"

        return {"success": True, "preview": preview}

    except Exception as exc:
        logger.exception("Failed to load subtitle preview")
        return {"success": False, "error": str(exc)}


app.include_router(video_editor_router)


def process_video_with_subtitles(project_id: str, video_path: str, template: str, subtitle_type: str, project: Any, external_subtitle_path: Optional[str] = None) -> Dict[str, Any]:
    """ì˜ìƒì— ë²ˆì—­ëœ ìë§‰ì„ í•©ì„±í•˜ëŠ” í•¨ìˆ˜"""
    from pathlib import Path
    import tempfile
    import os

    try:
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        output_dir = PACKAGE_DIR / "outputs" / "video_editor"
        output_dir.mkdir(parents=True, exist_ok=True)

        # SRT íŒŒì¼ ìƒì„± (ì„ íƒëœ ìë§‰ íƒ€ì… ì‚¬ìš©)
        if subtitle_type == "external" and external_subtitle_path:
            # ì™¸ë¶€ ìë§‰ íŒŒì¼ ì‚¬ìš©
            srt_file_path = external_subtitle_path
        else:
            # í”„ë¡œì íŠ¸ ìë§‰ ì‚¬ìš©
            srt_content = generate_srt_from_project(project, subtitle_type)

            # ì„ì‹œ SRT íŒŒì¼ ìƒì„±
            with tempfile.NamedTemporaryFile(mode='w', suffix='.srt', delete=False, encoding='utf-8') as srt_file:
                srt_file.write(srt_content)
                srt_file_path = srt_file.name

        # ì¶œë ¥ íŒŒì¼ëª… ìƒì„±
        video_name = Path(video_path).stem
        output_filename = f"{project_id}_{video_name}_{template}.mp4"
        output_path = output_dir / output_filename

        # FFmpeg ëª…ë ¹ì–´ êµ¬ì„± (í…œí”Œë¦¿ì— ë”°ë¥¸ ìë§‰ ìŠ¤íƒ€ì¼)
        project_language = getattr(project, "target_lang", None) or getattr(project, "language", None)
        subtitle_style = get_subtitle_style_for_template(template, project_language)

        ffmpeg_cmd = [
            'ffmpeg', '-y',  # -y: ì¶œë ¥ íŒŒì¼ ë®ì–´ì“°ê¸°
            '-i', video_path,  # ì…ë ¥ ì˜ìƒ
            '-vf', f"subtitles={srt_file_path}:force_style='{subtitle_style}'",  # ìë§‰ í•„í„°
            '-c:a', 'copy',  # ì˜¤ë””ì˜¤ ë³µì‚¬
            str(output_path)  # ì¶œë ¥ ê²½ë¡œ
        ]

        # FFmpeg ì‹¤í–‰
        logger.info(f"Running FFmpeg command: {' '.join(ffmpeg_cmd)}")
        result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True, check=True)

        # ì„ì‹œ íŒŒì¼ ì •ë¦¬ (ì™¸ë¶€ íŒŒì¼ì´ ì•„ë‹Œ ê²½ìš°ë§Œ)
        if subtitle_type != "external":
            os.unlink(srt_file_path)

        return {
            "output_path": str(output_path),
            "output_filename": output_filename,
            "template_used": template,
            "video_duration": get_video_duration(video_path)
        }

    except subprocess.CalledProcessError as exc:
        logger.error(f"FFmpeg error: {exc.stderr}")
        raise RuntimeError(f"ì˜ìƒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {exc.stderr}")
    except Exception as exc:
        logger.exception("Video processing failed")
        raise RuntimeError(f"ì˜ìƒ ì²˜ë¦¬ ì‹¤íŒ¨: {str(exc)}")


def generate_srt_from_project(project: Any, subtitle_type: str = "translated") -> str:
    """í”„ë¡œì íŠ¸ì˜ ì„¸ê·¸ë¨¼íŠ¸ì—ì„œ SRT í˜•ì‹ì˜ ìë§‰ ìƒì„±"""
    srt_lines = []

    for i, segment in enumerate(project.segments, 1):
        # ì„¸ê·¸ë¨¼íŠ¸ ì†ì„± ì´ë¦„ í™•ì¸ (start/end vs start_time/end_time)
        start_time = getattr(segment, 'start', getattr(segment, 'start_time', 0))
        end_time = getattr(segment, 'end', getattr(segment, 'end_time', 1))

        # ì„ íƒëœ ìë§‰ íƒ€ì…ì— ë”°ë¼ í…ìŠ¤íŠ¸ ì„ íƒ
        if subtitle_type == "original":
            text = getattr(segment, 'source_text', None)
        elif subtitle_type == "reverse":
            text = getattr(segment, 'reverse_translated_text', None)
        else:  # translated (ê¸°ë³¸ê°’)
            text = getattr(segment, 'translated_text', None)

        # í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ë‹¤ë¥¸ í…ìŠ¤íŠ¸ë¡œ ëŒ€ì²´
        if not text:
            text = getattr(segment, 'translated_text', None) or \
                   getattr(segment, 'source_text', None) or \
                   getattr(segment, 'original_text', '(ìë§‰ ì—†ìŒ)')

        # ì‹œê°„ì„ SRT í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (HH:MM:SS,mmm)
        start_srt = format_time_for_srt(start_time)
        end_srt = format_time_for_srt(end_time)

        srt_lines.append(str(i))
        srt_lines.append(f"{start_srt} --> {end_srt}")
        srt_lines.append(text)
        srt_lines.append("")  # ë¹ˆ ì¤„

    return "\n".join(srt_lines)


def format_time_for_srt(seconds: float) -> str:
    """ì´ˆë¥¼ SRT í˜•ì‹ì˜ íƒ€ì„ì½”ë“œë¡œ ë³€í™˜"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    millisecs = int((seconds - int(seconds)) * 1000)

    return f"{hours:02d}:{minutes:02d}:{secs:02d},{millisecs:03d}"


def _font_family_installed(family: str) -> bool:
    """fontconfigë¥¼ í†µí•´ ì§€ì •í•œ í°íŠ¸ íŒ¨ë°€ë¦¬ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
    try:
        result = subprocess.run(
            ["fc-match", "--format=%{family}", family],
            capture_output=True,
            text=True,
            check=False,
        )
    except FileNotFoundError:
        # fontconfigê°€ ì—†ëŠ” í™˜ê²½ì—ì„œëŠ” í™•ì¸ ë¶ˆê°€ì´ì§€ë§Œ, ì¼ë‹¨ ì‹œë„í•œë‹¤.
        return True
    matched = (result.stdout or "").strip()
    return bool(matched)


def _detect_font_family(language: Optional[str]) -> Optional[str]:
    """ìë§‰ ì–¸ì–´ë³„ë¡œ ì í•©í•œ í°íŠ¸ íŒ¨ë°€ë¦¬ë¥¼ íƒìƒ‰"""
    lang = (language or "").lower()
    candidates: List[str] = []

    if lang.startswith("ja"):
        candidates.extend(
            [
                "Noto Sans CJK JP",
                "Noto Sans JP",
                "Noto Serif CJK JP",
                "Noto Serif JP",
                "Droid Sans Japanese",
                "IPAGothic",
                "TakaoPGothic",
            ]
        )
    elif lang.startswith("ko"):
        candidates.extend(
            [
                "Noto Sans CJK KR",
                "NanumGothic",
                "NanumSquare",
                "NanumSquareRound",
                "NanumGothicCoding",
            ]
        )
    else:
        candidates.extend(
            [
                "Noto Sans CJK KR",
                "Noto Sans CJK JP",
                "DejaVu Sans",
            ]
        )

    # ëª¨ë“  ì–¸ì–´ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì¼ë°˜ í›„ë³´ ì¶”ê°€
    candidates.extend(
        [
            "Noto Sans CJK KR",
            "Noto Sans CJK JP",
            "Noto Sans CJK SC",
        ]
    )

    for family in candidates:
        if _font_family_installed(family):
            return family
    return None


def get_subtitle_style_for_template(template: str, language: Optional[str] = None) -> str:
    """í…œí”Œë¦¿ì— ë”°ë¥¸ ìë§‰ ìŠ¤íƒ€ì¼ ë°˜í™˜"""
    styles = {
        "classic": "FontSize=24,PrimaryColour=&Hffffff,OutlineColour=&H000000,BackColour=&H80000000,Bold=1,Outline=2,Shadow=1,Alignment=2,MarginV=20",
        "banner": "FontSize=26,PrimaryColour=&Hffffff,OutlineColour=&H000000,BackColour=&H80ff0000,Bold=1,Outline=2,Shadow=1,Alignment=2,MarginV=300",
    }
    base_style = styles.get(template, styles["classic"])
    font_family = _detect_font_family(language)
    if font_family:
        return f"{base_style},FontName={font_family}"
    return base_style


def get_video_duration(video_path: str) -> Optional[float]:
    """ì˜ìƒì˜ ì§€ì†ì‹œê°„ì„ ì´ˆ ë‹¨ìœ„ë¡œ ë°˜í™˜"""
    try:
        import subprocess
        cmd = ['ffprobe', '-v', 'quiet', '-print_format', 'json', '-show_format', video_path]
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        import json
        data = json.loads(result.stdout)
        return float(data['format']['duration'])
    except Exception as exc:
        logger.warning(f"Failed to get video duration: {exc}")
        return None


def default_ytdl_form() -> Dict[str, Any]:
    settings = load_ytdl_settings()
    return {
        "urls": "",
        **settings,
    }


def load_ytdl_settings() -> Dict[str, Any]:
    settings = DEFAULT_YTDL_SETTINGS.copy()
    if YTDL_SETTINGS_PATH.exists():
        try:
            data = json.loads(YTDL_SETTINGS_PATH.read_text(encoding="utf-8"))
            if isinstance(data, dict):
                for key, value in data.items():
                    if key in settings:
                        if isinstance(settings[key], bool):
                            settings[key] = bool(value)
                        else:
                            settings[key] = value
        except (OSError, json.JSONDecodeError) as exc:
            logger.warning("Failed to load YTDL settings: %s", exc)
    return settings


def save_ytdl_settings(values: Dict[str, Any]) -> None:
    payload = {}
    for key in DEFAULT_YTDL_SETTINGS:
        if key not in values:
            continue
        original = DEFAULT_YTDL_SETTINGS[key]
        value = values[key]
        if isinstance(original, bool):
            payload[key] = bool(value)
        else:
            payload[key] = value
    try:
        YTDL_SETTINGS_PATH.write_text(
            json.dumps(payload, ensure_ascii=False, indent=2),
            encoding="utf-8",
        )
    except OSError as exc:
        logger.warning("Failed to save YTDL settings: %s", exc)


def load_download_history() -> List[Dict[str, Any]]:
    """Load download history from JSON file."""
    if not YTDL_HISTORY_PATH.exists():
        return []
    try:
        data = json.loads(YTDL_HISTORY_PATH.read_text(encoding="utf-8"))
        return data if isinstance(data, list) else []
    except (OSError, json.JSONDecodeError) as exc:
        logger.warning("Failed to load download history: %s", exc)
        return []


def save_download_history(history: List[Dict[str, Any]]) -> None:
    """Save download history to JSON file."""
    try:
        YTDL_HISTORY_PATH.write_text(
            json.dumps(history, ensure_ascii=False, indent=2),
            encoding="utf-8",
        )
    except OSError as exc:
        logger.warning("Failed to save download history: %s", exc)


def extract_audio_sources(
    video_files: List[Path],
    extract_vocal: bool,
    extract_instruments: bool,
    progress_callback: Optional[Callable[[int, int, str], None]] = None
) -> List[Path]:
    """
    Extract vocal and/or instruments from video files using demucs.

    Args:
        video_files: List of video file paths
        extract_vocal: Whether to extract vocal track
        extract_instruments: Whether to extract instruments track
        progress_callback: Optional callback(current, total, message)

    Returns:
        List of extracted audio file paths
    """
    import subprocess

    extracted_files = []

    # ë¹„ë””ì˜¤ íŒŒì¼ë§Œ í•„í„°ë§ (ìë§‰ íŒŒì¼ ì œì™¸)
    video_extensions = {".mp4", ".webm", ".mkv", ".avi", ".mov"}
    videos = [f for f in video_files if f.suffix.lower() in video_extensions]

    total_files = len(videos)

    for idx, video_path in enumerate(videos, 1):
        try:
            if progress_callback:
                progress_callback(idx, total_files, f"ì˜¤ë””ì˜¤ ì¶”ì¶œ ì¤‘ ({idx}/{total_files}): {video_path.name}")

            # ë¨¼ì € ë¹„ë””ì˜¤ì—ì„œ ì˜¤ë””ì˜¤ ì¶”ì¶œ
            audio_path = video_path.with_suffix(".wav")

            # ffmpegë¡œ ì˜¤ë””ì˜¤ ì¶”ì¶œ
            cmd = [
                "ffmpeg",
                "-i", str(video_path),
                "-vn",  # ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì œì™¸
                "-acodec", "pcm_s16le",  # WAV format
                "-ar", "44100",  # ìƒ˜í”Œë§ ë ˆì´íŠ¸
                "-ac", "2",  # ìŠ¤í…Œë ˆì˜¤
                "-y",  # ë®ì–´ì“°ê¸°
                str(audio_path),
            ]

            subprocess.run(cmd, check=True, capture_output=True)

            if progress_callback:
                progress_callback(idx, total_files, f"Vocal/Instruments ë¶„ë¦¬ ì¤‘ ({idx}/{total_files}): {video_path.name}")

            # demucsë¡œ ì˜¤ë””ì˜¤ ë¶„ë¦¬
            output_dir = video_path.parent / "separated"
            output_dir.mkdir(exist_ok=True)

            demucs_cmd = [
                "demucs",
                "--two-stems", "vocals",  # vocalê³¼ ë‚˜ë¨¸ì§€(instruments)ë¡œ ë¶„ë¦¬
                "-o", str(output_dir),
                str(audio_path),
            ]

            try:
                subprocess.run(demucs_cmd, check=True, capture_output=True)

                # demucs ì¶œë ¥ ê²½ë¡œ (htdemucs ëª¨ë¸ ì‚¬ìš©)
                stem_dir = output_dir / "htdemucs" / audio_path.stem

                if extract_vocal:
                    vocal_file = stem_dir / "vocals.wav"
                    if vocal_file.exists():
                        # íŒŒì¼ ì´ë¦„ ë³€ê²½ (ğŸ¤ ì´ëª¨ì§€ë¡œ vocal ëª…í™•íˆ í‘œì‹œ)
                        final_vocal = video_path.with_name(
                            f"{video_path.stem}__ğŸ¤VOCAL.wav"
                        )
                        vocal_file.rename(final_vocal)
                        extracted_files.append(final_vocal)

                if extract_instruments:
                    instruments_file = stem_dir / "no_vocals.wav"
                    if instruments_file.exists():
                        # íŒŒì¼ ì´ë¦„ ë³€ê²½ (ğŸ¸ ì´ëª¨ì§€ë¡œ instruments ëª…í™•íˆ í‘œì‹œ)
                        final_instruments = video_path.with_name(
                            f"{video_path.stem}__ğŸ¸INSTRUMENTS.wav"
                        )
                        instruments_file.rename(final_instruments)
                        extracted_files.append(final_instruments)

                # ì •ë¦¬: separated í´ë” ì‚­ì œ
                import shutil
                if output_dir.exists():
                    shutil.rmtree(output_dir)

            except subprocess.CalledProcessError:
                logger.warning(f"demucs not available, skipping audio separation for {video_path}")

            # ì„ì‹œ ì˜¤ë””ì˜¤ íŒŒì¼ ì‚­ì œ
            if audio_path.exists():
                audio_path.unlink()

        except Exception as e:
            logger.warning(f"Failed to extract audio from {video_path}: {e}")
            continue

    return extracted_files


def add_to_download_history(urls: List[str], files: List[Path], settings: Dict[str, Any]) -> None:
    """Add download record to history."""
    history = load_download_history()

    download_record = {
        "timestamp": datetime.now().isoformat(),
        "urls": urls,
        "files": [str(f) for f in files],
        "settings": {
            "output_dir": settings.get("output_dir"),
            "sub_langs": settings.get("sub_langs"),
            "download_subs": settings.get("download_subs"),
            "auto_subs": settings.get("auto_subs"),
        }
    }

    history.insert(0, download_record)  # Add to beginning
    # Keep only last 100 records
    history = history[:100]

    save_download_history(history)


def delete_download_files(file_paths: List[str]) -> Dict[str, Any]:
    """Delete downloaded files and return result."""
    deleted = []
    errors = []

    for file_path in file_paths:
        try:
            path = Path(file_path)
            if path.exists():
                path.unlink()
                deleted.append(file_path)

                # Also try to delete related files (subtitles, etc.)
                base_name = path.stem
                parent_dir = path.parent
                for related_file in parent_dir.glob(f"{base_name}.*"):
                    if related_file != path and related_file.exists():
                        related_file.unlink()
                        deleted.append(str(related_file))
            else:
                errors.append(f"File not found: {file_path}")
        except Exception as exc:
            errors.append(f"Error deleting {file_path}: {str(exc)}")

    return {"deleted": deleted, "errors": errors}


TRANSLATOR_SETTINGS_PATH = BASE_DIR / "translator_settings.json"
DEFAULT_TRANSLATOR_SETTINGS: Dict[str, Any] = {
    "target_lang": "ja",
    "translation_mode": "reinterpret",
    "tone_hint": "ë“œë¼ë§ˆí•˜ê³  ìœ ì¾Œí•˜ë¨¼ì„œ ìœ ë¨¸ëŸ¬ìŠ¤í•˜ê²Œ",
}

def load_translator_settings() -> Dict[str, Any]:
    settings = DEFAULT_TRANSLATOR_SETTINGS.copy()
    if TRANSLATOR_SETTINGS_PATH.exists():
        try:
            data = json.loads(TRANSLATOR_SETTINGS_PATH.read_text(encoding="utf-8"))
            if isinstance(data, dict):
                settings.update(data)
        except (OSError, json.JSONDecodeError) as exc:
            logger.warning("Failed to load Translator settings: %s", exc)
    return settings

def save_translator_settings(values: Dict[str, Any]) -> None:
    payload = {}
    for key in DEFAULT_TRANSLATOR_SETTINGS:
        if key in values and values[key] is not None:
            payload[key] = values[key]
    try:
        TRANSLATOR_SETTINGS_PATH.write_text(
            json.dumps(payload, ensure_ascii=False, indent=2),
            encoding="utf-8",
        )
    except OSError as exc:
        logger.warning("Failed to save Translator settings: %s", exc)


def _split_urls(raw: str) -> List[str]:
    return [line.strip() for line in raw.replace("\r", "\n").splitlines() if line.strip()]


@app.get("/text-removal", response_class=HTMLResponse)
async def text_removal_page(request: Request) -> HTMLResponse:
    context = {
        "request": request,
        "nav_active": "text_removal",
        "torch_cuda_available": torch.cuda.is_available() if torch is not None else False,
    }
    return templates.TemplateResponse("text_removal.html", context)


@app.post("/api/text-removal/preview")
async def text_removal_preview(video: UploadFile = File(...)) -> Dict[str, Any]:
    """Store uploaded video and return the first-frame preview."""

    if cv2 is None:
        raise HTTPException(
            status_code=500,
            detail="OpenCV(opencv-contrib-python) íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.",
        )

    if not video.filename:
        raise HTTPException(status_code=400, detail="ì—…ë¡œë“œí•  ì˜ìƒ íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”.")

    session_id = uuid4().hex
    session_dir = TEXT_REMOVAL_UPLOAD_DIR / session_id
    session_dir.mkdir(parents=True, exist_ok=True)

    suffix = Path(video.filename).suffix or ".mp4"
    input_path = session_dir / f"input{suffix}"

    try:
        file_bytes = await video.read()
        input_path.write_bytes(file_bytes)
    except Exception as exc:  # pragma: no cover - filesystem failure
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì˜ìƒ íŒŒì¼ì„ ì €ì¥í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {exc}",
        ) from exc

    try:
        frame, fps, frame_count, width, height, original_path, processed_path = prepare_video_preview(
            input_path, session_dir
        )
    except RuntimeError as exc:
        raise HTTPException(status_code=400, detail=str(exc)) from exc

    success, buffer = cv2.imencode(".png", frame)
    if not success:
        raise HTTPException(status_code=500, detail="ë¯¸ë¦¬ë³´ê¸° ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    preview_image = base64.b64encode(buffer).decode("ascii")

    meta = {
        "input_path": str(original_path),
        "processed_path": str(processed_path),
        "original_filename": video.filename,
        "fps": fps,
        "frame_count": frame_count,
        "width": width,
        "height": height,
        "transcoded": processed_path != original_path,
    }
    (session_dir / "meta.json").write_text(
        json.dumps(meta, ensure_ascii=False, indent=2),
        encoding="utf-8",
    )

    return {
        "session_id": session_id,
        "preview_image": f"data:image/png;base64,{preview_image}",
        "fps": fps,
        "frame_count": frame_count,
        "width": width,
        "height": height,
        "message": "ë¯¸ë¦¬ë³´ê¸°ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.",
        "transcoded": processed_path != original_path,
    }


@app.post("/api/text-removal/process")
async def text_removal_process(payload: TextRemovalProcessRequest) -> Dict[str, Any]:
    if cv2 is None:
        raise HTTPException(
            status_code=500,
            detail="OpenCV(opencv-contrib-python) íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.",
        )

    session_dir = TEXT_REMOVAL_UPLOAD_DIR / payload.session_id
    if not session_dir.exists():
        raise HTTPException(status_code=404, detail="ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì˜ìƒì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")

    meta_path = session_dir / "meta.json"
    if not meta_path.exists():
        raise HTTPException(status_code=400, detail="ì„¸ì…˜ ì •ë³´ê°€ ì†ìƒë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")

    try:
        meta = json.loads(meta_path.read_text(encoding="utf-8"))
    except json.JSONDecodeError as exc:
        raise HTTPException(status_code=400, detail="ì„¸ì…˜ ì •ë³´ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.") from exc

    processed_path_str = meta.get("processed_path") or meta.get("input_path")
    if not processed_path_str:
        raise HTTPException(status_code=400, detail="ì—…ë¡œë“œëœ ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    source_path = Path(processed_path_str)
    if not source_path.exists():
        source_path = Path(meta.get("input_path", processed_path_str))
    if not source_path.exists():
        raise HTTPException(status_code=400, detail="ì—…ë¡œë“œëœ ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    trim_start = float(meta.get("trim_start", 0.0) or 0.0)
    trim_duration = float(meta.get("trim_duration", 4.0) or 4.0)
    trimmed_path_str = meta.get("trimmed_path")
    trimmed_path = Path(trimmed_path_str) if trimmed_path_str else session_dir / "input_trimmed.mp4"

    effective_source = source_path
    if not trimmed_path.exists():
        try:
            trimmed_path, effective_source = trim_video_clip(
                source_path, trimmed_path, duration=trim_duration, start=trim_start
            )
        except Exception as exc:  # pylint: disable=broad-except
            logger.exception("Failed to trim video clip: %s", exc)
            raise HTTPException(
                status_code=500,
                detail=f"ì˜ìƒ íŠ¸ë¦¬ë°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {exc}",
            ) from exc

    if trimmed_path.exists() and trimmed_path.stat().st_size == 0:
        trimmed_path.unlink(missing_ok=True)
        raise HTTPException(
            status_code=500,
            detail="íŠ¸ë¦¬ë°ëœ í´ë¦½ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ì‹œì‘ ì‹œê°„ê³¼ ê¸¸ì´ë¥¼ ì¡°ì •í•´ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.",
        )

    if effective_source != source_path:
        meta["processed_path"] = str(effective_source)

    if trimmed_path.exists():
        meta.update(
            {
                "trimmed_path": str(trimmed_path),
                "trim_start": trim_start,
                "trim_duration": trim_duration,
            }
        )
        meta_path.write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8")

    video_path = trimmed_path if trimmed_path.exists() else effective_source

    if not payload.boxes:
        raise HTTPException(status_code=400, detail="ì¸í˜ì¸íŒ…í•  ì˜ì—­ì„ ìµœì†Œ 1ê°œ ì´ìƒ ì§€ì •í•˜ì„¸ìš”.")

    output_path = session_dir / "restored.mp4"

    effective_max_frames = payload.max_frames if payload.max_frames and payload.max_frames > 0 else None

    config = RemovalConfig(
        video_path=video_path,
        output_path=output_path,
        boxes=[(box.x, box.y, box.width, box.height) for box in payload.boxes],
        model_id=payload.model_id,
        prompt=payload.prompt,
        negative_prompt=payload.negative_prompt,
        strength=payload.strength,
        guidance_scale=payload.guidance_scale,
        num_inference_steps=payload.num_inference_steps,
        dilate_radius=payload.dilate,
        device=payload.device,
        dtype=payload.dtype,
        seed=payload.seed,
        fps_override=payload.fps,
        max_frames=effective_max_frames,
    )

    try:
        await run_in_threadpool(run_text_removal, config)
    except DiffusionUnavailableError as exc:
        raise HTTPException(status_code=500, detail=str(exc)) from exc
    except TrackerUnavailableError as exc:
        raise HTTPException(status_code=500, detail=str(exc)) from exc
    except Exception as exc:  # pylint: disable=broad-except
        logger.exception("Text removal failed: %s", exc)
        raise HTTPException(
            status_code=500,
            detail=f"ì˜ìƒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {exc}",
        ) from exc

    if effective_source != source_path:
        meta["processed_path"] = str(effective_source)

    meta.update(
        {
            "output_path": str(output_path),
            "prompt": payload.prompt,
            "negative_prompt": payload.negative_prompt,
            "boxes": [box.model_dump() for box in payload.boxes],
            "strength": payload.strength,
            "guidance_scale": payload.guidance_scale,
            "num_inference_steps": payload.num_inference_steps,
            "dilate": payload.dilate,
            "max_frames": effective_max_frames,
            "trimmed_path": str(trimmed_path),
            "trim_start": trim_start,
            "trim_duration": trim_duration,
        }
    )
    meta_path.write_text(
        json.dumps(meta, ensure_ascii=False, indent=2),
        encoding="utf-8",
    )

    return {
        "session_id": payload.session_id,
        "video_url": f"/api/text-removal/download/{payload.session_id}",
        "output_path": str(output_path),
        "message": "ì˜ìƒ ì¸í˜ì¸íŒ…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
    }


@app.post("/api/text-removal/fill-background")
async def text_removal_fill_background(payload: TextRemovalFillRequest) -> Dict[str, Any]:
    if cv2 is None:
        raise HTTPException(
            status_code=500,
            detail="OpenCV(opencv-contrib-python) íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.",
        )

    session_dir = TEXT_REMOVAL_UPLOAD_DIR / payload.session_id
    if not session_dir.exists():
        raise HTTPException(status_code=404, detail="ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì˜ìƒì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")

    meta_path = session_dir / "meta.json"
    if not meta_path.exists():
        raise HTTPException(status_code=400, detail="ì„¸ì…˜ ì •ë³´ê°€ ì†ìƒë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")

    try:
        meta = json.loads(meta_path.read_text(encoding="utf-8"))
    except json.JSONDecodeError as exc:
        raise HTTPException(status_code=400, detail="ì„¸ì…˜ ì •ë³´ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.") from exc

    processed_path_str = meta.get("processed_path") or meta.get("input_path")
    if not processed_path_str:
        raise HTTPException(status_code=400, detail="ì—…ë¡œë“œëœ ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    video_path = Path(meta.get("trimmed_path") or processed_path_str)
    if not video_path.exists():
        video_path = Path(processed_path_str)
    if not video_path.exists():
        raise HTTPException(status_code=400, detail="ì—…ë¡œë“œëœ ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    if not payload.boxes:
        raise HTTPException(status_code=400, detail="ê°€ë¦´ ì˜ì—­ì„ ìµœì†Œ í•œ ê°œ ì´ìƒ ì§€ì •í•˜ì„¸ìš”.")

    output_path = session_dir / "background_filled.mp4"

    effective_max_frames = payload.max_frames if payload.max_frames and payload.max_frames > 0 else None

    config = BackgroundFillConfig(
        video_path=video_path,
        output_path=output_path,
        boxes=[(box.x, box.y, box.width, box.height) for box in payload.boxes],
        dilate_radius=max(int(payload.dilate or 0), 0),
        fps_override=payload.fps,
        max_frames=effective_max_frames,
    )

    try:
        await run_in_threadpool(run_background_fill, config)
    except TrackerUnavailableError as exc:
        raise HTTPException(status_code=500, detail=str(exc)) from exc
    except Exception as exc:  # pylint: disable=broad-except
        logger.exception("Background fill failed: %s", exc)
        raise HTTPException(
            status_code=500,
            detail=f"ì˜ìƒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {exc}",
        ) from exc

    meta.update(
        {
            "background_output_path": str(output_path),
            "background_fill_dilate": max(int(payload.dilate or 0), 0),
            "background_boxes": [box.model_dump() for box in payload.boxes],
            "background_max_frames": effective_max_frames,
        }
    )
    meta_path.write_text(
        json.dumps(meta, ensure_ascii=False, indent=2),
        encoding="utf-8",
    )

    return {
        "session_id": payload.session_id,
        "video_url": f"/api/text-removal/download/background/{payload.session_id}",
        "output_path": str(output_path),
        "message": "ë°°ê²½ ì±„ìš°ê¸°ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
    }


@app.post("/api/text-removal/split-media")
async def text_removal_split_media(payload: TextRemovalSplitRequest) -> Dict[str, Any]:
    session_dir = TEXT_REMOVAL_UPLOAD_DIR / payload.session_id
    if not session_dir.exists():
        raise HTTPException(status_code=404, detail="ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì˜ìƒì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")

    meta_path = session_dir / "meta.json"
    if not meta_path.exists():
        raise HTTPException(status_code=400, detail="ì„¸ì…˜ ì •ë³´ê°€ ì†ìƒë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")

    try:
        meta = json.loads(meta_path.read_text(encoding="utf-8"))
    except json.JSONDecodeError as exc:
        raise HTTPException(status_code=400, detail="ì„¸ì…˜ ì •ë³´ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.") from exc

    processed_path_str = meta.get("processed_path") or meta.get("input_path")
    if not processed_path_str:
        raise HTTPException(status_code=400, detail="ì—…ë¡œë“œëœ ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    video_path = Path(meta.get("trimmed_path") or processed_path_str)
    if not video_path.exists():
        video_path = Path(processed_path_str)
    if not video_path.exists():
        raise HTTPException(status_code=400, detail="ì—…ë¡œë“œëœ ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    try:
        outputs = split_media_components(video_path, session_dir)
    except Exception as exc:  # pylint: disable=broad-except
        logger.exception("Media split failed: %s", exc)
        raise HTTPException(
            status_code=500,
            detail=f"ë¯¸ë””ì–´ ë¶„ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {exc}",
        ) from exc

    meta.update(
        {
            "split_audio_path": str(outputs.get("audio", "")) if outputs.get("audio") else None,
            "split_video_path": str(outputs.get("video", "")) if outputs.get("video") else None,
            "split_subtitles_path": str(outputs.get("subtitle", "")) if outputs.get("subtitle") else None,
        }
    )
    meta_path.write_text(
        json.dumps(meta, ensure_ascii=False, indent=2),
        encoding="utf-8",
    )

    audio_url = (
        f"/api/text-removal/download/split/{payload.session_id}?kind=audio"
        if outputs.get("audio")
        else None
    )
    video_url = (
        f"/api/text-removal/download/split/{payload.session_id}?kind=video"
        if outputs.get("video")
        else None
    )
    subtitle_url = (
        f"/api/text-removal/download/split/{payload.session_id}?kind=subtitle"
        if outputs.get("subtitle")
        else None
    )

    return {
        "session_id": payload.session_id,
        "audio_url": audio_url,
        "video_url": video_url,
        "subtitle_url": subtitle_url,
        "message": "ìŒì„±, ì˜ìƒ, ìë§‰ ë¶„ë¦¬ë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.",
    }


@app.post("/api/text-removal/trim")
async def text_removal_trim(payload: TextRemovalTrimRequest) -> Dict[str, Any]:
    session_dir = TEXT_REMOVAL_UPLOAD_DIR / payload.session_id
    if not session_dir.exists():
        raise HTTPException(status_code=404, detail="ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì˜ìƒì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")

    meta_path = session_dir / "meta.json"
    if not meta_path.exists():
        raise HTTPException(status_code=400, detail="ì„¸ì…˜ ì •ë³´ê°€ ì†ìƒë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")

    try:
        meta = json.loads(meta_path.read_text(encoding="utf-8"))
    except json.JSONDecodeError as exc:
        raise HTTPException(status_code=400, detail="ì„¸ì…˜ ì •ë³´ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.") from exc

    processed_path_str = meta.get("processed_path") or meta.get("input_path")
    if not processed_path_str:
        raise HTTPException(status_code=400, detail="ì—…ë¡œë“œëœ ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    source_path = Path(processed_path_str)
    if not source_path.exists():
        source_path = Path(meta.get("input_path", processed_path_str))
    if not source_path.exists():
        raise HTTPException(status_code=400, detail="ì—…ë¡œë“œëœ ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    duration = float(payload.duration or 0.0)
    if duration <= 0:
        raise HTTPException(status_code=400, detail="ì˜ë¼ë‚¼ ê¸¸ì´ê°€ 0ë³´ë‹¤ ì»¤ì•¼ í•©ë‹ˆë‹¤.")

    start = float(payload.start or 0.0)
    if start < 0:
        raise HTTPException(status_code=400, detail="ì‹œì‘ ì‹œê°„ì€ 0 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.")

    trimmed_path = session_dir / "input_trimmed.mp4"
    effective_source = source_path
    try:
        trimmed_path, effective_source = trim_video_clip(
            source_path, trimmed_path, duration=duration, start=start
        )
    except (ValueError, FileNotFoundError) as exc:
        raise HTTPException(status_code=400, detail=str(exc)) from exc
    except Exception as exc:  # pylint: disable=broad-except
        logger.exception("Trim preview failed: %s", exc)
        raise HTTPException(
            status_code=500,
            detail=f"ì˜ìƒ ìë¥´ê¸°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {exc}",
        ) from exc

    if trimmed_path.exists() and trimmed_path.stat().st_size == 0:
        trimmed_path.unlink(missing_ok=True)
        raise HTTPException(status_code=500, detail="íŠ¸ë¦¬ë°ëœ ì˜ìƒì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ êµ¬ê°„ì„ ì‹œë„í•˜ì„¸ìš”.")

    if effective_source != source_path:
        meta["processed_path"] = str(effective_source)

    meta.update(
        {
            "trimmed_path": str(trimmed_path),
            "trim_start": start,
            "trim_duration": duration,
        }
    )
    meta_path.write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8")

    return {
        "session_id": payload.session_id,
        "video_url": f"/api/text-removal/download/trim/{payload.session_id}",
        "message": "íŠ¸ë¦¬ë°í•œ ë¯¸ë¦¬ë³´ê¸° ì˜ìƒì„ ì¤€ë¹„í–ˆìŠµë‹ˆë‹¤.",
        "start": start,
        "duration": duration,
    }


@app.get("/api/text-removal/download/{session_id}")
async def text_removal_download(session_id: str) -> FileResponse:
    session_dir = TEXT_REMOVAL_UPLOAD_DIR / session_id
    meta_path = session_dir / "meta.json"
    if not session_dir.exists() or not meta_path.exists():
        raise HTTPException(status_code=404, detail="ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    meta = json.loads(meta_path.read_text(encoding="utf-8"))
    output_path = Path(meta.get("output_path", session_dir / "restored.mp4"))
    if not output_path.exists():
        raise HTTPException(status_code=404, detail="ì²˜ë¦¬ëœ ì˜ìƒì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    original_name = meta.get("original_filename", output_path.name)
    stem = Path(original_name).stem or "restored"
    download_name = f"{stem}_restored{output_path.suffix}"

    return FileResponse(output_path, filename=download_name)


@app.get("/api/text-removal/download/trim/{session_id}")
async def text_removal_download_trim(session_id: str) -> FileResponse:
    session_dir = TEXT_REMOVAL_UPLOAD_DIR / session_id
    meta_path = session_dir / "meta.json"
    if not session_dir.exists() or not meta_path.exists():
        raise HTTPException(status_code=404, detail="ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    meta = json.loads(meta_path.read_text(encoding="utf-8"))
    target_path = Path(meta.get("trimmed_path", session_dir / "input_trimmed.mp4"))
    if not target_path.exists():
        raise HTTPException(status_code=404, detail="íŠ¸ë¦¬ë°ëœ ì˜ìƒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    original_name = meta.get("original_filename", target_path.name)
    stem = Path(original_name).stem or "clip"
    download_name = f"{stem}_trimmed{target_path.suffix}"
    return FileResponse(target_path, filename=download_name)


@app.get("/api/text-removal/download/background/{session_id}")
async def text_removal_download_background(session_id: str) -> FileResponse:
    session_dir = TEXT_REMOVAL_UPLOAD_DIR / session_id
    meta_path = session_dir / "meta.json"
    if not session_dir.exists() or not meta_path.exists():
        raise HTTPException(status_code=404, detail="ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    meta = json.loads(meta_path.read_text(encoding="utf-8"))
    output_path = Path(meta.get("background_output_path", session_dir / "background_filled.mp4"))
    if not output_path.exists():
        raise HTTPException(status_code=404, detail="ë°°ê²½ ì²˜ë¦¬ëœ ì˜ìƒì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    original_name = meta.get("original_filename", output_path.name)
    stem = Path(original_name).stem or "restored"
    download_name = f"{stem}_background{output_path.suffix}"

    return FileResponse(output_path, filename=download_name)


@app.get("/api/text-removal/download/split/{session_id}")
async def text_removal_download_split(session_id: str, kind: str = "audio") -> FileResponse:
    session_dir = TEXT_REMOVAL_UPLOAD_DIR / session_id
    meta_path = session_dir / "meta.json"
    if not session_dir.exists() or not meta_path.exists():
        raise HTTPException(status_code=404, detail="ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    meta = json.loads(meta_path.read_text(encoding="utf-8"))
    path_map = {
        "audio": meta.get("split_audio_path"),
        "video": meta.get("split_video_path"),
        "subtitle": meta.get("split_subtitles_path"),
    }
    target_path = path_map.get(kind)
    if not target_path:
        raise HTTPException(status_code=404, detail=f"ìš”ì²­í•œ {kind} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    file_path = Path(target_path)
    if not file_path.exists():
        raise HTTPException(status_code=404, detail="ìš”ì²­í•œ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    original_name = meta.get("original_filename", file_path.name)
    stem = Path(original_name).stem or "output"
    suffix = file_path.suffix
    if kind == "audio":
        download_name = f"{stem}_audio{suffix}"
    elif kind == "video":
        download_name = f"{stem}_video{suffix}"
    else:
        download_name = f"{stem}_subtitles{suffix}"

    return FileResponse(file_path, filename=download_name)


@app.get("/ytdl", response_class=HTMLResponse)
async def ytdl_index(request: Request) -> HTMLResponse:
    context = {
        "request": request,
        "form_values": default_ytdl_form(),
        "result": None,
        "error": None,
        "settings_saved": False,
        "nav_active": "ytdl",
    }
    return templates.TemplateResponse("ytdl.html", context)


@app.get("/video-analyzer", response_class=HTMLResponse)
async def video_analyzer_index(request: Request) -> HTMLResponse:
    """ì˜ìƒ ë¶„ì„ê¸° í˜ì´ì§€."""
    context = {
        "request": request,
        "form_values": {
            "video_path": "",
            "search_directory": "",
            "auto_search": True,
        },
        "result": None,
        "error": None,
        "nav_active": "video_analyzer",
    }
    return templates.TemplateResponse("video_analyzer.html", context)


@app.post("/video-analyzer", response_class=HTMLResponse)
async def video_analyzer_process(
    request: Request,
    video_file: Optional[UploadFile] = File(None),
    subtitle_file: Optional[UploadFile] = File(None),
    video_path: str = Form(""),
    search_directory: str = Form(""),
    auto_search: bool = Form(False),
) -> HTMLResponse:
    """ì˜ìƒì—ì„œ ìë§‰ ì¶”ì¶œ ë° ë¶„ì„."""
    context = {
        "request": request,
        "form_values": {
            "video_path": video_path,
            "search_directory": search_directory,
            "auto_search": auto_search,
        },
        "result": None,
        "error": None,
        "nav_active": "video_analyzer",
    }

    try:
        # ì˜ìƒ íŒŒì¼ ì²˜ë¦¬
        video_file_path = None
        if video_file and video_file.filename:
            # ì—…ë¡œë“œëœ íŒŒì¼ ì €ì¥
            upload_dir = Path("temp_uploads")
            upload_dir.mkdir(exist_ok=True)
            video_file_path = upload_dir / f"{uuid4()}_{video_file.filename}"
            with open(video_file_path, "wb") as f:
                content = await video_file.read()
                f.write(content)
        elif video_path:
            # ê²½ë¡œë¡œ ì§€ì •ëœ íŒŒì¼
            video_file_path = Path(video_path)
            if not video_file_path.exists():
                raise ValueError(f"ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")

        if not video_file_path:
            raise ValueError("ì˜ìƒ íŒŒì¼ì„ ì„ íƒí•˜ê±°ë‚˜ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”.")

        # ìë§‰ íŒŒì¼ ì°¾ê¸°
        subtitle_path = None
        subtitles_data = []

        # ì—…ë¡œë“œëœ ìë§‰ íŒŒì¼ì´ ìˆëŠ” ê²½ìš°
        if subtitle_file and subtitle_file.filename:
            upload_dir = Path("temp_uploads")
            upload_dir.mkdir(exist_ok=True)
            subtitle_path = upload_dir / f"{uuid4()}_{subtitle_file.filename}"
            with open(subtitle_path, "wb") as f:
                content = await subtitle_file.read()
                f.write(content)
        else:
            # ìë§‰ íŒŒì¼ ìë™ ê²€ìƒ‰
            search_paths = []

            logging.info(f"ğŸ” ìë§‰ ìë™ ê²€ìƒ‰ ì‹œì‘")
            logging.info(f"  - auto_search: {auto_search}")
            logging.info(f"  - search_directory: {search_directory}")

            # ì˜ìƒ íŒŒì¼ê³¼ ê°™ì€ ë””ë ‰í† ë¦¬ì—ì„œ ê²€ìƒ‰
            if video_file_path:
                import re
                video_dir = video_file_path.parent
                video_stem = video_file_path.stem
                video_id = None
                video_id_match = re.search(r'\[([a-zA-Z0-9_-]+)\]', video_file_path.name)
                if video_id_match:
                    video_id = video_id_match.group(1)

                logging.info(f"  - ì˜ìƒ íŒŒì¼: {video_file_path.name}")
                logging.info(f"  - ì˜ìƒ ë””ë ‰í† ë¦¬: {video_dir}")
                logging.info(f"  - ë¹„ë””ì˜¤ ID: {video_id}")

                # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ìë§‰ íŒŒì¼ ìš°ì„  ê²€ìƒ‰
                exact_match_paths = [
                    video_dir / f"{video_stem}.srt",
                    video_dir / f"{video_stem}.vtt",
                    video_dir / f"{video_stem}.ko.srt",
                    video_dir / f"{video_stem}.en.srt",
                ]

                # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” íŒŒì¼ì´ ìˆëŠ”ì§€ ë¨¼ì € í™•ì¸
                for path in exact_match_paths:
                    if path.exists() and path.is_file():
                        subtitle_path = path
                        logging.info(f"  âœ… ì •í™•í•œ íŒŒì¼ëª… ë§¤ì¹­: {subtitle_path.name}")
                        break

                # ì •í™•í•œ ë§¤ì¹­ì´ ì—†ìœ¼ë©´, ë¹„ë””ì˜¤ ID ë§¤ì¹­ ì‹œë„
                if not subtitle_path and video_id:
                    logging.info(f"  ğŸ” ë¹„ë””ì˜¤ IDë¡œ ìë§‰ ê²€ìƒ‰ ì¤‘: [{video_id}]")
                    # ê°™ì€ ë¹„ë””ì˜¤ IDë¥¼ ê°€ì§„ ìë§‰ íŒŒì¼ ì°¾ê¸°
                    for ext in [".srt", ".vtt", ".ko.srt", ".en.srt"]:
                        matching_subtitles = list(video_dir.glob(f"*[{video_id}]*{ext}"))
                        if matching_subtitles:
                            subtitle_path = matching_subtitles[0]
                            logging.info(f"  âœ… ë¹„ë””ì˜¤ ID ë§¤ì¹­: {subtitle_path.name}")
                            break

                # ë¹„ë””ì˜¤ ID ë§¤ì¹­ë„ ì‹¤íŒ¨í•˜ë©´, ê²€ìƒ‰ ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°
                if not subtitle_path and search_directory and auto_search:
                    logging.info(f"  ğŸ” ê²€ìƒ‰ ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°: {search_directory}")
                    search_dir = Path(search_directory)
                    if search_dir.exists() and search_dir.is_dir():
                        # ë¨¼ì € ë¹„ë””ì˜¤ IDë¡œ ê²€ìƒ‰
                        if video_id:
                            logging.info(f"  ğŸ” ê²€ìƒ‰ ë””ë ‰í† ë¦¬ì—ì„œ ë¹„ë””ì˜¤ IDë¡œ ê²€ìƒ‰: [{video_id}]")
                            for ext in [".srt", ".vtt", ".ko.srt", ".en.srt"]:
                                matching_subtitles = list(search_dir.glob(f"*[{video_id}]*{ext}"))
                                if matching_subtitles:
                                    subtitle_path = matching_subtitles[0]
                                    logging.info(f"  âœ… ê²€ìƒ‰ ë””ë ‰í† ë¦¬ì—ì„œ ë¹„ë””ì˜¤ ID ë§¤ì¹­: {subtitle_path.name}")
                                    break

                        # ë¹„ë””ì˜¤ ID ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ, íŒŒì¼ëª… ìœ ì‚¬ë„ë¡œ ì°¾ê¸°
                        if not subtitle_path:
                            logging.info(f"  ğŸ” íŒŒì¼ëª… ìœ ì‚¬ë„ë¡œ ìë§‰ ê²€ìƒ‰ ì¤‘")
                            all_subtitle_files = []
                            for ext in [".srt", ".vtt", ".ass", ".ssa"]:
                                all_subtitle_files.extend(search_dir.glob(f"*{ext}"))

                            logging.info(f"  - ê²€ìƒ‰ ë””ë ‰í† ë¦¬ì˜ ìë§‰ íŒŒì¼ ìˆ˜: {len(all_subtitle_files)}")

                            # íŒŒì¼ëª… ìœ ì‚¬ë„ ê³„ì‚° (ê°€ì¥ ê¸´ ê³µí†µ ë¶€ë¶„ ë¬¸ìì—´)
                            best_match = None
                            best_score = 0

                            for sub_file in all_subtitle_files:
                                # ë¹„ë””ì˜¤ íŒŒì¼ëª…ê³¼ ìë§‰ íŒŒì¼ëª…ì˜ ìœ ì‚¬ë„ ê³„ì‚°
                                common_length = len(set(video_stem.lower().split()) &
                                                    set(sub_file.stem.lower().split()))
                                if common_length > best_score:
                                    best_score = common_length
                                    best_match = sub_file

                            if best_match and best_score > 0:
                                subtitle_path = best_match
                                logging.info(f"  âœ… ìœ ì‚¬ë„ ë§¤ì¹­ (ì ìˆ˜: {best_score}): {subtitle_path.name}")
                            else:
                                logging.info(f"  âŒ ìœ ì‚¬í•œ ìë§‰ íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
                    else:
                        logging.warning(f"  âš ï¸ ê²€ìƒ‰ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ ë””ë ‰í† ë¦¬ê°€ ì•„ë‹™ë‹ˆë‹¤: {search_directory}")
                elif not subtitle_path:
                    if not auto_search:
                        logging.info(f"  âš ï¸ ìë™ ê²€ìƒ‰ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
                    elif not search_directory:
                        logging.info(f"  âš ï¸ ê²€ìƒ‰ ë””ë ‰í† ë¦¬ê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        # ìë§‰ íŒŒì¼ íŒŒì‹±
        if subtitle_path and subtitle_path.exists():
            logging.info(f"âœ… ìë§‰ íŒŒì¼ ë°œê²¬: {subtitle_path}")
            logging.info(f"  - ê²½ë¡œ: {subtitle_path}")
            with open(subtitle_path, "r", encoding="utf-8") as f:
                content = f.read()

            # ê°œì„ ëœ SRT íŒŒì‹±
            if subtitle_path.suffix.lower() == ".srt":
                import re

                # ì •ê·œì‹ìœ¼ë¡œ ìë§‰ ë¸”ë¡ ì¶”ì¶œ
                # íŒ¨í„´: ìˆ«ì -> ì‹œê°„ ì½”ë“œ -> í…ìŠ¤íŠ¸ (í•œ ì¤„ ì´ìƒ)
                subtitle_pattern = r'(\d+)\s*\n(\d{2}:\d{2}:\d{2},\d{3})\s*-->\s*(\d{2}:\d{2}:\d{2},\d{3})\s*\n((?:(?!\d+\s*\n\d{2}:\d{2}:).+\n?)+)'

                matches = re.finditer(subtitle_pattern, content, re.MULTILINE)

                for match in matches:
                    index = match.group(1)
                    start = match.group(2)
                    end = match.group(3)
                    text = match.group(4).strip()

                    # í…ìŠ¤íŠ¸ ì •ë¦¬
                    # 1. ì¤„ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
                    lines = text.split('\n')
                    # 2. ë‹¨ë… ìˆ«ìë§Œ ìˆëŠ” ì¤„ ì œê±° (ìë§‰ ë²ˆí˜¸ê°€ ì„ì¸ ê²½ìš°)
                    lines = [line.strip() for line in lines if line.strip() and not line.strip().isdigit()]
                    # 3. ê³µë°±ìœ¼ë¡œ ì—°ê²°
                    text = ' '.join(lines)

                    if text:  # ë¹ˆ í…ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
                        subtitles_data.append({
                            "start": start.strip(),
                            "end": end.strip(),
                            "text": text.strip(),
                        })

        if not subtitle_path:
            logging.error(f"âŒ ìë§‰ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            logging.error(f"  - ì˜ìƒ íŒŒì¼: {video_file_path.name if video_file_path else 'None'}")
            logging.error(f"  - ê²€ìƒ‰ ë””ë ‰í† ë¦¬: {search_directory or 'ì§€ì •ë˜ì§€ ì•ŠìŒ'}")
            logging.error(f"  - ìë™ ê²€ìƒ‰: {auto_search}")
            raise ValueError("ìë§‰ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìë§‰ íŒŒì¼ì„ ì§ì ‘ ì—…ë¡œë“œí•˜ê±°ë‚˜ ê²€ìƒ‰ í´ë”ë¥¼ ì§€ì •í•˜ì„¸ìš”.")

        context["result"] = {
            "video_name": video_file_path.name,
            "subtitle_file": subtitle_path.name if subtitle_path else None,
            "subtitle_count": len(subtitles_data),
            "subtitles": subtitles_data,
            "download_url": None,  # í•„ìš”ì‹œ êµ¬í˜„
        }

    except ValueError as e:
        context["error"] = str(e)
    except Exception as e:
        logging.exception("ì˜ìƒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
        context["error"] = f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    return templates.TemplateResponse("video_analyzer.html", context)


@app.get("/api/video-analyzer/videos")
async def api_get_video_files(folder: Optional[str] = None) -> List[Dict[str, Any]]:
    """Get list of available video files.

    Args:
        folder: Optional folder path to search. If provided, only searches this folder.
    """
    video_files = []
    video_extensions = {".mp4", ".webm", ".mkv", ".avi", ".mov", ".flv", ".m4v", ".wmv"}

    # í´ë” íŒŒë¼ë¯¸í„°ê°€ ì œê³µëœ ê²½ìš° í•´ë‹¹ í´ë”ë§Œ ê²€ìƒ‰
    if folder:
        search_dirs = [Path(folder)]
        max_depth = 1  # ì§€ì •ëœ í´ë”ëŠ” í•˜ìœ„ í´ë”ë¥¼ ê²€ìƒ‰í•˜ì§€ ì•ŠìŒ (ì„±ëŠ¥ í–¥ìƒ)
    else:
        # ê¸°ë³¸ ê²€ìƒ‰ í´ë”ë“¤
        search_dirs = [
            Path("/home/sk/ws/youtubeanalysis/youtube/download"),  # ê¸°ë³¸ í´ë”
        ]
        max_depth = 1  # í•˜ìœ„ í´ë” ê²€ìƒ‰í•˜ì§€ ì•ŠìŒ (ì„±ëŠ¥ í–¥ìƒ)

    for search_dir in search_dirs:
        if not search_dir.exists():
            continue

        try:
            # ëª¨ë“  ê²½ìš°ì— ì§ì ‘ í•˜ìœ„ íŒŒì¼ë§Œ ê²€ìƒ‰ (í•˜ìœ„ í´ë” ê²€ìƒ‰ ì•ˆ í•¨)
            for item in search_dir.iterdir():
                if item.is_file() and item.suffix.lower() in video_extensions:
                    try:
                        stat_info = item.stat()

                        # ìë§‰ íŒŒì¼ í™•ì¸
                        # 1. ì •í™•íˆ ê°™ì€ ì´ë¦„ì˜ .srt íŒŒì¼
                        subtitle_path = item.with_suffix('.srt')
                        has_subtitle = subtitle_path.exists()

                        # 2. ì–¸ì–´ ì½”ë“œê°€ í¬í•¨ëœ ìë§‰ íŒŒì¼ (.ko.srt, .en.srt ë“±)
                        if not has_subtitle:
                            # ë¹„ë””ì˜¤ ID ì¶”ì¶œ (ì˜ˆ: [vTMssu3XB7g])
                            import re
                            video_id_match = re.search(r'\[([a-zA-Z0-9_-]+)\]', item.name)

                            if video_id_match:
                                video_id = video_id_match.group(1)
                                # ê°™ì€ ë¹„ë””ì˜¤ IDë¥¼ ê°€ì§„ ìë§‰ íŒŒì¼ ì°¾ê¸°
                                for srt_file in item.parent.glob(f"*[{video_id}]*.srt"):
                                    if srt_file.is_file():
                                        subtitle_path = srt_file
                                        has_subtitle = True
                                        break

                            # ë¹„ë””ì˜¤ IDê°€ ì—†ìœ¼ë©´ íŒŒì¼ëª… ê¸°ì¤€ìœ¼ë¡œ ì°¾ê¸°
                            if not has_subtitle:
                                # í™•ì¥ìë¥¼ ì œê±°í•œ íŒŒì¼ëª…ìœ¼ë¡œ ìë§‰ ì°¾ê¸°
                                base_name = item.stem
                                for srt_file in item.parent.glob(f"{base_name}*.srt"):
                                    if srt_file.is_file():
                                        subtitle_path = srt_file
                                        has_subtitle = True
                                        break

                        video_info = {
                            "name": item.name,
                            "path": str(item.absolute()),
                            "size": stat_info.st_size,
                            "size_mb": round(stat_info.st_size / (1024 * 1024), 2),
                            "modified": datetime.fromtimestamp(stat_info.st_mtime).isoformat(),
                            "directory": str(item.parent),
                            "has_subtitle": has_subtitle,
                        }

                        if has_subtitle:
                            video_info["subtitle_path"] = str(subtitle_path.absolute())

                        video_files.append(video_info)
                    except (OSError, PermissionError):
                        continue
        except (OSError, PermissionError):
            continue

    # ìˆ˜ì • ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ìµœì‹ ìˆœ ì •ë ¬
    video_files.sort(key=lambda x: x["modified"], reverse=True)

    # ìµœëŒ€ 100ê°œë¡œ ì œí•œ
    return video_files[:100]


@app.get("/api/video-analyzer/stream")
async def api_stream_video(path: str):
    """Stream video or subtitle file for preview.

    Args:
        path: Absolute path to the video or subtitle file.
    """
    import re
    from fastapi.responses import Response

    file_path = Path(path)

    # ë³´ì•ˆ: íŒŒì¼ì´ ì¡´ì¬í•˜ê³  ì‹¤ì œ íŒŒì¼ì¸ì§€ í™•ì¸
    if not file_path.exists() or not file_path.is_file():
        raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # ì§€ì›ë˜ëŠ” í™•ì¥ì í™•ì¸
    video_extensions = {".mp4", ".webm", ".mkv", ".avi", ".mov", ".flv", ".m4v", ".wmv"}
    subtitle_extensions = {".srt", ".vtt", ".ass", ".ssa"}
    supported_extensions = video_extensions | subtitle_extensions

    if file_path.suffix.lower() not in supported_extensions:
        raise HTTPException(status_code=400, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")

    # SRT íŒŒì¼ì„ WebVTTë¡œ ë³€í™˜
    if file_path.suffix.lower() == ".srt":
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                srt_content = f.read()

            # SRTë¥¼ WebVTTë¡œ ë³€í™˜
            vtt_content = "WEBVTT\n\n" + re.sub(
                r'(\d{2}):(\d{2}):(\d{2}),(\d{3})',
                r'\1:\2:\3.\4',
                srt_content
            )

            return Response(
                content=vtt_content,
                media_type="text/vtt; charset=utf-8",
                headers={"Content-Disposition": f"inline; filename={file_path.stem}.vtt"}
            )
        except Exception as e:
            logging.error(f"SRT to VTT conversion error: {e}")
            # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ì›ë³¸ íŒŒì¼ ë°˜í™˜
            pass

    # MIME íƒ€ì… ê²°ì •
    mime_types = {
        ".mp4": "video/mp4",
        ".webm": "video/webm",
        ".mkv": "video/x-matroska",
        ".avi": "video/x-msvideo",
        ".mov": "video/quicktime",
        ".flv": "video/x-flv",
        ".m4v": "video/mp4",
        ".wmv": "video/x-ms-wmv",
        ".srt": "text/vtt",
        ".vtt": "text/vtt",
        ".ass": "text/x-ass",
        ".ssa": "text/x-ssa",
    }
    media_type = mime_types.get(file_path.suffix.lower(), "application/octet-stream")

    return FileResponse(
        path=str(file_path),
        media_type=media_type,
        filename=file_path.name,
    )


@app.get("/api/video-analyzer/download")
async def api_download_file(path: str):
    """Download video or other file.

    Args:
        path: Absolute path to the file.
    """
    file_path = Path(path)

    # ë³´ì•ˆ: íŒŒì¼ì´ ì¡´ì¬í•˜ê³  ì‹¤ì œ íŒŒì¼ì¸ì§€ í™•ì¸
    if not file_path.exists() or not file_path.is_file():
        raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # MIME íƒ€ì… ê²°ì •
    mime_types = {
        ".mp4": "video/mp4",
        ".webm": "video/webm",
        ".mkv": "video/x-matroska",
        ".avi": "video/x-msvideo",
        ".mov": "video/quicktime",
        ".srt": "text/srt",
        ".vtt": "text/vtt",
    }
    media_type = mime_types.get(file_path.suffix.lower(), "application/octet-stream")

    return FileResponse(
        path=str(file_path),
        media_type=media_type,
        filename=file_path.name,
        headers={"Content-Disposition": f"attachment; filename={file_path.name}"}
    )


@app.post("/api/video-analyzer/extract-frame")
async def api_extract_frame(payload: Dict[str, Any] = Body(...)):
    """Extract a single frame from video.

    Args:
        payload: {
            "video_path": str,
            "frame_type": "first" | "last"
        }
    """
    import cv2
    import base64

    video_path = Path(payload["video_path"])
    frame_type = payload["frame_type"]

    if not video_path.exists():
        raise HTTPException(status_code=404, detail="ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    transcoded_file = None
    try:
        cap, transcoded_file = open_video_with_transcode_fallback(video_path)

        if frame_type == "first":
            # ì²« í”„ë ˆì„
            cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
            ret, frame = cap.read()
            timestamp = 0.0
        elif frame_type == "last":
            # ë§ˆì§€ë§‰ í”„ë ˆì„
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            if total_frames > 0:
                cap.set(cv2.CAP_PROP_POS_FRAMES, total_frames - 1)
                ret, frame = cap.read()
                fps = cap.get(cv2.CAP_PROP_FPS)
                timestamp = round((total_frames - 1) / fps, 2) if fps > 0 else 0.0
            else:
                raise HTTPException(status_code=400, detail="ë¹„ë””ì˜¤ì— í”„ë ˆì„ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            raise HTTPException(status_code=400, detail="ì˜ëª»ëœ frame_typeì…ë‹ˆë‹¤.")

        cap.release()

        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if transcoded_file and transcoded_file.exists():
            try:
                transcoded_file.unlink()
                logger.info(f"Cleaned up transcoded file: {transcoded_file}")
            except Exception as cleanup_exc:
                logger.warning(f"Failed to clean up transcoded file: {cleanup_exc}")

        if not ret:
            raise HTTPException(status_code=400, detail="í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # JPEGë¡œ ì¸ì½”ë”©
        _, buffer = cv2.imencode('.jpg', frame)
        frame_base64 = base64.b64encode(buffer).decode('utf-8')

        return {
            "frame_data": frame_base64,
            "timestamp": timestamp
        }

    except Exception as e:
        # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if transcoded_file and transcoded_file.exists():
            try:
                transcoded_file.unlink()
            except Exception:
                pass
        logging.exception("í”„ë ˆì„ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
        raise HTTPException(status_code=500, detail=f"í”„ë ˆì„ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")


@app.post("/api/video-analyzer/extract-frames-interval")
async def api_extract_frames_interval(payload: Dict[str, Any] = Body(...)):
    """Extract frames at regular intervals from video.

    Args:
        payload: {
            "video_path": str,
            "interval": int (seconds)
        }
    """
    import cv2
    import base64

    video_path = Path(payload["video_path"])
    interval = payload.get("interval", 10)

    if not video_path.exists():
        raise HTTPException(status_code=404, detail="ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    transcoded_file = None
    try:
        cap, transcoded_file = open_video_with_transcode_fallback(video_path)

        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps if fps > 0 else 0

        frames_data = []
        times = []

        # ê°„ê²©ë§ˆë‹¤ ì‹œê°„ ì¶”ê°€
        current_time = 0
        while current_time < duration:
            times.append(current_time)
            current_time += interval

        # ë§ˆì§€ë§‰ í”„ë ˆì„ ì¶”ê°€ (durationì´ intervalì˜ ë°°ìˆ˜ê°€ ì•„ë‹ ê²½ìš°)
        if len(times) == 0 or times[-1] < duration - 0.1:
            times.append(duration - 0.1)

        # ëª¨ë“  í”„ë ˆì„ ì¶”ì¶œ
        for time in times:
            frame_number = int(time * fps)

            if frame_number >= total_frames:
                frame_number = total_frames - 1

            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)
            ret, frame = cap.read()

            if ret:
                # JPEGë¡œ ì¸ì½”ë”©
                _, buffer = cv2.imencode('.jpg', frame)
                frame_base64 = base64.b64encode(buffer).decode('utf-8')

                frames_data.append({
                    "frame_data": frame_base64,
                    "timestamp": round(time, 2)
                })

        cap.release()

        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if transcoded_file and transcoded_file.exists():
            try:
                transcoded_file.unlink()
            except Exception:
                pass

        return {
            "frames": frames_data,
            "total_duration": round(duration, 2)
        }

    except Exception as e:
        # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if transcoded_file and transcoded_file.exists():
            try:
                transcoded_file.unlink()
            except Exception:
                pass
        logging.exception("í”„ë ˆì„ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
        raise HTTPException(status_code=500, detail=f"í”„ë ˆì„ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")


@app.post("/api/video-analyzer/extract-frames-by-segments")
async def api_extract_frames_by_segments(payload: Dict[str, Any] = Body(...)):
    """ê° ìë§‰ êµ¬ê°„ë§ˆë‹¤ ê· ë“±í•˜ê²Œ Nê°œì˜ í”„ë ˆì„ì„ ì¶”ì¶œ

    Args:
        payload: {
            "video_path": str,
            "subtitle_path": str,
            "frames_per_segment": int (ê° êµ¬ê°„ë‹¹ ì¶”ì¶œí•  í”„ë ˆì„ ê°œìˆ˜)
        }
    """
    import cv2
    import base64
    import re

    video_path = Path(payload["video_path"])
    subtitle_path = Path(payload.get("subtitle_path", ""))
    frames_per_segment = payload.get("frames_per_segment", 2)

    if not video_path.exists():
        raise HTTPException(status_code=404, detail="ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    if not subtitle_path.exists():
        raise HTTPException(status_code=404, detail="ìë§‰ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    try:
        # ìë§‰ íŒŒì¼ì—ì„œ êµ¬ê°„ ì •ë³´ ì¶”ì¶œ
        with open(subtitle_path, 'r', encoding='utf-8') as f:
            content = f.read()

        file_ext = subtitle_path.suffix.lower()
        segments = []

        if file_ext == '.srt':
            # SRT í˜•ì‹ íŒŒì‹± - ì‹œì‘ê³¼ ë ì‹œê°„ ëª¨ë‘ ì¶”ì¶œ
            time_pattern = r'(\d{2}):(\d{2}):(\d{2})[,\.](\d{3})\s*-->\s*(\d{2}):(\d{2}):(\d{2})[,\.](\d{3})'
            matches = re.findall(time_pattern, content)

            for match in matches:
                # ì‹œì‘ ì‹œê°„
                start_hours, start_min, start_sec, start_ms = int(match[0]), int(match[1]), int(match[2]), int(match[3])
                start_time = start_hours * 3600 + start_min * 60 + start_sec + start_ms / 1000

                # ë ì‹œê°„
                end_hours, end_min, end_sec, end_ms = int(match[4]), int(match[5]), int(match[6]), int(match[7])
                end_time = end_hours * 3600 + end_min * 60 + end_sec + end_ms / 1000

                segments.append({"start": start_time, "end": end_time})

        elif file_ext == '.vtt':
            # VTT í˜•ì‹ íŒŒì‹±
            time_pattern = r'(\d{2}):(\d{2}):(\d{2})\.(\d{3})\s*-->\s*(\d{2}):(\d{2}):(\d{2})\.(\d{3})'
            matches = re.findall(time_pattern, content)

            for match in matches:
                # ì‹œì‘ ì‹œê°„
                start_hours, start_min, start_sec, start_ms = int(match[0]), int(match[1]), int(match[2]), int(match[3])
                start_time = start_hours * 3600 + start_min * 60 + start_sec + start_ms / 1000

                # ë ì‹œê°„
                end_hours, end_min, end_sec, end_ms = int(match[4]), int(match[5]), int(match[6]), int(match[7])
                end_time = end_hours * 3600 + end_min * 60 + end_sec + end_ms / 1000

                segments.append({"start": start_time, "end": end_time})
        else:
            raise HTTPException(status_code=400, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” ìë§‰ í˜•ì‹ì…ë‹ˆë‹¤. (.srt ë˜ëŠ” .vttë§Œ ì§€ì›)")

        if not segments:
            raise HTTPException(status_code=400, detail="ìë§‰ íŒŒì¼ì—ì„œ ì‹œê°„ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # ì˜ìƒ ì—´ê¸°
        transcoded_file = None
        cap, transcoded_file = open_video_with_transcode_fallback(video_path)

        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps if fps > 0 else 0

        frames_data = []

        # ê° êµ¬ê°„ë§ˆë‹¤ ê· ë“±í•˜ê²Œ Nê°œ ì¶”ì¶œ
        for segment_idx, segment in enumerate(segments):
            start_time = segment["start"]
            end_time = segment["end"]
            segment_duration = end_time - start_time

            # êµ¬ê°„ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ìŠ¤í‚µ
            if segment_duration < 0.1:
                continue

            # Nê°œë¡œ ê· ë“± ë¶„í• 
            # frames_per_segment=2: ì‹œì‘(1/2), ë(2/2)
            # frames_per_segment=3: 1/3, 2/3, 3/3
            # frames_per_segment=4: 1/4, 2/4, 3/4, 4/4
            for i in range(frames_per_segment):
                # ê· ë“± ë¶„í•  ìœ„ì¹˜ ê³„ì‚°
                ratio = (i + 1) / frames_per_segment
                time = start_time + segment_duration * ratio

                # ì˜ìƒ ëì„ ë„˜ì–´ê°€ì§€ ì•Šë„ë¡
                if time >= duration:
                    time = duration - 0.1

                frame_number = int(time * fps)
                if frame_number >= total_frames:
                    frame_number = total_frames - 1

                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)
                ret, frame = cap.read()

                if ret:
                    # JPEGë¡œ ì¸ì½”ë”©
                    _, buffer = cv2.imencode('.jpg', frame)
                    frame_base64 = base64.b64encode(buffer).decode('utf-8')

                    frames_data.append({
                        "frame_data": frame_base64,
                        "timestamp": round(time, 2),
                        "segment_index": segment_idx + 1,
                        "frame_in_segment": i + 1
                    })

        cap.release()

        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if transcoded_file and transcoded_file.exists():
            try:
                transcoded_file.unlink()
            except Exception:
                pass

        return {
            "frames": frames_data,
            "total_segments": len(segments),
            "frames_per_segment": frames_per_segment,
            "total_frames": len(frames_data)
        }

    except HTTPException:
        # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if 'transcoded_file' in locals() and transcoded_file and transcoded_file.exists():
            try:
                transcoded_file.unlink()
            except Exception:
                pass
        raise
    except Exception as e:
        # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if 'transcoded_file' in locals() and transcoded_file and transcoded_file.exists():
            try:
                transcoded_file.unlink()
            except Exception:
                pass
        logging.exception("ì„¸ê·¸ë¨¼íŠ¸ë³„ í”„ë ˆì„ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
        raise HTTPException(status_code=500, detail=f"í”„ë ˆì„ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")


@app.post("/api/video-analyzer/get-subtitle-times")
async def api_get_subtitle_times(payload: Dict[str, Any] = Body(...)) -> Dict[str, Any]:
    """ìë§‰ íŒŒì¼ì—ì„œ ì‹œì‘ ì‹œê°„ ì¶”ì¶œ"""
    try:
        subtitle_path = Path(payload.get("subtitle_path", ""))

        if not subtitle_path.exists():
            raise HTTPException(status_code=404, detail="ìë§‰ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        times = []

        # íŒŒì¼ í™•ì¥ìì— ë”°ë¼ ì²˜ë¦¬
        file_ext = subtitle_path.suffix.lower()

        with open(subtitle_path, 'r', encoding='utf-8') as f:
            content = f.read()

        if file_ext == '.srt':
            # SRT í˜•ì‹ íŒŒì‹±
            import re
            # ì‹œê°„ íŒ¨í„´: 00:00:00,000 --> 00:00:02,000
            time_pattern = r'(\d{2}):(\d{2}):(\d{2})[,\.](\d{3})\s*-->\s*(\d{2}):(\d{2}):(\d{2})[,\.](\d{3})'
            matches = re.findall(time_pattern, content)

            for match in matches:
                # ì‹œì‘ ì‹œê°„ë§Œ ì¶”ì¶œ
                hours, minutes, seconds, milliseconds = int(match[0]), int(match[1]), int(match[2]), int(match[3])
                time_in_seconds = hours * 3600 + minutes * 60 + seconds + milliseconds / 1000
                times.append(time_in_seconds)

        elif file_ext == '.vtt':
            # VTT í˜•ì‹ íŒŒì‹±
            import re
            # ì‹œê°„ íŒ¨í„´: 00:00:00.000 --> 00:00:02.000
            time_pattern = r'(\d{2}):(\d{2}):(\d{2})\.(\d{3})\s*-->\s*(\d{2}):(\d{2}):(\d{2})\.(\d{3})'
            matches = re.findall(time_pattern, content)

            for match in matches:
                # ì‹œì‘ ì‹œê°„ë§Œ ì¶”ì¶œ
                hours, minutes, seconds, milliseconds = int(match[0]), int(match[1]), int(match[2]), int(match[3])
                time_in_seconds = hours * 3600 + minutes * 60 + seconds + milliseconds / 1000
                times.append(time_in_seconds)

        else:
            raise HTTPException(status_code=400, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” ìë§‰ íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. (.srt, .vtt ì§€ì›)")

        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        times = sorted(list(set(times)))

        return {
            "times": times,
            "count": len(times)
        }

    except HTTPException:
        raise
    except Exception as e:
        logging.exception("ìë§‰ ì‹œê°„ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
        raise HTTPException(status_code=500, detail=f"ìë§‰ ì‹œê°„ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")


@app.get("/api/video-analyzer/read-subtitle")
async def api_read_subtitle(path: str) -> Dict[str, Any]:
    """ìë§‰ íŒŒì¼ ë‚´ìš© ì½ê¸°"""
    try:
        subtitle_path = Path(path)

        if not subtitle_path.exists():
            raise HTTPException(status_code=404, detail="ìë§‰ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        subtitles = []
        file_ext = subtitle_path.suffix.lower()

        with open(subtitle_path, 'r', encoding='utf-8') as f:
            content = f.read()

        if file_ext == '.srt':
            # SRT í˜•ì‹ íŒŒì‹±
            import re
            # SRT ë¸”ë¡ ë¶„ë¦¬ (ë¹ˆ ì¤„ë¡œ êµ¬ë¶„)
            blocks = content.strip().split('\n\n')

            for block in blocks:
                lines = block.strip().split('\n')
                if len(lines) >= 3:
                    # ì²« ì¤„: ë²ˆí˜¸
                    index = lines[0].strip()
                    # ë‘˜ì§¸ ì¤„: ì‹œê°„
                    time_line = lines[1].strip()
                    # ì…‹ì§¸ ì¤„ ì´í›„: í…ìŠ¤íŠ¸
                    text = '\n'.join(lines[2:]).strip()

                    # ì‹œê°„ íŒŒì‹±
                    time_pattern = r'(\d{2}):(\d{2}):(\d{2})[,\.](\d{3})\s*-->\s*(\d{2}):(\d{2}):(\d{2})[,\.](\d{3})'
                    match = re.match(time_pattern, time_line)

                    if match:
                        start_h, start_m, start_s, start_ms = int(match.group(1)), int(match.group(2)), int(match.group(3)), int(match.group(4))
                        end_h, end_m, end_s, end_ms = int(match.group(5)), int(match.group(6)), int(match.group(7)), int(match.group(8))

                        start_time = f"{start_h:02d}:{start_m:02d}:{start_s:02d},{start_ms:03d}"
                        end_time = f"{end_h:02d}:{end_m:02d}:{end_s:02d},{end_ms:03d}"

                        subtitles.append({
                            'index': index,
                            'start': start_time,
                            'end': end_time,
                            'text': text
                        })

        elif file_ext == '.vtt':
            # VTT í˜•ì‹ íŒŒì‹±
            import re
            lines = content.split('\n')
            i = 0
            index = 1

            while i < len(lines):
                line = lines[i].strip()

                # ì‹œê°„ íŒ¨í„´ ì°¾ê¸°
                time_pattern = r'(\d{2}):(\d{2}):(\d{2})\.(\d{3})\s*-->\s*(\d{2}):(\d{2}):(\d{2})\.(\d{3})'
                match = re.match(time_pattern, line)

                if match:
                    start_h, start_m, start_s, start_ms = int(match.group(1)), int(match.group(2)), int(match.group(3)), int(match.group(4))
                    end_h, end_m, end_s, end_ms = int(match.group(5)), int(match.group(6)), int(match.group(7)), int(match.group(8))

                    start_time = f"{start_h:02d}:{start_m:02d}:{start_s:02d},{start_ms:03d}"
                    end_time = f"{end_h:02d}:{end_m:02d}:{end_s:02d},{end_ms:03d}"

                    # ë‹¤ìŒ ì¤„ë“¤ì´ í…ìŠ¤íŠ¸
                    text_lines = []
                    i += 1
                    while i < len(lines) and lines[i].strip():
                        text_lines.append(lines[i].strip())
                        i += 1

                    text = '\n'.join(text_lines)

                    subtitles.append({
                        'index': str(index),
                        'start': start_time,
                        'end': end_time,
                        'text': text
                    })
                    index += 1

                i += 1

        else:
            raise HTTPException(status_code=400, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” ìë§‰ íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. (.srt, .vtt ì§€ì›)")

        return {
            "success": True,
            "subtitle_path": str(subtitle_path),
            "subtitles": subtitles,
            "count": len(subtitles)
        }

    except HTTPException:
        raise
    except Exception as e:
        logging.exception("ìë§‰ íŒŒì¼ ì½ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
        raise HTTPException(status_code=500, detail=f"ìë§‰ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}")


@app.post("/api/video-analyzer/cut-by-subtitles-start")
async def api_cut_video_by_subtitles_start(payload: Dict[str, Any] = Body(...)) -> Dict[str, Any]:
    """ìë§‰ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì˜ìƒ ìë¥´ê¸° ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ)"""
    import threading
    from uuid import uuid4 as generate_uuid

    task_id = str(generate_uuid())
    video_path_str = payload.get("video_path", "")
    subtitle_path_str = payload.get("subtitle_path", "")

    # ì´ˆê¸° ìƒíƒœ ì„¤ì •
    cut_video_tasks[task_id] = {
        "status": "starting",
        "progress": 0,
        "total": 0,
        "current": 0,
        "message": "ìë§‰ íŒŒì¼ ë¶„ì„ ì¤‘...",
        "zip_path": None,
        "error": None,
        "completed": False,
    }

    def run_cut_video():
        """ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ ì˜ìƒ ìë¥´ê¸° ì‹¤í–‰"""
        import subprocess
        import zipfile
        import tempfile
        import re
        from pathlib import Path

        try:
            video_path = Path(video_path_str)
            subtitle_path = Path(subtitle_path_str)

            if not video_path.exists():
                raise Exception("ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            if not subtitle_path.exists():
                raise Exception("ìë§‰ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            cut_video_tasks[task_id]["message"] = "ìë§‰ íŒŒì¼ íŒŒì‹± ì¤‘..."

            # ìë§‰ íŒŒì¼ì—ì„œ ì‹œê°„ êµ¬ê°„ ì¶”ì¶œ
            file_ext = subtitle_path.suffix.lower()
            time_ranges = []

            with open(subtitle_path, 'r', encoding='utf-8') as f:
                content = f.read()

            if file_ext == '.srt':
                # SRT í˜•ì‹ íŒŒì‹±
                time_pattern = r'(\d{2}):(\d{2}):(\d{2})[,\.](\d{3})\s*-->\s*(\d{2}):(\d{2}):(\d{2})[,\.](\d{3})'
                matches = re.findall(time_pattern, content)

                for match in matches:
                    # ì‹œì‘ ì‹œê°„
                    start_h, start_m, start_s, start_ms = int(match[0]), int(match[1]), int(match[2]), int(match[3])
                    start_time = start_h * 3600 + start_m * 60 + start_s + start_ms / 1000

                    # ì¢…ë£Œ ì‹œê°„
                    end_h, end_m, end_s, end_ms = int(match[4]), int(match[5]), int(match[6]), int(match[7])
                    end_time = end_h * 3600 + end_m * 60 + end_s + end_ms / 1000

                    time_ranges.append((start_time, end_time))

            elif file_ext == '.vtt':
                # VTT í˜•ì‹ íŒŒì‹±
                time_pattern = r'(\d{2}):(\d{2}):(\d{2})\.(\d{3})\s*-->\s*(\d{2}):(\d{2}):(\d{2})\.(\d{3})'
                matches = re.findall(time_pattern, content)

                for match in matches:
                    # ì‹œì‘ ì‹œê°„
                    start_h, start_m, start_s, start_ms = int(match[0]), int(match[1]), int(match[2]), int(match[3])
                    start_time = start_h * 3600 + start_m * 60 + start_s + start_ms / 1000

                    # ì¢…ë£Œ ì‹œê°„
                    end_h, end_m, end_s, end_ms = int(match[4]), int(match[5]), int(match[6]), int(match[7])
                    end_time = end_h * 3600 + end_m * 60 + end_s + end_ms / 1000

                    time_ranges.append((start_time, end_time))
            else:
                raise Exception("ì§€ì›í•˜ì§€ ì•ŠëŠ” ìë§‰ íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. (.srt, .vtt ì§€ì›)")

            if not time_ranges:
                raise Exception("ìë§‰ ì‹œê°„ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            cut_video_tasks[task_id]["total"] = len(time_ranges)
            cut_video_tasks[task_id]["status"] = "cutting"
            cut_video_tasks[task_id]["message"] = f"ì˜ìƒ ìë¥´ê¸° ì‹œì‘ (ì´ {len(time_ranges)}ê°œ í´ë¦½)"

            # ì˜êµ¬ ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„± (ë‹¤ìš´ë¡œë“œ ì™„ë£Œë  ë•Œê¹Œì§€ ìœ ì§€)
            temp_dir = tempfile.mkdtemp()
            temp_path = Path(temp_dir)
            clip_files = []

            # ê° ì‹œê°„ êµ¬ê°„ë§ˆë‹¤ ì˜ìƒ ìë¥´ê¸°
            for idx, (start, end) in enumerate(time_ranges):
                duration = end - start
                if duration <= 0:
                    continue

                cut_video_tasks[task_id]["current"] = idx + 1
                cut_video_tasks[task_id]["progress"] = int((idx + 1) / len(time_ranges) * 100)
                cut_video_tasks[task_id]["message"] = f"í´ë¦½ {idx + 1}/{len(time_ranges)} ì²˜ë¦¬ ì¤‘..."

                output_file = temp_path / f"clip_{idx:04d}.mp4"

                # ffmpeg ëª…ë ¹ì–´ ì‹¤í–‰
                cmd = [
                    "ffmpeg",
                    "-ss", str(start),
                    "-i", str(video_path),
                    "-t", str(duration),
                    "-c", "copy",  # re-encoding ì—†ì´ ë¹ ë¥´ê²Œ ìë¥´ê¸°
                    "-y",  # ë®ì–´ì“°ê¸°
                    str(output_file)
                ]

                result = subprocess.run(cmd, capture_output=True, text=True)

                if result.returncode != 0:
                    logging.error(f"ffmpeg ì˜¤ë¥˜: {result.stderr}")
                    continue

                if output_file.exists():
                    clip_files.append(output_file)

            if not clip_files:
                raise Exception("ì˜ìƒ í´ë¦½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            cut_video_tasks[task_id]["message"] = "ZIP íŒŒì¼ ìƒì„± ì¤‘..."
            cut_video_tasks[task_id]["progress"] = 95

            # ZIP íŒŒì¼ ìƒì„±
            zip_path = temp_path / f"{video_path.stem}_clips.zip"
            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                for clip_file in clip_files:
                    zipf.write(clip_file, clip_file.name)

            cut_video_tasks[task_id].update({
                "status": "completed",
                "progress": 100,
                "message": f"ì™„ë£Œ! {len(clip_files)}ê°œ í´ë¦½ ìƒì„±ë¨",
                "zip_path": str(zip_path),
                "completed": True,
            })

        except Exception as e:
            logging.exception("ì˜ìƒ ìë¥´ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
            cut_video_tasks[task_id].update({
                "status": "error",
                "message": f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "error": str(e),
                "completed": True,
            })

    # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œë¡œ ì‹œì‘
    thread = threading.Thread(target=run_cut_video, daemon=True)
    thread.start()

    return {
        "task_id": task_id,
        "message": "ì˜ìƒ ìë¥´ê¸° ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
    }


@app.get("/api/video-analyzer/cut-status/{task_id}")
async def api_get_cut_video_status(task_id: str) -> Dict[str, Any]:
    """ì˜ìƒ ìë¥´ê¸° ì§„í–‰ ìƒíƒœ ì¡°íšŒ"""
    if task_id not in cut_video_tasks:
        raise HTTPException(status_code=404, detail="ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    return cut_video_tasks[task_id]


@app.get("/api/video-analyzer/cut-download/{task_id}")
async def api_download_cut_video(task_id: str):
    """ì™„ë£Œëœ ì˜ìƒ í´ë¦½ ZIP íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
    if task_id not in cut_video_tasks:
        raise HTTPException(status_code=404, detail="ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    task = cut_video_tasks[task_id]

    if task["status"] != "completed":
        raise HTTPException(status_code=400, detail="ì‘ì—…ì´ ì•„ì§ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    zip_path = task.get("zip_path")
    if not zip_path or not Path(zip_path).exists():
        raise HTTPException(status_code=404, detail="ZIP íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    return FileResponse(
        path=zip_path,
        media_type="application/zip",
        filename=Path(zip_path).name,
    )


@app.post("/api/video-analyzer/extract-frames-by-times")
async def api_extract_frames_by_times(payload: Dict[str, Any] = Body(...)) -> Dict[str, Any]:
    """íŠ¹ì • ì‹œê°„ ëª©ë¡ì— ëŒ€í•´ í”„ë ˆì„ ì¶”ì¶œ"""
    try:
        video_path = Path(payload.get("video_path", ""))
        times = payload.get("times", [])

        if not video_path.exists():
            raise HTTPException(status_code=404, detail="ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        if not times:
            raise HTTPException(status_code=400, detail="ì¶”ì¶œí•  ì‹œê°„ ëª©ë¡ì´ í•„ìš”í•©ë‹ˆë‹¤.")

        # OpenCVë¡œ ì˜ìƒ ì—´ê¸°
        transcoded_file = None
        cap, transcoded_file = open_video_with_transcode_fallback(video_path)

        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps if fps > 0 else 0

        frames_data = []

        for time in times:
            frame_number = int(time * fps)

            if frame_number >= total_frames:
                frame_number = total_frames - 1

            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)
            ret, frame = cap.read()

            if not ret:
                continue

            # JPEGë¡œ ì¸ì½”ë”©
            success, buffer = cv2.imencode('.jpg', frame)

            if not success:
                continue

            # Base64 ì¸ì½”ë”©
            frame_base64 = base64.b64encode(buffer).decode('utf-8')

            frames_data.append({
                "frame_data": frame_base64,
                "timestamp": round(time, 2)
            })

        cap.release()

        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if transcoded_file and transcoded_file.exists():
            try:
                transcoded_file.unlink()
            except Exception:
                pass

        return {
            "frames": frames_data,
            "total_extracted": len(frames_data)
        }

    except HTTPException:
        # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if 'transcoded_file' in locals() and transcoded_file and transcoded_file.exists():
            try:
                transcoded_file.unlink()
            except Exception:
                pass
        raise
    except Exception as e:
        # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if 'transcoded_file' in locals() and transcoded_file and transcoded_file.exists():
            try:
                transcoded_file.unlink()
            except Exception:
                pass
        logging.exception("í”„ë ˆì„ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
        raise HTTPException(status_code=500, detail=f"í”„ë ˆì„ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")


@app.post("/api/video-analyzer/merge-videos")
async def api_merge_videos(payload: Dict[str, Any] = Body(...)) -> Dict[str, Any]:
    """ì—¬ëŸ¬ ì˜ìƒì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°"""
    import subprocess
    import tempfile
    import json

    def get_video_duration(video_path: str) -> float:
        """ffprobeë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ìƒì˜ ê¸¸ì´ë¥¼ ì´ˆ ë‹¨ìœ„ë¡œ ë°˜í™˜"""
        cmd = [
            'ffprobe',
            '-v', 'error',
            '-show_entries', 'format=duration',
            '-of', 'json',
            str(video_path)
        ]
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode != 0:
            return 0.0
        try:
            data = json.loads(result.stdout)
            return float(data['format']['duration'])
        except:
            return 0.0

    def format_srt_time(seconds: float) -> str:
        """ì´ˆë¥¼ SRT ì‹œê°„ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (HH:MM:SS,mmm)"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        millis = int((seconds % 1) * 1000)
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"

    def create_subtitle_file(video_paths: list, output_path: Path) -> str:
        """ê° ì˜ìƒì˜ ì œëª©ì„ ìë§‰ìœ¼ë¡œ ì¶”ê°€í•œ SRT íŒŒì¼ ìƒì„±"""
        subtitle_entries = []
        current_time = 0.0

        for idx, video_path in enumerate(video_paths):
            # ì˜ìƒ íŒŒì¼ëª…ì—ì„œ ì œëª© ì¶”ì¶œ (í™•ì¥ì ì œê±°)
            video_name = Path(video_path).stem

            # ì˜ìƒ ê¸¸ì´ ê°€ì ¸ì˜¤ê¸°
            duration = get_video_duration(video_path)

            # ìë§‰ ì—”íŠ¸ë¦¬ ìƒì„± (ê° ì˜ìƒì˜ ì „ì²´ ê¸¸ì´ ë™ì•ˆ ì œëª© í‘œì‹œ)
            start_time = current_time
            end_time = current_time + duration  # ì˜ìƒ ì „ì²´ ê¸¸ì´ ë™ì•ˆ í‘œì‹œ

            subtitle_entries.append({
                'index': idx + 1,
                'start': format_srt_time(start_time),
                'end': format_srt_time(end_time),
                'text': f"ğŸ“Œ ì œëª© [{idx + 1}] {video_name}"
            })

            # ë‹¤ìŒ ì˜ìƒ ì‹œì‘ ì‹œê°„ ì—…ë°ì´íŠ¸
            current_time += duration

        # SRT íŒŒì¼ ë‚´ìš© ìƒì„±
        srt_content = []
        for entry in subtitle_entries:
            srt_content.append(str(entry['index']))
            srt_content.append(f"{entry['start']} --> {entry['end']}")
            srt_content.append(entry['text'])
            srt_content.append("")  # ë¹ˆ ì¤„

        # SRT íŒŒì¼ ì €ì¥
        subtitle_path = output_path.with_suffix('.srt')
        subtitle_path.write_text('\n'.join(srt_content), encoding='utf-8')

        return str(subtitle_path)

    try:
        video_paths = payload.get("video_paths", [])
        output_name = payload.get("output_name", "merged_video.mp4")

        if len(video_paths) < 2:
            raise HTTPException(status_code=400, detail="ìµœì†Œ 2ê°œì˜ ì˜ìƒì´ í•„ìš”í•©ë‹ˆë‹¤.")

        # ì²« ë²ˆì§¸ ì˜ìƒì˜ ë””ë ‰í† ë¦¬ë¥¼ ì¶œë ¥ ë””ë ‰í† ë¦¬ë¡œ ì‚¬ìš©
        first_video = Path(video_paths[0])
        output_dir = first_video.parent
        output_path = output_dir / output_name

        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        for video_path in video_paths:
            if not Path(video_path).exists():
                raise HTTPException(status_code=404, detail=f"ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")

        # FFmpeg concat íŒŒì¼ ìƒì„±
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as concat_file:
            concat_file_path = concat_file.name
            for video_path in video_paths:
                # FFmpeg concat í˜•ì‹: file '/absolute/path/to/video.mp4'
                concat_file.write(f"file '{Path(video_path).absolute()}'\n")

        try:
            # ê° ì˜ìƒ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (í•´ìƒë„, ì½”ë± ë“± í™•ì¸)
            # ì„œë¡œ ë‹¤ë¥¸ í¬ë§·ì˜ ì˜ìƒì„ í•©ì¹˜ë ¤ë©´ ì¬ì¸ì½”ë”© í•„ìš”

            # FFmpeg filter_complexë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ì˜ìƒì„ ë™ì¼í•œ í¬ë§·ìœ¼ë¡œ ì •ê·œí™”
            # 1. ëª¨ë“  ì˜ìƒì„ ë™ì¼í•œ í•´ìƒë„ë¡œ ìŠ¤ì¼€ì¼ë§
            # 2. ë™ì¼í•œ í”„ë ˆì„ë ˆì´íŠ¸ ì ìš©
            # 3. h264 ì½”ë±ìœ¼ë¡œ ì¬ì¸ì½”ë”©

            # ì…ë ¥ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ìƒì„±
            input_args = []
            for video_path in video_paths:
                input_args.extend(['-i', str(Path(video_path).absolute())])

            # filter_complex êµ¬ì„±
            # ê° ì…ë ¥ì„ 1080p, 30fpsë¡œ ì •ê·œí™”í•œ í›„ concat
            filter_parts = []
            for i in range(len(video_paths)):
                # scale: 1080pë¡œ ìŠ¤ì¼€ì¼ë§ (ë¹„ìœ¨ ìœ ì§€, íŒ¨ë”© ì¶”ê°€)
                # fps: 30fpsë¡œ ë³€í™˜
                # setsar: ì •ì‚¬ê°í˜• í”½ì…€ ë¹„ìœ¨ ì„¤ì •
                filter_parts.append(
                    f"[{i}:v]scale=1080:1920:force_original_aspect_ratio=decrease,"
                    f"pad=1080:1920:(ow-iw)/2:(oh-ih)/2,fps=30,setsar=1[v{i}];"
                )

            # concat í•„í„° ì¶”ê°€
            concat_inputs = ''.join([f"[v{i}]" for i in range(len(video_paths))])
            filter_parts.append(f"{concat_inputs}concat=n={len(video_paths)}:v=1:a=0[outv]")

            filter_complex = ''.join(filter_parts)

            cmd = [
                '/usr/bin/ffmpeg',  # ì‹œìŠ¤í…œ ffmpeg ì‚¬ìš© (AV1 ì§€ì›)
                *input_args,
                '-filter_complex', filter_complex,
                '-map', '[outv]',
                '-c:v', 'libx264',  # h264 ì½”ë± ì‚¬ìš©
                '-preset', 'medium',  # ì¸ì½”ë”© ì†ë„/í’ˆì§ˆ ê· í˜•
                '-crf', '23',  # í’ˆì§ˆ ì„¤ì • (ë‚®ì„ìˆ˜ë¡ ê³ í’ˆì§ˆ)
                '-y',  # ì¶œë ¥ íŒŒì¼ ë®ì–´ì“°ê¸°
                str(output_path)
            ]

            logging.info(f"FFmpeg ëª…ë ¹ ì‹¤í–‰: {' '.join(cmd)}")

            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=600  # 10ë¶„ íƒ€ì„ì•„ì›ƒ (ì¬ì¸ì½”ë”©ì´ë¯€ë¡œ ì‹œê°„ ë” í•„ìš”)
            )

            if result.returncode != 0:
                logging.error(f"FFmpeg ì—ëŸ¬: {result.stderr}")
                raise HTTPException(
                    status_code=500,
                    detail=f"ì˜ìƒ í•©ì¹˜ê¸° ì‹¤íŒ¨: {result.stderr}"
                )

            # ì¶œë ¥ íŒŒì¼ í¬ê¸° í™•ì¸
            if output_path.exists():
                size_mb = output_path.stat().st_size / (1024 * 1024)

                # ìë§‰ íŒŒì¼ ìƒì„±
                subtitle_path = create_subtitle_file(video_paths, output_path)
                logging.info(f"ìë§‰ íŒŒì¼ ìƒì„±: {subtitle_path}")

                return {
                    "success": True,
                    "output_path": str(output_path),
                    "subtitle_path": subtitle_path,
                    "size_mb": round(size_mb, 2),
                    "video_count": len(video_paths)
                }
            else:
                raise HTTPException(status_code=500, detail="ì¶œë ¥ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        finally:
            # ì„ì‹œ concat íŒŒì¼ ì‚­ì œ
            try:
                Path(concat_file_path).unlink()
            except:
                pass

    except HTTPException:
        raise
    except subprocess.TimeoutExpired:
        raise HTTPException(status_code=500, detail="ì˜ìƒ í•©ì¹˜ê¸° ì‹œê°„ ì´ˆê³¼ (5ë¶„)")
    except Exception as e:
        logging.exception("ì˜ìƒ í•©ì¹˜ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
        raise HTTPException(status_code=500, detail=f"ì˜ìƒ í•©ì¹˜ê¸° ì‹¤íŒ¨: {str(e)}")


@app.post("/api/video-analyzer/create-preview-video")
async def api_create_preview_video(payload: Dict[str, Any] = Body(...)) -> Dict[str, Any]:
    """í…ìŠ¤íŠ¸ ì˜¤ë²„ë ˆì´ì™€ ìë§‰ì´ ì ìš©ëœ í”„ë¦¬ë·° ì˜ìƒ ìƒì„±"""
    import subprocess
    import tempfile

    try:
        video_path = payload.get("video_path")
        subtitle_path = payload.get("subtitle_path")
        video_width = payload.get("video_width")
        video_height = payload.get("video_height")
        overlays = payload.get("overlays", {})
        black_bars = payload.get("black_bars", {})

        if not video_path or not Path(video_path).exists():
            raise HTTPException(status_code=404, detail="ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        video_file = Path(video_path)
        output_dir = video_file.parent
        output_name = f"{video_file.stem}_preview.mp4"
        output_path = output_dir / output_name

        # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„± (ì´ëª¨ì§€ PNG ì˜¤ë²„ë ˆì´ ì €ì¥ìš©)
        temp_dir = Path(tempfile.mkdtemp())

        # ì¶œë ¥ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ê³ ìœ í•œ ì´ë¦„ ìƒì„±
        counter = 1
        while output_path.exists():
            output_name = f"{video_file.stem}_preview_{counter}.mp4"
            output_path = output_dir / output_name
            counter += 1

        logging.info(f"ğŸ¬ í”„ë¦¬ë·° ì˜ìƒ ìƒì„± ì‹œì‘: {video_path}")
        logging.info(f"ğŸ“ ë¹„ë””ì˜¤ í¬ê¸°: {video_width}x{video_height}")
        logging.info(f"ğŸ“ ì˜¤ë²„ë ˆì´: {overlays}")
        logging.info(f"â¬› ê²€ì • ë°°ê²½: {black_bars}")

        # FFmpeg filter_complex êµ¬ì„±
        filters = []

        # 1. ê²€ì • ë°°ê²½ ì¶”ê°€
        if black_bars.get("top", {}).get("enabled"):
            top_height_percent = black_bars["top"].get("height", 15)
            top_opacity = black_bars["top"].get("opacity", 0.8)
            top_height = int(video_height * top_height_percent / 100)
            filters.append(
                f"drawbox=x=0:y=0:w={video_width}:h={top_height}:color=black@{top_opacity}:t=fill"
            )

        if black_bars.get("bottom", {}).get("enabled"):
            bottom_height_percent = black_bars["bottom"].get("height", 15)
            bottom_opacity = black_bars["bottom"].get("opacity", 0.8)
            bottom_height = int(video_height * bottom_height_percent / 100)
            bottom_y = video_height - bottom_height
            filters.append(
                f"drawbox=x=0:y={bottom_y}:w={video_width}:h={bottom_height}:color=black@{bottom_opacity}:t=fill"
            )

        # 2. í…ìŠ¤íŠ¸ ì˜¤ë²„ë ˆì´ ì¶”ê°€ (ì œëª©, ë¶€ì œëª©, í•œê¸€ ìë§‰, ì˜ì–´ ìë§‰)
        png_overlays = []  # PNG ì˜¤ë²„ë ˆì´ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
        for overlay_key, overlay_data in overlays.items():
            # ì´ëª¨ì§€ ê°ì§€
            text = str(overlay_data.get("text", ""))
            if _contains_emoji(text):
                # ì´ëª¨ì§€ê°€ ìˆìœ¼ë©´ PNGë¡œ ë Œë”ë§
                logging.info(f"ğŸ¨ ì´ëª¨ì§€ ê°ì§€: PNG ë Œë”ë§ ì‚¬ìš© - {overlay_key}")
                result = await _create_overlay_png_filter(overlay_data, video_width, video_height, temp_dir)
                if result:
                    _, meta = result
                    if meta and meta.get("png_path"):
                        png_overlays.append(meta)
                        logging.info(
                            "ğŸ“ PNG ì˜¤ë²„ë ˆì´ ì¶”ê°€: %s - í°íŠ¸í¬ê¸°=%spx, PNG=%s",
                            overlay_key,
                            meta.get("font_size"),
                            meta.get("png_path"),
                        )
            else:
                # ì´ëª¨ì§€ê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ drawtext ì‚¬ìš©
                result = _build_drawtext_filter(overlay_data, video_width, video_height)
                if result:
                    drawtext_filter, meta = result
                    logging.info(
                        "ğŸ“ í…ìŠ¤íŠ¸ ì˜¤ë²„ë ˆì´ ì¶”ê°€: %s - í°íŠ¸í¬ê¸°=%spx, í°íŠ¸=%s",
                        overlay_key,
                        meta.get("font_size"),
                        meta.get("fontfile") or "default",
                    )
                    logging.info(f"   FFmpeg í•„í„°: {drawtext_filter}")
                    filters.append(drawtext_filter)

        # 3. ìë§‰ íŒŒì¼ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
        if subtitle_path and Path(subtitle_path).exists():
            # ìë§‰ íŒŒì¼ ì ˆëŒ€ ê²½ë¡œ
            subtitle_path_abs = str(Path(subtitle_path).absolute())
            # FFmpeg subtitles í•„í„°: ê²½ë¡œì˜ íŠ¹ìˆ˜ë¬¸ì ì´ìŠ¤ì¼€ì´í”„
            # ì½œë¡ ê³¼ ë°±ìŠ¬ë˜ì‹œë¥¼ ì´ìŠ¤ì¼€ì´í”„ (FFmpeg filter ë¬¸ë²•)
            subtitle_path_escaped = subtitle_path_abs.replace("\\", "\\\\\\\\").replace(":", "\\:").replace("'", "\\'")

            # ìë§‰ ìŠ¤íƒ€ì¼ ì„¤ì • (í¬ê³  ì˜ ë³´ì´ë„ë¡)
            # force_style: ìë§‰ ìŠ¤íƒ€ì¼ ê°•ì œ ì ìš©
            # FontSize: í°íŠ¸ í¬ê¸° (ê¸°ë³¸ê°’ë³´ë‹¤ í¬ê²Œ)
            # PrimaryColour: ë…¸ë€ìƒ‰ (&H00FFFF)
            # OutlineColour: ê²€ì • í…Œë‘ë¦¬ (&H000000)
            # BorderStyle: í…Œë‘ë¦¬ ìŠ¤íƒ€ì¼ (1 = í…Œë‘ë¦¬ + ê·¸ë¦¼ì)
            # Outline: í…Œë‘ë¦¬ ë‘ê»˜
            # Shadow: ê·¸ë¦¼ì ê¹Šì´
            # MarginV: í•˜ë‹¨ ì—¬ë°±
            style = "FontName=Arial,FontSize=48,PrimaryColour=&H00FFFF,OutlineColour=&H000000,BorderStyle=1,Outline=3,Shadow=2,MarginV=80"
            filters.append(f"subtitles={subtitle_path_escaped}:force_style='{style}'")

        # FFmpeg ëª…ë ¹ì–´ êµ¬ì„±
        cmd = [
            "/usr/bin/ffmpeg",
            "-i", str(video_file.absolute()),
        ]

        # PNG ì˜¤ë²„ë ˆì´ê°€ ìˆìœ¼ë©´ ë³„ë„ ì…ë ¥ìœ¼ë¡œ ì¶”ê°€
        if png_overlays:
            for png_meta in png_overlays:
                cmd.extend(["-i", png_meta["png_path"]])

        cmd.append("-y")  # íŒŒì¼ ë®ì–´ì“°ê¸°

        # í•„í„° êµ¬ì„±
        if png_overlays:
            # PNG ì˜¤ë²„ë ˆì´ê°€ ìˆìœ¼ë©´ filter_complex ì‚¬ìš©
            filter_parts = []

            # ê¸°ë³¸ ë¹„ë””ì˜¤ í•„í„° ì ìš©
            if filters:
                base_filter = ",".join(filters)
                filter_parts.append(f"[0:v]{base_filter}[v0]")
                current_input = "[v0]"
            else:
                current_input = "[0:v]"

            # PNG ì˜¤ë²„ë ˆì´ ì¶”ê°€
            for idx, png_meta in enumerate(png_overlays, start=1):
                y_pixel = png_meta.get("y_pixel", video_height // 2)
                # overlay í•„í„°: PNGë¥¼ ë¹„ë””ì˜¤ ì¤‘ì•™ì— ë°°ì¹˜
                overlay_expr = f"{current_input}[{idx}:v]overlay=(W-w)/2:{y_pixel}"
                if idx < len(png_overlays):
                    filter_parts.append(f"{overlay_expr}[v{idx}]")
                    current_input = f"[v{idx}]"
                else:
                    # ë§ˆì§€ë§‰ ì˜¤ë²„ë ˆì´
                    filter_parts.append(overlay_expr)

            filter_complex = ";".join(filter_parts)
            cmd.extend(["-filter_complex", filter_complex])
        elif filters:
            # PNG ì˜¤ë²„ë ˆì´ê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ëŒ€ë¡œ -vf ì‚¬ìš©
            filter_string = ",".join(filters)
            cmd.extend(["-vf", filter_string])

        # ì¶œë ¥ ì˜µì…˜
        cmd.extend([
            "-c:v", "libx264",
            "-preset", "medium",
            "-crf", "23",
            "-c:a", "copy",  # ì˜¤ë””ì˜¤ëŠ” ë³µì‚¬
            str(output_path)
        ])

        logging.info(f"ğŸ¬ FFmpeg ëª…ë ¹ì–´: {' '.join(cmd)}")

        # FFmpeg ì‹¤í–‰
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=600  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
        )

        if result.returncode != 0:
            logging.error(f"FFmpeg ì—ëŸ¬: {result.stderr}")
            raise HTTPException(
                status_code=500,
                detail=f"í”„ë¦¬ë·° ì˜ìƒ ìƒì„± ì‹¤íŒ¨: {result.stderr}"
            )

        # ì¶œë ¥ íŒŒì¼ í™•ì¸
        if output_path.exists():
            size_mb = output_path.stat().st_size / (1024 * 1024)
            logging.info(f"âœ… í”„ë¦¬ë·° ì˜ìƒ ìƒì„± ì™„ë£Œ: {output_path} ({size_mb:.2f}MB)")

            return {
                "success": True,
                "preview_path": str(output_path),
                "file_name": output_name,
                "size_mb": round(size_mb, 2)
            }
        else:
            raise HTTPException(status_code=500, detail="í”„ë¦¬ë·° ì˜ìƒ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    except HTTPException:
        raise
    except subprocess.TimeoutExpired:
        raise HTTPException(status_code=500, detail="í”„ë¦¬ë·° ì˜ìƒ ìƒì„± ì‹œê°„ ì´ˆê³¼ (10ë¶„)")
    except Exception as e:
        logging.exception("í”„ë¦¬ë·° ì˜ìƒ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
        raise HTTPException(status_code=500, detail=f"í”„ë¦¬ë·° ì˜ìƒ ìƒì„± ì‹¤íŒ¨: {str(e)}")
    finally:
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
        if 'temp_dir' in locals() and temp_dir and temp_dir.exists():
            try:
                import shutil
                shutil.rmtree(temp_dir)
                logging.info(f"ğŸ—‘ï¸  ì„ì‹œ ë””ë ‰í† ë¦¬ ì‚­ì œ: {temp_dir}")
            except Exception as e:
                logging.warning(f"ì„ì‹œ ë””ë ‰í† ë¦¬ ì‚­ì œ ì‹¤íŒ¨: {temp_dir}, {e}")


def srt_time_to_seconds(srt_time: str) -> float:
    """SRT ì‹œê°„ í˜•ì‹ (00:00:05,000)ì„ ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜"""
    # 00:00:05,000 â†’ 5.0
    hours, minutes, seconds_ms = srt_time.split(':')
    seconds, milliseconds = seconds_ms.replace(',', '.').split('.')
    total_seconds = int(hours) * 3600 + int(minutes) * 60 + int(seconds) + int(milliseconds) / 1000
    return total_seconds


@app.post("/api/video-analyzer/create-final-video")
async def api_create_final_video(
    video_path: str = Form(...),
    video_width: int = Form(...),
    video_height: int = Form(...),
    overlays: str = Form(...),
    black_bars: str = Form(...),
    tracks: str = Form(...),
    subtitle_style: str = Form("{}"),
    canvas_subtitle_positions: str = Form(None),
    audio_file: UploadFile = File(None),
    commentary_file: UploadFile = File(None),
    bgm_file: UploadFile = File(None)
) -> Dict[str, Any]:
    """ì²´í¬ëœ íŠ¸ë™ì„ í•©ì„±í•˜ì—¬ ìµœì¢… MP4 ì˜ìƒ ìƒì„±"""
    import subprocess
    import tempfile
    import shutil
    import time

    # â±ï¸ ì„±ëŠ¥ ì¸¡ì • ì‹œì‘
    perf_start_total = time.time()
    perf_marks = {}

    try:
        # JSON íŒŒì‹± ì„±ëŠ¥ ì¸¡ì •
        perf_start = time.time()
        overlays_data = json.loads(overlays)
        black_bars_data = json.loads(black_bars)
        tracks_data = json.loads(tracks)
        subtitle_style_data = json.loads(subtitle_style) if subtitle_style else {}
        canvas_positions_data = json.loads(canvas_subtitle_positions) if canvas_subtitle_positions else None
        perf_marks['json_parsing'] = time.time() - perf_start

        if not video_path or not Path(video_path).exists():
            raise HTTPException(status_code=404, detail="ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        video_file = Path(video_path)
        output_dir = video_file.parent
        output_name = f"{video_file.stem}_final.mp4"
        output_path = output_dir / output_name

        # ì¶œë ¥ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ê³ ìœ í•œ ì´ë¦„ ìƒì„±
        counter = 1
        while output_path.exists():
            output_name = f"{video_file.stem}_final_{counter}.mp4"
            output_path = output_dir / output_name
            counter += 1

        logging.info(f"ğŸ¬ ìµœì¢… ì˜ìƒ ìƒì„± ì‹œì‘: {video_path}")
        logging.info(f"ğŸ“ ë¹„ë””ì˜¤ í¬ê¸°: {video_width}x{video_height}")
        logging.info(f"ğŸ“ ì˜¤ë²„ë ˆì´: {overlays_data}")
        logging.info(f"â¬› ê²€ì • ë°°ê²½: {black_bars_data}")
        logging.info(f"ğŸ­ ìë§‰ ìŠ¤íƒ€ì¼: {subtitle_style_data}")
        logging.info(f"ğŸ“ Canvas ìë§‰ ìœ„ì¹˜: {canvas_positions_data}")
        logging.info(f"ğŸµ íŠ¸ë™: {tracks_data}")

        # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        perf_start = time.time()
        temp_dir = Path(tempfile.mkdtemp())
        temp_audio_files = []
        perf_marks['temp_dir_creation'] = time.time() - perf_start

        try:
            # ì—…ë¡œë“œëœ ì˜¤ë””ì˜¤ íŒŒì¼ ì €ì¥
            perf_start = time.time()
            audio_inputs = []

            if audio_file:
                audio_path = temp_dir / audio_file.filename
                with open(audio_path, "wb") as f:
                    shutil.copyfileobj(audio_file.file, f)
                temp_audio_files.append(audio_path)
                audio_inputs.append(str(audio_path))
                logging.info(f"ğŸµ Audio íŒŒì¼ ì €ì¥: {audio_path}")

            if commentary_file:
                commentary_path = temp_dir / commentary_file.filename
                with open(commentary_path, "wb") as f:
                    shutil.copyfileobj(commentary_file.file, f)
                temp_audio_files.append(commentary_path)
                audio_inputs.append(str(commentary_path))
                logging.info(f"ğŸ™ï¸ Commentary íŒŒì¼ ì €ì¥: {commentary_path}")

            if bgm_file:
                bgm_path = temp_dir / bgm_file.filename
                with open(bgm_path, "wb") as f:
                    shutil.copyfileobj(bgm_file.file, f)
                temp_audio_files.append(bgm_path)
                audio_inputs.append(str(bgm_path))
                logging.info(f"ğŸµ BGM íŒŒì¼ ì €ì¥: {bgm_path}")

            perf_marks['audio_file_save'] = time.time() - perf_start

            # FFmpeg filter_complex êµ¬ì„±
            perf_start = time.time()
            video_filters = []

            # 1. ê²€ì • ë°°ê²½ ì¶”ê°€
            if black_bars_data.get("top", {}).get("enabled"):
                top_height_percent = black_bars_data["top"].get("height", 15)
                top_opacity = black_bars_data["top"].get("opacity", 0.8)  # 0-1 ë²”ìœ„ë¡œ ì „ì†¡ë¨
                top_height = int(video_height * top_height_percent / 100)
                logging.info(f"â¬› ìƒë‹¨ ê²€ì •ë°”: ë†’ì´={top_height_percent}%, íˆ¬ëª…ë„={top_opacity} ({top_opacity * 100:.0f}%)")
                video_filters.append(
                    f"drawbox=x=0:y=0:w={video_width}:h={top_height}:color=black@{top_opacity}:t=fill"
                )

            if black_bars_data.get("bottom", {}).get("enabled"):
                bottom_height_percent = black_bars_data["bottom"].get("height", 15)
                bottom_opacity = black_bars_data["bottom"].get("opacity", 0.8)  # 0-1 ë²”ìœ„ë¡œ ì „ì†¡ë¨
                bottom_height = int(video_height * bottom_height_percent / 100)
                bottom_y = video_height - bottom_height
                logging.info(f"â¬› í•˜ë‹¨ ê²€ì •ë°”: ë†’ì´={bottom_height_percent}%, íˆ¬ëª…ë„={bottom_opacity} ({bottom_opacity * 100:.0f}%)")
                video_filters.append(
                    f"drawbox=x=0:y={bottom_y}:w={video_width}:h={bottom_height}:color=black@{bottom_opacity}:t=fill"
                )

            # 2. ì œëª©/ë¶€ì œëª© ì˜¤ë²„ë ˆì´ ë¨¼ì € ì¶”ê°€ (ë°°ë„ˆë³´ë‹¤ ì•„ë˜ ë ˆì´ì–´)
            # âš ï¸ korean, english, japaneseëŠ” SRT ìë§‰ìœ¼ë¡œ ì²˜ë¦¬ë˜ë¯€ë¡œ drawtextì—ì„œ ì œì™¸
            logging.info(f"ğŸ“¦ ë°›ì€ overlays_data: {overlays_data}")
            png_overlays = []  # PNG ì˜¤ë²„ë ˆì´ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
            overlay_drawtext = {}  # title/subtitle drawtext í•„í„° ì €ì¥ (ìˆ˜ì§ ìŠ¤íƒ ì ìš©ìš©)
            for overlay_key, overlay_data in overlays_data.items():
                # korean, english, japaneseëŠ” SRT subtitles í•„í„°ë¡œ ì²˜ë¦¬ë˜ë¯€ë¡œ ê±´ë„ˆëœ€
                if overlay_key in {"korean", "english", "japanese"}:
                    logging.info(f"â­ï¸  í…ìŠ¤íŠ¸ ì˜¤ë²„ë ˆì´ ê±´ë„ˆëœ€: {overlay_key} (SRT ìë§‰ìœ¼ë¡œ ì²˜ë¦¬ë¨)")
                    continue

                logging.info(f"ğŸ” ì²˜ë¦¬ ì¤‘ì¸ ì˜¤ë²„ë ˆì´: {overlay_key} = {overlay_data}")

                # ì´ëª¨ì§€ ê°ì§€
                text = str(overlay_data.get("text", ""))
                if _contains_emoji(text):
                    # ì´ëª¨ì§€ê°€ ìˆìœ¼ë©´ PNGë¡œ ë Œë”ë§
                    logging.info(f"ğŸ¨ ì´ëª¨ì§€ ê°ì§€: PNG ë Œë”ë§ ì‚¬ìš© - {overlay_key}")
                    result = await _create_overlay_png_filter(overlay_data, video_width, video_height, temp_dir)
                    if result:
                        _, meta = result
                        if meta and meta.get("png_path"):
                            # ìˆ˜ì§ ìŠ¤íƒì„ ìœ„í•´ overlay_type ì¶”ê°€
                            meta["overlay_type"] = overlay_key
                            png_overlays.append(meta)
                            logging.info(
                                "ğŸ“ PNG ì˜¤ë²„ë ˆì´ ì¶”ê°€: %s - í°íŠ¸í¬ê¸°=%spx, PNG=%s",
                                overlay_key,
                                meta.get("font_size"),
                                meta.get("png_path"),
                            )
                else:
                    # ì´ëª¨ì§€ê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ drawtext ì‚¬ìš©
                    result = _build_drawtext_filter(overlay_data, video_width, video_height)
                    if result:
                        drawtext_filter, meta = result
                        logging.info(
                            "ğŸ“ í…ìŠ¤íŠ¸ ì˜¤ë²„ë ˆì´ ì¶”ê°€: %s - í°íŠ¸í¬ê¸°=%spx, ìœ„ì¹˜=(%s, %s), í°íŠ¸=%s",
                            overlay_key,
                            meta.get("font_size"),
                            overlay_data.get("x"),
                            overlay_data.get("y"),
                            meta.get("fontfile") or "default",
                        )
                        logging.info(f"   FFmpeg í•„í„°: {drawtext_filter}")
                        # title/subtitleì€ ë‚˜ì¤‘ì— ìˆ˜ì§ ìŠ¤íƒ ìœ„ì¹˜ë¡œ êµì²´í•˜ê¸° ìœ„í•´ ë³„ë„ ì €ì¥
                        overlay_drawtext[overlay_key] = {
                            "filter": drawtext_filter,
                            "meta": meta,
                            "data": overlay_data
                        }
                        # âš ï¸ video_filtersì—ëŠ” ì¶”ê°€í•˜ì§€ ì•ŠìŒ (PNG ì´í›„ì— ë³„ë„ë¡œ ì²˜ë¦¬)
                        logging.info(f"ğŸ“¦ overlay_drawtextì—ë§Œ ì €ì¥ (video_filters ì œì™¸): {overlay_key}")

            # 3. ë°°ë„ˆ í…œí”Œë¦¿ ì²˜ë¦¬ (ì œëª©/ë¶€ì œëª© ìœ„ì— ë°°ì¹˜)
            template_type = subtitle_style_data.get("template", "classic")
            if template_type == "banner":
                banner_primary = subtitle_style_data.get("banner_primary_text")
                banner_secondary = subtitle_style_data.get("banner_secondary_text")

                # ë°°ë„ˆ ë°°ê²½ ì¶”ê°€ (ìƒë‹¨ 21%)
                banner_height = int(video_height * 0.21)
                video_filters.append(
                    f"drawbox=x=0:y=0:w={video_width}:h={banner_height}:color=black@0.92:t=fill"
                )

                # CJK í°íŠ¸ ê²½ë¡œ ì„¤ì • (í•œê¸€, ì¼ë³¸ì–´, ì¤‘êµ­ì–´ ì§€ì›)
                cjk_font_path = "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc"

                # ì£¼ í…ìŠ¤íŠ¸ (ìƒë‹¨)
                if banner_primary:
                    primary_y = int(banner_height * 0.35)
                    primary_size = int(video_height * 0.025)  # 2.5% of height
                    # í…ìŠ¤íŠ¸ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
                    escaped_primary = banner_primary.replace("'", "'\\''").replace(":", "\\:")
                    primary_filter = f"drawtext=text='{escaped_primary}':fontfile={cjk_font_path}:fontsize={primary_size}:fontcolor=white:borderw=2:bordercolor=black:x=(w-text_w)/2:y={primary_y}"
                    video_filters.append(primary_filter)
                    logging.info(f"ğŸ­ ë°°ë„ˆ ì£¼ í…ìŠ¤íŠ¸: {banner_primary} (í¬ê¸°: {primary_size}px, í°íŠ¸: Noto Sans CJK)")

                # ë³´ì¡° í…ìŠ¤íŠ¸ (í•˜ë‹¨) - ë¹„ì–´ìˆìœ¼ë©´ ì‹¤ì‹œê°„ ìë§‰ í‘œì‹œë¨
                if banner_secondary:
                    secondary_y = int(banner_height * 0.72)
                    secondary_size = int(video_height * 0.022)  # 2.2% of height
                    # í…ìŠ¤íŠ¸ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
                    escaped_secondary = banner_secondary.replace("'", "'\\''").replace(":", "\\:")
                    secondary_filter = f"drawtext=text='{escaped_secondary}':fontfile={cjk_font_path}:fontsize={secondary_size}:fontcolor=#ffd400:borderw=2:bordercolor=black:x=(w-text_w)/2:y={secondary_y}"
                    video_filters.append(secondary_filter)
                    logging.info(f"ğŸ­ ë°°ë„ˆ ë³´ì¡° í…ìŠ¤íŠ¸: {banner_secondary} (í¬ê¸°: {secondary_size}px, í°íŠ¸: Noto Sans CJK)")
                else:
                    logging.info("ğŸ’¡ ë°°ë„ˆ ë³´ì¡° í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŒ - í•˜ë‹¨ì— ì‹¤ì‹œê°„ ìë§‰ í‘œì‹œ")

            # 4. ìë§‰ íŒŒì¼ ìƒì„± (SRT í˜•ì‹)
            subtitle_files = []

            # ì£¼ìë§‰ (translationSubtitle)
            if tracks_data.get("translationSubtitle", {}).get("enabled") and tracks_data.get("translationSubtitle", {}).get("data"):
                translation_srt = temp_dir / "translation.srt"
                with open(translation_srt, "w", encoding="utf-8") as f:
                    for idx, sub in enumerate(tracks_data["translationSubtitle"]["data"], 1):
                        f.write(f"{idx}\n")
                        f.write(f"{sub['start']} --> {sub['end']}\n")
                        f.write(f"{sub['text']}\n\n")
                subtitle_files.append(("translation", str(translation_srt)))
                logging.info(f"ğŸ“ ì£¼ìë§‰ íŒŒì¼ ìƒì„±: {translation_srt}")

            # ì¼ë³¸ì–´ ìë§‰ (japaneseSubtitle)
            if tracks_data.get("japaneseSubtitle", {}).get("enabled") and tracks_data.get("japaneseSubtitle", {}).get("data"):
                japanese_srt = temp_dir / "japanese.srt"
                with open(japanese_srt, "w", encoding="utf-8") as f:
                    for idx, sub in enumerate(tracks_data["japaneseSubtitle"]["data"], 1):
                        f.write(f"{idx}\n")
                        f.write(f"{sub['start']} --> {sub['end']}\n")
                        f.write(f"{sub['text']}\n\n")
                subtitle_files.append(("japanese", str(japanese_srt)))
                logging.info(f"ğŸ“ ì¼ë³¸ì–´ìë§‰ íŒŒì¼ ìƒì„±: {japanese_srt}")

            # ë³´ì¡°ìë§‰ (descriptionSubtitle)
            if tracks_data.get("descriptionSubtitle", {}).get("enabled") and tracks_data.get("descriptionSubtitle", {}).get("data"):
                description_srt = temp_dir / "description.srt"
                with open(description_srt, "w", encoding="utf-8") as f:
                    for idx, sub in enumerate(tracks_data["descriptionSubtitle"]["data"], 1):
                        f.write(f"{idx}\n")
                        f.write(f"{sub['start']} --> {sub['end']}\n")
                        f.write(f"{sub['text']}\n\n")
                subtitle_files.append(("description", str(description_srt)))
                logging.info(f"ğŸ“ ë³´ì¡°ìë§‰ íŒŒì¼ ìƒì„±: {description_srt}")

            # ë©”ì¸ìë§‰ (mainSubtitle)
            if tracks_data.get("mainSubtitle", {}).get("enabled") and tracks_data.get("mainSubtitle", {}).get("data"):
                main_srt = temp_dir / "main.srt"
                with open(main_srt, "w", encoding="utf-8") as f:
                    for idx, sub in enumerate(tracks_data["mainSubtitle"]["data"], 1):
                        f.write(f"{idx}\n")
                        f.write(f"{sub['start']} --> {sub['end']}\n")
                        f.write(f"{sub['text']}\n\n")
                subtitle_files.append(("main", str(main_srt)))
                logging.info(f"ğŸ“ ë©”ì¸ìë§‰ íŒŒì¼ ìƒì„±: {main_srt}")

            # ğŸ¯ ìë§‰ì„ drawtextë¡œ ë Œë”ë§ (SRT ëŒ€ì‹  ì œëª©ì²˜ëŸ¼ í‘œì‹œ)
            # SRT íŒŒì¼ì€ ë‹¤ìš´ë¡œë“œìš©ìœ¼ë¡œë§Œ ìœ ì§€
            logging.info("ğŸš€ [DEBUG] drawtext ìë§‰ ë Œë”ë§ ì‹œì‘ (ìˆ˜ì§ ìŠ¤íƒ ëª¨ë“œ)")
            subtitle_drawtext_filters = []

            # ê²€ì •ë°” ë†’ì´ ê³„ì‚° (ìë§‰ ìœ„ì¹˜ ì¡°ì •ì— ì‚¬ìš©)
            top_bar_height = 0
            bottom_bar_height = 0
            if black_bars_data.get("top", {}).get("enabled"):
                top_bar_height = int(video_height * black_bars_data["top"].get("height", 15) / 100)
            if black_bars_data.get("bottom", {}).get("enabled"):
                bottom_bar_height = int(video_height * black_bars_data["bottom"].get("height", 15) / 100)

            logging.info(f"â¬› ê²€ì •ë°” ë†’ì´: ìƒë‹¨={top_bar_height}px, í•˜ë‹¨={bottom_bar_height}px")

            # FFmpeg drawtextìš© í…ìŠ¤íŠ¸ ì´ìŠ¤ì¼€ì´í”„
            def escape_drawtext(text: str) -> str:
                """FFmpeg drawtext í•„í„°ìš© í…ìŠ¤íŠ¸ ì´ìŠ¤ì¼€ì´í”„"""
                # íŠ¹ìˆ˜ ë¬¸ì ì´ìŠ¤ì¼€ì´í”„
                text = text.replace("\\", "\\\\")  # ë°±ìŠ¬ë˜ì‹œ
                text = text.replace(":", "\\:")     # ì½œë¡ 
                text = text.replace("'", "\\'")     # ì‘ì€ë”°ì˜´í‘œ
                text = text.replace("%", "\\%")     # í¼ì„¼íŠ¸
                return text

            # ğŸ¯ ìˆ˜ì§ ìŠ¤íƒì„ ìœ„í•œ ëª¨ë“  í…ìŠ¤íŠ¸ ì •ë³´ ìˆ˜ì§‘ (ì œëª©, ë¶€ì œëª©, ìë§‰ í¬í•¨)
            all_text_tracks = []
            subtitle_spacing = 20  # í…ìŠ¤íŠ¸ ì‚¬ì´ ê°„ê²© (px)

            # 1. ìë§‰ íŠ¸ë™ ìˆ˜ì§‘ (tracks_data) - ì—­ìˆœìœ¼ë¡œ ìˆ˜ì§‘ (reversed í›„ ì •ìˆœì´ ë¨)
            for track_key in ["description", "translation", "main"]:
                track_field = f"{track_key}Subtitle"
                if tracks_data.get(track_field, {}).get("enabled") and \
                   tracks_data.get(track_field, {}).get("data"):

                    # Canvas ìŠ¤íƒ€ì¼ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                    canvas_style = canvas_positions_data.get(track_key, {}) if canvas_positions_data else {}
                    font_size = canvas_style.get("fontSize", 40)

                    # ìƒ‰ìƒ ë³€í™˜ (CSS â†’ FFmpeg)
                    font_color = "white"
                    color = canvas_style.get("color", "rgb(255, 255, 255)")
                    if "rgb" in color:
                        import re
                        match = re.search(r'rgba?\((\d+),\s*(\d+),\s*(\d+)', color)
                        if match:
                            r, g, b = match.group(1), match.group(2), match.group(3)
                            font_color = f"#{int(r):02x}{int(g):02x}{int(b):02x}"

                    all_text_tracks.append({
                        "type": track_key,
                        "layer": "subtitle",
                        "data": tracks_data[track_field]["data"],
                        "font_size": font_size,
                        "font_color": font_color,
                        "border_width": canvas_style.get("borderWidth", 3)
                    })
                    logging.info(f"ğŸ“‹ {track_key} ìë§‰ ìˆ˜ì§‘: {font_size}px")

            # 2. ì œëª©/ë¶€ì œëª© ìˆ˜ì§‘ (overlays_data) - ì—­ìˆœìœ¼ë¡œ ìˆ˜ì§‘ (reversed í›„ title â†’ subtitleì´ ë¨)
            # PNG ë©”íƒ€ë°ì´í„°ë„ í•¨ê»˜ ì €ì¥í•˜ì—¬ ì‹¤ì œ ë†’ì´ ì‚¬ìš©
            for overlay_key in ["subtitle", "title"]:
                if overlay_key in overlays_data:
                    overlay = overlays_data[overlay_key]
                    if overlay and overlay.get("text"):
                        font_size = overlay.get("fontSize", 56)

                        # PNG ë©”íƒ€ë°ì´í„°ì—ì„œ ì‹¤ì œ ë†’ì´ ì°¾ê¸°
                        png_height = None
                        for png_meta in png_overlays:
                            if png_meta.get("overlay_type") == overlay_key:
                                png_height = png_meta.get("png_height")
                                logging.info(f"ğŸ“ {overlay_key} PNG ë†’ì´ ë°œê²¬: {png_height}px")
                                break

                        all_text_tracks.append({
                            "type": overlay_key,
                            "layer": "overlay",  # PNG ë˜ëŠ” drawtext
                            "font_size": font_size,
                            "png_height": png_height,  # PNG ë†’ì´ ì €ì¥
                            "data": overlay
                        })
                        logging.info(f"ğŸ“‹ {overlay_key} ìˆ˜ì§‘: {font_size}px (PNGë†’ì´={png_height}px)")

            # ìˆ˜ì§ ìŠ¤íƒ ìœ„ì¹˜ ê³„ì‚° (ìƒë‹¨ ê²€ì •ë°” ì•„ë˜ì—ì„œ ì‹œì‘í•˜ì—¬ ì•„ë˜ë¡œ ë°°ì¹˜)
            # ìˆ˜ì§‘ ìˆœì„œ: description, translation, main, subtitle, title
            # reversed í›„ ìˆœì„œ: title (ë§¨ ìœ„) â†’ subtitle â†’ main â†’ translation â†’ description (ë§¨ ì•„ë˜)
            top_bar_bottom = top_bar_height
            current_y = top_bar_bottom + subtitle_spacing

            subtitle_positions = {}
            # ì—­ìˆœìœ¼ë¡œ ë°°ì¹˜ (titleì´ ë§¨ ìœ„ì— ì˜¤ë„ë¡)
            for track_info in reversed(all_text_tracks):
                track_type = track_info["type"]
                font_size = track_info["font_size"]
                png_height = track_info.get("png_height")  # PNG ë†’ì´ (ìˆìœ¼ë©´)

                # PNGê°€ ìˆìœ¼ë©´ PNG ë†’ì´ë¥¼ ì‚¬ìš©, ì—†ìœ¼ë©´ font_size ì‚¬ìš©
                if png_height:
                    # PNGì˜ ê²½ìš°: ìƒë‹¨ ìœ„ì¹˜ì— PNGë¥¼ ë°°ì¹˜
                    y_center = current_y + png_height // 2
                    subtitle_positions[track_type] = y_center
                    # ë‹¤ìŒ í…ìŠ¤íŠ¸ëŠ” PNG í•˜ë‹¨ ì´í›„ë¡œ ë°°ì¹˜
                    current_y = current_y + png_height + subtitle_spacing
                    logging.info(f"ğŸ“ {track_type} PNG ì¤‘ì‹¬ ìœ„ì¹˜: {y_center}px (PNGë†’ì´: {png_height}px)")
                else:
                    # drawtextì˜ ê²½ìš°: í…ìŠ¤íŠ¸ ì¤‘ì‹¬ ìœ„ì¹˜ ê³„ì‚°
                    y_center = current_y + font_size // 2
                    subtitle_positions[track_type] = y_center
                    # ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ìœ„í•´ ìœ„ì¹˜ ì—…ë°ì´íŠ¸ (ì•„ë˜ë¡œ ì´ë™)
                    current_y = current_y + font_size + subtitle_spacing
                    layer_name = "ì œëª©/ë¶€ì œëª©" if track_info["layer"] == "overlay" else "ìë§‰"
                    logging.info(f"ğŸ“ {track_type} {layer_name} ì¤‘ì‹¬ ìœ„ì¹˜: {y_center}px (í°íŠ¸í¬ê¸°: {font_size}px)")

            # Canvas ìœ„ì¹˜ ì •ë³´ì—ì„œ drawtext íŒŒë¼ë¯¸í„° ìƒì„±
            def get_drawtext_params(sub_type: str, text: str, start_sec: float, end_sec: float):
                """ìë§‰ íƒ€ì…ì— ë§ëŠ” drawtext íŒŒë¼ë¯¸í„° ìƒì„±"""

                # ê¸°ë³¸ê°’ ì„¤ì •
                font_size = 60
                font_color = "white"
                border_width = 3
                font_file = "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc"

                # Canvas ìœ„ì¹˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                canvas_style = canvas_positions_data.get(sub_type, {}) if canvas_positions_data else {}

                if canvas_style:
                    font_size = canvas_style.get("fontSize", font_size)

                    # ìƒ‰ìƒ ë³€í™˜ (CSS â†’ FFmpeg)
                    color = canvas_style.get("color", "rgb(255, 255, 255)")
                    if "rgb" in color:
                        import re
                        match = re.search(r'rgba?\((\d+),\s*(\d+),\s*(\d+)', color)
                        if match:
                            r, g, b = match.group(1), match.group(2), match.group(3)
                            font_color = f"#{int(r):02x}{int(g):02x}{int(b):02x}"

                    border_width = canvas_style.get("borderWidth", border_width)

                # Canvas yPositionì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ ìˆ˜ì§ ìŠ¤íƒ ìœ„ì¹˜ ì‚¬ìš©
                if canvas_style and "yPosition" in canvas_style:
                    y_position = canvas_style["yPosition"]  # 0~1 ë¹„ìœ¨
                    y_pixel = int(video_height * y_position)
                    logging.info(f"âœ… {sub_type} Canvas yPosition ì‚¬ìš©: {y_position:.4f} â†’ {y_pixel}px")
                else:
                    # ìˆ˜ì§ ìŠ¤íƒì—ì„œ ê³„ì‚°ëœ y ìœ„ì¹˜ ì‚¬ìš©
                    y_pixel = subtitle_positions.get(sub_type, video_height - bottom_bar_height - font_size - 20)
                    logging.info(f"ğŸ“ {sub_type} ìˆ˜ì§ ìŠ¤íƒ ìœ„ì¹˜ ì‚¬ìš©: {y_pixel}px")

                # í…ìŠ¤íŠ¸ ì´ìŠ¤ì¼€ì´í”„
                escaped_text = escape_drawtext(text)

                # drawtext í•„í„° ìƒì„±
                # y ì¢Œí‘œ: í…ìŠ¤íŠ¸ ì¤‘ì‹¬ì´ y_pixelì— ì˜¤ë„ë¡ (text_h/2ë¥¼ ë¹¼ì„œ ì¡°ì •)
                # ì™¸ë¶€ ê²€ìƒ‰ ê²°ê³¼: text_hëŠ” í…ìŠ¤íŠ¸ì˜ ì‹¤ì œ ë†’ì´ (ascent - descent)
                return {
                    "text": escaped_text,
                    "fontfile": font_file,
                    "fontsize": font_size,
                    "fontcolor": font_color,
                    "borderw": border_width,
                    "bordercolor": "black",
                    "x": "(w-text_w)/2",  # ì¤‘ì•™ ì •ë ¬
                    "y": f"({y_pixel}-text_h/2)",  # í…ìŠ¤íŠ¸ ì¤‘ì‹¬ ê¸°ì¤€ ì •ë ¬
                    "enable": f"between(t,{start_sec},{end_sec})"
                }

            # ê° ìë§‰ íŠ¸ë™ì˜ ë°ì´í„°ë¥¼ drawtext í•„í„°ë¡œ ë³€í™˜
            if tracks_data.get("translationSubtitle", {}).get("enabled") and \
               tracks_data.get("translationSubtitle", {}).get("data"):
                logging.info("ğŸ¬ ì£¼ìë§‰(translation)ì„ drawtextë¡œ ë Œë”ë§")
                for sub in tracks_data["translationSubtitle"]["data"]:
                    start_sec = srt_time_to_seconds(sub["start"])
                    end_sec = srt_time_to_seconds(sub["end"])
                    params = get_drawtext_params("translation", sub["text"], start_sec, end_sec)

                    drawtext_filter = (
                        f"drawtext=fontfile='{params['fontfile']}'"
                        f":text='{params['text']}'"
                        f":fontsize={params['fontsize']}"
                        f":fontcolor={params['fontcolor']}"
                        f":borderw={params['borderw']}"
                        f":bordercolor={params['bordercolor']}"
                        f":x={params['x']}"
                        f":y={params['y']}"
                        f":enable='{params['enable']}'"
                    )
                    subtitle_drawtext_filters.append(drawtext_filter)
                    logging.info(f"   âœ… ì£¼ìë§‰: {sub['text'][:30]}... ({start_sec:.1f}s-{end_sec:.1f}s)")

            if tracks_data.get("descriptionSubtitle", {}).get("enabled") and \
               tracks_data.get("descriptionSubtitle", {}).get("data"):
                logging.info("ğŸ¬ ë³´ì¡°ìë§‰(description)ì„ drawtextë¡œ ë Œë”ë§")
                for sub in tracks_data["descriptionSubtitle"]["data"]:
                    start_sec = srt_time_to_seconds(sub["start"])
                    end_sec = srt_time_to_seconds(sub["end"])
                    params = get_drawtext_params("description", sub["text"], start_sec, end_sec)

                    drawtext_filter = (
                        f"drawtext=fontfile='{params['fontfile']}'"
                        f":text='{params['text']}'"
                        f":fontsize={params['fontsize']}"
                        f":fontcolor={params['fontcolor']}"
                        f":borderw={params['borderw']}"
                        f":bordercolor={params['bordercolor']}"
                        f":x={params['x']}"
                        f":y={params['y']}"
                        f":enable='{params['enable']}'"
                    )
                    subtitle_drawtext_filters.append(drawtext_filter)
                    logging.info(f"   âœ… ë³´ì¡°ìë§‰: {sub['text'][:30]}... ({start_sec:.1f}s-{end_sec:.1f}s)")

            if tracks_data.get("mainSubtitle", {}).get("enabled") and \
               tracks_data.get("mainSubtitle", {}).get("data"):
                logging.info("ğŸ¬ ë©”ì¸ìë§‰(main)ì„ drawtextë¡œ ë Œë”ë§")
                for sub in tracks_data["mainSubtitle"]["data"]:
                    start_sec = srt_time_to_seconds(sub["start"])
                    end_sec = srt_time_to_seconds(sub["end"])
                    params = get_drawtext_params("main", sub["text"], start_sec, end_sec)

                    drawtext_filter = (
                        f"drawtext=fontfile='{params['fontfile']}'"
                        f":text='{params['text']}'"
                        f":fontsize={params['fontsize']}"
                        f":fontcolor={params['fontcolor']}"
                        f":borderw={params['borderw']}"
                        f":bordercolor={params['bordercolor']}"
                        f":x={params['x']}"
                        f":y={params['y']}"
                        f":enable='{params['enable']}'"
                    )
                    subtitle_drawtext_filters.append(drawtext_filter)
                    logging.info(f"   âœ… ë©”ì¸ìë§‰: {sub['text'][:30]}... ({start_sec:.1f}s-{end_sec:.1f}s)")

            if tracks_data.get("japaneseSubtitle", {}).get("enabled") and \
               tracks_data.get("japaneseSubtitle", {}).get("data"):
                logging.info("ğŸ¬ ì¼ë³¸ì–´ìë§‰(japanese)ì„ drawtextë¡œ ë Œë”ë§")
                for sub in tracks_data["japaneseSubtitle"]["data"]:
                    start_sec = srt_time_to_seconds(sub["start"])
                    end_sec = srt_time_to_seconds(sub["end"])
                    params = get_drawtext_params("japanese", sub["text"], start_sec, end_sec)

                    drawtext_filter = (
                        f"drawtext=fontfile='{params['fontfile']}'"
                        f":text='{params['text']}'"
                        f":fontsize={params['fontsize']}"
                        f":fontcolor={params['fontcolor']}"
                        f":borderw={params['borderw']}"
                        f":bordercolor={params['bordercolor']}"
                        f":x={params['x']}"
                        f":y={params['y']}"
                        f":enable='{params['enable']}'"
                    )
                    subtitle_drawtext_filters.append(drawtext_filter)
                    logging.info(f"   âœ… ì¼ë³¸ì–´ìë§‰: {sub['text'][:30]}... ({start_sec:.1f}s-{end_sec:.1f}s)")

            logging.info(f"ğŸ“Š ì´ {len(subtitle_drawtext_filters)}ê°œ drawtext ìë§‰ í•„í„° ìƒì„±ë¨")

            # ìë§‰ í•„í„° ì¶”ê°€ (ë¹„ë””ì˜¤ í•´ìƒë„ ê¸°ì¤€ìœ¼ë¡œ ì ì ˆí•œ í¬ê¸° ì‚¬ìš©)
            # âš ï¸ ì•„ë˜ SRT ê¸°ë°˜ ì½”ë“œëŠ” ë‹¤ìš´ë¡œë“œìš© íŒŒì¼ ìƒì„±ë§Œì„ ìœ„í•´ ìœ ì§€
            if subtitle_files:
                # overlays í°íŠ¸ í¬ê¸°ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë˜ ì•ˆì „í•œ ë²”ìœ„ ë‚´ë¡œ ì œí•œ
                def adjust_subtitle_size(overlay_size):
                    # âš ï¸ ì›¹ ë¯¸ë¦¬ë³´ê¸°ì™€ ë™ê¸°í™”: í°íŠ¸ í¬ê¸° ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    # ì‚¬ìš©ìê°€ ì›¹ UIì—ì„œ ì¡°ì •í•œ í¬ê¸°ë¥¼ ê·¸ëŒ€ë¡œ ì¶œë ¥ì— ë°˜ì˜
                    adjusted = int(round(overlay_size))  # í¬ê¸° ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    return max(18, min(adjusted, 120))  # ìµœì†Œ 18px, ìµœëŒ€ 120px

                # CSS ìƒ‰ìƒì„ ASS/SSA í˜•ì‹(&HBBGGRR)ìœ¼ë¡œ ë³€í™˜
                def css_to_ass_color(css_color):
                    """CSS colorë¥¼ ASS/SSA í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (BGR ìˆœì„œ)"""
                    if not css_color:
                        return "&H00FFFFFF"  # ê¸°ë³¸: í°ìƒ‰

                    # rgb(r, g, b) í˜•ì‹ íŒŒì‹±
                    import re
                    match = re.search(r'rgba?\((\d+),\s*(\d+),\s*(\d+)', css_color)
                    if match:
                        r, g, b = int(match.group(1)), int(match.group(2)), int(match.group(3))
                        # ASSëŠ” BGR ìˆœì„œ
                        return f"&H00{b:02X}{g:02X}{r:02X}"
                    return "&H00FFFFFF"

                # overlaysì—ì„œ í°íŠ¸ í¬ê¸° ë° ìƒ‰ìƒ ì¶”ì¶œ (None ì•ˆì „ ì²˜ë¦¬)
                korean_overlay = overlays_data.get("korean") or {}
                japanese_overlay = overlays_data.get("japanese") or {}
                english_overlay = overlays_data.get("english") or {}
                title_overlay = overlays_data.get("title") or {}

                korean_overlay_size = korean_overlay.get("fontSize", 64)
                japanese_overlay_size = japanese_overlay.get("fontSize", 60)
                english_overlay_size = english_overlay.get("fontSize", 56)
                title_overlay_size = title_overlay.get("fontSize", 96)

                korean_font_size = adjust_subtitle_size(korean_overlay_size)
                japanese_font_size = adjust_subtitle_size(japanese_overlay_size)
                english_font_size = adjust_subtitle_size(english_overlay_size)
                title_font_size = adjust_subtitle_size(title_overlay_size)

                # SRT ìë§‰ì€ ëª¨ë‘ í°ìƒ‰ìœ¼ë¡œ ê³ ì •
                korean_color = "&H00FFFFFF"  # í°ìƒ‰
                japanese_color = "&H00FFFFFF"  # í°ìƒ‰
                english_color = "&H00FFFFFF"  # í°ìƒ‰
                title_color = "&H00FFFFFF"  # í°ìƒ‰

                # ê²€ì •ë°” ë†’ì´ ê³„ì‚° (ìë§‰ ìœ„ì¹˜ ì¡°ì •ì— ì‚¬ìš©)
                top_bar_height = 0
                bottom_bar_height = 0
                if black_bars_data.get("top", {}).get("enabled"):
                    top_bar_height = int(video_height * black_bars_data["top"].get("height", 15) / 100)
                if black_bars_data.get("bottom", {}).get("enabled"):
                    bottom_bar_height = int(video_height * black_bars_data["bottom"].get("height", 15) / 100)

                logging.info(f"ğŸ“ ìë§‰ í¬ê¸° ì¡°ì •: korean {korean_overlay_size}â†’{korean_font_size}, japanese {japanese_overlay_size}â†’{japanese_font_size}, english {english_overlay_size}â†’{english_font_size}, title {title_overlay_size}â†’{title_font_size}")
                logging.info(f"ğŸ¨ SRT ìë§‰ ìƒ‰ìƒ: ëª¨ë‘ í°ìƒ‰ (korean={korean_color}, japanese={japanese_color}, english={english_color}, title={title_color})")
                logging.info(f"â¬› ê²€ì •ë°” ë†’ì´: ìƒë‹¨={top_bar_height}px, í•˜ë‹¨={bottom_bar_height}px")

                for sub_type, sub_path in subtitle_files:
                    # ìë§‰ íŒŒì¼ ê²½ë¡œ ì´ìŠ¤ì¼€ì´í”„
                    sub_path_escaped = sub_path.replace("\\", "\\\\\\\\").replace(":", "\\:").replace("'", "\\'")

                    # CJK(í•œê¸€, ì¼ë³¸ì–´, ì¤‘êµ­ì–´) ì§€ì› í°íŠ¸ ì‚¬ìš©
                    # ìë§‰ íƒ€ì…ë³„ë¡œ ì ì ˆí•œ í°íŠ¸ ì„ íƒ
                    # âš ï¸ ì´ëª¨ì§€ ì§€ì›ì„ ìœ„í•´ Noto Color Emoji fallback ì¶”ê°€
                    if sub_type == "description":
                        # description ìë§‰ì€ ì¼ë³¸ì–´ë¥¼ í¬í•¨í•  ê°€ëŠ¥ì„±ì´ ë†’ìœ¼ë¯€ë¡œ ì¼ë³¸ì–´ í°íŠ¸ ìš°ì„ 
                        font_name = "Noto Sans CJK JP,Noto Color Emoji"
                    elif sub_type == "japanese":
                        # ì¼ë³¸ì–´ ìë§‰ì€ ëª…ì‹œì ìœ¼ë¡œ ì¼ë³¸ì–´ í°íŠ¸ ì‚¬ìš©
                        font_name = "Noto Sans CJK JP,Noto Color Emoji"
                    else:
                        # í•œêµ­ì–´ ìë§‰ (translation, main ë“±)
                        font_name = "Noto Sans CJK KR,Noto Color Emoji"

                    # ìë§‰ ìœ„ì¹˜ ë° ìŠ¤íƒ€ì¼ ì„¤ì •
                    # Canvas ìœ„ì¹˜ ì •ë³´ ì‚¬ìš© (ìˆìœ¼ë©´ Canvas ìœ„ì¹˜, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’)
                    if sub_type == "translation":
                        # ì£¼ìë§‰: Canvas ìœ„ì¹˜ ì •ë³´ ì‚¬ìš©
                        font_size = korean_font_size
                        primary_color = korean_color
                        outline_width = max(2, int(font_size * 0.06))

                        # Canvas ìœ„ì¹˜ ì •ë³´ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
                        if canvas_positions_data and canvas_positions_data.get("translation"):
                            canvas_style = canvas_positions_data["translation"]
                            y_position = canvas_style.get("yPosition", 0.15)  # 0~1 ë¹„ìœ¨
                            font_size = canvas_style.get("fontSize", korean_font_size)

                            # y_positionì„ ì‹¤ì œ í”½ì…€ë¡œ ë³€í™˜
                            y_pixel = int(video_height * y_position)

                            # ìƒë‹¨ ê²€ì •ë°” ì˜ì—­ ì•ˆìª½ì´ë©´ ê²€ì •ë°” ë°”ë¡œ ì•„ë˜ë¡œ ì´ë™
                            if y_pixel < top_bar_height:
                                y_pixel = top_bar_height + font_size + 20  # ê²€ì •ë°” + í°íŠ¸ í¬ê¸° + ì—¬ìœ  20px
                                logging.info(f"   ğŸ“ ì£¼ìë§‰ ìœ„ì¹˜ ì¡°ì •: ê²€ì •ë°” ì˜ì—­({top_bar_height}px) í”¼í•´ì„œ {y_pixel}pxë¡œ ì´ë™")

                            # MarginV ê³„ì‚° (í•˜ë‹¨ì—ì„œë¶€í„°ì˜ ê±°ë¦¬)
                            margin_v = video_height - y_pixel

                            # ìƒ‰ìƒ ë³€í™˜ í•„ìš”ì‹œ
                            if canvas_style.get("color"):
                                primary_color = css_to_ass_color(canvas_style["color"])
                            if canvas_style.get("borderWidth"):
                                outline_width = canvas_style["borderWidth"]
                        else:
                            margin_v = 200  # ê¸°ë³¸ê°’

                        style = f"FontName={font_name},FontSize={font_size},PrimaryColour={primary_color},OutlineColour=&H000000,BorderStyle=1,Outline={outline_width},Shadow=1,Alignment=2,MarginV={margin_v}"
                    elif sub_type == "japanese":
                        # ì¼ë³¸ì–´ìë§‰: í•˜ë‹¨ì—ì„œ 130px ìœ„ (í•œê¸€ê³¼ ì˜ì–´ ì‚¬ì´)
                        font_size = japanese_font_size
                        primary_color = japanese_color
                        outline_width = max(2, int(font_size * 0.06))
                        margin_v = 130
                        # ì¼ë³¸ì–´ í°íŠ¸ ì‚¬ìš©
                        jp_font_name = "Noto Sans CJK JP"
                        style = f"FontName={jp_font_name},FontSize={font_size},PrimaryColour={primary_color},OutlineColour=&H000000,BorderStyle=1,Outline={outline_width},Shadow=1,Alignment=2,MarginV={margin_v}"
                    elif sub_type == "description":
                        # ë³´ì¡°ìë§‰: Canvas ìœ„ì¹˜ ì •ë³´ ì‚¬ìš©
                        font_size = english_font_size
                        primary_color = english_color
                        outline_width = max(2, int(font_size * 0.06))

                        # Canvas ìœ„ì¹˜ ì •ë³´ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
                        if canvas_positions_data and canvas_positions_data.get("description"):
                            canvas_style = canvas_positions_data["description"]
                            y_position = canvas_style.get("yPosition", 0.70)  # 0~1 ë¹„ìœ¨
                            font_size = canvas_style.get("fontSize", english_font_size)

                            # y_positionì„ ì‹¤ì œ í”½ì…€ë¡œ ë³€í™˜
                            y_pixel = int(video_height * y_position)

                            # í•˜ë‹¨ ê²€ì •ë°” ì˜ì—­ ì•ˆìª½ì´ë©´ ê²€ì •ë°” ë°”ë¡œ ìœ„ë¡œ ì´ë™
                            if y_pixel > (video_height - bottom_bar_height):
                                y_pixel = video_height - bottom_bar_height - font_size - 20  # ê²€ì •ë°” ìœ„ + ì—¬ìœ  20px
                                logging.info(f"   ğŸ“ ë³´ì¡°ìë§‰ ìœ„ì¹˜ ì¡°ì •: ê²€ì •ë°” ì˜ì—­ í”¼í•´ì„œ {y_pixel}pxë¡œ ì´ë™")

                            # MarginV ê³„ì‚° (í•˜ë‹¨ì—ì„œë¶€í„°ì˜ ê±°ë¦¬)
                            margin_v = video_height - y_pixel

                            if canvas_style.get("color"):
                                primary_color = css_to_ass_color(canvas_style["color"])
                            if canvas_style.get("borderWidth"):
                                outline_width = canvas_style["borderWidth"]
                        else:
                            margin_v = 60  # ê¸°ë³¸ê°’

                        style = f"FontName={font_name},FontSize={font_size},PrimaryColour={primary_color},OutlineColour=&H000000,BorderStyle=1,Outline={outline_width},Shadow=1,Alignment=2,MarginV={margin_v}"
                    else:
                        # ë©”ì¸ìë§‰: Canvas ìœ„ì¹˜ ì •ë³´ ì‚¬ìš©
                        font_size = title_font_size
                        primary_color = title_color
                        outline_width = max(2, int(font_size * 0.06))

                        # Canvas ìœ„ì¹˜ ì •ë³´ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
                        if canvas_positions_data and canvas_positions_data.get("main"):
                            canvas_style = canvas_positions_data["main"]
                            y_position = canvas_style.get("yPosition", 0.85)  # 0~1 ë¹„ìœ¨
                            font_size = canvas_style.get("fontSize", title_font_size)

                            # y_positionì„ ì‹¤ì œ í”½ì…€ë¡œ ë³€í™˜
                            y_pixel = int(video_height * y_position)

                            # í•˜ë‹¨ ê²€ì •ë°” ì˜ì—­ ì•ˆìª½ì´ë©´ ê²€ì •ë°” ë°”ë¡œ ìœ„ë¡œ ì´ë™
                            if y_pixel > (video_height - bottom_bar_height):
                                y_pixel = video_height - bottom_bar_height - font_size - 20  # ê²€ì •ë°” ìœ„ + ì—¬ìœ  20px
                                logging.info(f"   ğŸ“ ë©”ì¸ìë§‰ ìœ„ì¹˜ ì¡°ì •: ê²€ì •ë°” ì˜ì—­ í”¼í•´ì„œ {y_pixel}pxë¡œ ì´ë™")

                            # MarginV ê³„ì‚° (í•˜ë‹¨ì—ì„œë¶€í„°ì˜ ê±°ë¦¬)
                            margin_v = video_height - y_pixel

                            if canvas_style.get("color"):
                                primary_color = css_to_ass_color(canvas_style["color"])
                            if canvas_style.get("borderWidth"):
                                outline_width = canvas_style["borderWidth"]
                        else:
                            margin_v = 220  # ê¸°ë³¸ê°’

                        style = f"FontName={font_name},FontSize={font_size},PrimaryColour={primary_color},OutlineColour=&H000000,BorderStyle=1,Outline={outline_width},Shadow=1,Alignment=2,MarginV={margin_v}"

                    logging.info(f"ğŸ“ SRT ìë§‰ ìŠ¤íƒ€ì¼ (ë‹¤ìš´ë¡œë“œìš©ë§Œ): {sub_type} - í°íŠ¸={font_name} {font_size}px, ìƒ‰ìƒ={primary_color}, ì™¸ê³½ì„ ={outline_width}px, MarginV={margin_v}")
                    # âš ï¸ SRT ìë§‰ í•„í„°ëŠ” ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (drawtextë¡œ ëŒ€ì²´)
                    # video_filters.append(f"subtitles={sub_path_escaped}:force_style='{style}'")

            # FFmpeg ëª…ë ¹ì–´ êµ¬ì„±
            cmd = ["/usr/bin/ffmpeg", "-i", str(video_file.absolute())]

            # PNG ì˜¤ë²„ë ˆì´ê°€ ìˆìœ¼ë©´ ë³„ë„ ì…ë ¥ìœ¼ë¡œ ì¶”ê°€
            if png_overlays:
                for png_meta in png_overlays:
                    cmd.extend(["-i", png_meta["png_path"]])

            # ì˜¤ë””ì˜¤ íŒŒì¼ ì…ë ¥ ì¶”ê°€
            audio_input_start_idx = 1 + len(png_overlays)  # PNG ì˜¤ë²„ë ˆì´ ê°œìˆ˜ë§Œí¼ ì¸ë±ìŠ¤ ì¡°ì •
            for audio_input in audio_inputs:
                cmd.extend(["-i", audio_input])

            cmd.append("-y")  # íŒŒì¼ ë®ì–´ì“°ê¸°

            # ë¹„ë””ì˜¤ í•„í„° ì ìš©
            video_filter_output_label = None  # PNG ì˜¤ë²„ë ˆì´ ì ìš© ì‹œ ì‚¬ìš©í•  ì¶œë ¥ ë ˆì´ë¸”

            # ğŸ¯ SRT ìë§‰ í•„í„°ëŠ” ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (drawtextë¡œ ëŒ€ì²´)
            # subtitle_filters = [f for f in video_filters if f.startswith("subtitles=")]
            other_filters = [f for f in video_filters if not f.startswith("subtitles=")]
            logging.info(f"ğŸ” other_filters ê°œìˆ˜: {len(other_filters)}, overlay_drawtext ê°œìˆ˜: {len(overlay_drawtext)}")
            logging.info(f"ğŸ” overlay_drawtext keys: {list(overlay_drawtext.keys())}")

            if png_overlays or subtitle_drawtext_filters:
                # PNG ì˜¤ë²„ë ˆì´ ë˜ëŠ” drawtext ìë§‰ì´ ìˆìœ¼ë©´ filter_complex ì‚¬ìš©
                filter_parts = []

                # ğŸ¯ overlay_drawtextì—ì„œ title/subtitle í•„í„°ë¥¼ ê°€ì ¸ì™€ì„œ ìˆ˜ì§ ìŠ¤íƒ ìœ„ì¹˜ë¡œ ì—…ë°ì´íŠ¸
                import re
                overlay_drawtext_filters = []  # title/subtitle drawtext í•„í„° ì €ì¥

                for overlay_key, overlay_info in overlay_drawtext.items():
                    original_filter = overlay_info["filter"]
                    overlay_data = overlay_info.get("data", {})

                    # Canvas y ìœ„ì¹˜ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ ìˆ˜ì§ ìŠ¤íƒ ìœ„ì¹˜ ì‚¬ìš©
                    if overlay_data and "y" in overlay_data and overlay_data["y"] is not None:
                        # Canvas ì›ë³¸ y ìœ„ì¹˜ ì‚¬ìš©
                        canvas_y = overlay_data["y"]
                        # ì´ë¯¸ ì›ë³¸ í•„í„°ê°€ Canvas ìœ„ì¹˜ë¥¼ ì‚¬ìš©í•˜ê³  ìˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                        overlay_drawtext_filters.append(original_filter)
                        logging.info(f"âœ… {overlay_key} drawtext Canvas ìœ„ì¹˜ ì‚¬ìš©: y={canvas_y}px (ì›ë³¸ í•„í„° ìœ ì§€)")
                    elif overlay_key in subtitle_positions:
                        # ìˆ˜ì§ ìŠ¤íƒ ìœ„ì¹˜ë¡œ y ê°’ ì—…ë°ì´íŠ¸
                        y_pos = subtitle_positions[overlay_key]
                        updated_filter = re.sub(r":y='clip\([^']+\)'", f":y='clip({y_pos}-text_h/2,0,{video_height}-text_h)'", original_filter)
                        overlay_drawtext_filters.append(updated_filter)
                        logging.info(f"ğŸ“ {overlay_key} drawtext ìˆ˜ì§ ìŠ¤íƒ ìœ„ì¹˜ ì‚¬ìš©: {y_pos}px")
                    else:
                        # ê¸°ë³¸ê°’ìœ¼ë¡œ ì›ë³¸ í•„í„° ì‚¬ìš©
                        overlay_drawtext_filters.append(original_filter)
                        logging.info(f"âš ï¸ {overlay_key} drawtext ì›ë³¸ ìœ„ì¹˜ ì‚¬ìš© (Canvas/ìˆ˜ì§ìŠ¤íƒ ì •ë³´ ì—†ìŒ)")

                logging.info(f"ğŸ“‹ overlay_drawtext_filters ì¤€ë¹„ ì™„ë£Œ: {len(overlay_drawtext_filters)}ê°œ")

                # 1. ë‹¤ë¥¸ ë¹„ë””ì˜¤ í•„í„° ë¨¼ì € ì ìš© (black bar ë“±, title/subtitle drawtext ì œì™¸)
                if other_filters:
                    base_filter = ",".join(other_filters)
                    filter_parts.append(f"[0:v]{base_filter}[v0]")
                    current_input = "[v0]"
                else:
                    current_input = "[0:v]"

                # 2. PNG ì˜¤ë²„ë ˆì´ ì ìš© (Canvas ìœ„ì¹˜ ìš°ì„ , ì—†ìœ¼ë©´ ìˆ˜ì§ ìŠ¤íƒ ìœ„ì¹˜ ì‚¬ìš©)
                if png_overlays:
                    for idx, png_meta in enumerate(png_overlays, start=1):
                        overlay_type = png_meta.get("overlay_type", "title")
                        png_height = png_meta.get("png_height", 140)

                        # Canvasì—ì„œ ì„¤ì •ëœ y_pixelì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
                        if "y_pixel" in png_meta and png_meta["y_pixel"] is not None:
                            # PNG ë Œë”ë§ ì‹œ ê³„ì‚°ëœ ì›ë³¸ y ìœ„ì¹˜ ì‚¬ìš© (Canvas ìœ„ì¹˜)
                            y_center = png_meta["y_pixel"]
                            y_pixel = y_center - png_height // 2
                            logging.info(f"âœ… PNG ì˜¤ë²„ë ˆì´ '{overlay_type}' Canvas ìœ„ì¹˜ ì‚¬ìš©: ì¤‘ì‹¬={y_center}px, ìƒë‹¨={y_pixel}px, ë†’ì´={png_height}px")
                        elif overlay_type in subtitle_positions:
                            # ìˆ˜ì§ ìŠ¤íƒì—ì„œ ê³„ì‚°ëœ ìœ„ì¹˜ ì‚¬ìš©
                            y_center = subtitle_positions[overlay_type]
                            y_pixel = y_center - png_height // 2
                            logging.info(f"ğŸ“ PNG ì˜¤ë²„ë ˆì´ '{overlay_type}' ìˆ˜ì§ ìŠ¤íƒ ìœ„ì¹˜ ì‚¬ìš©: ì¤‘ì‹¬={y_center}px, ìƒë‹¨={y_pixel}px, ë†’ì´={png_height}px")
                        else:
                            y_pixel = video_height // 2
                            logging.info(f"âš ï¸ PNG ì˜¤ë²„ë ˆì´ '{overlay_type}' ê¸°ë³¸ ìœ„ì¹˜ ì‚¬ìš©: {y_pixel}px")

                        # overlay í•„í„°: PNGë¥¼ ë¹„ë””ì˜¤ ì¤‘ì•™ì— ë°°ì¹˜ (yëŠ” PNG ìƒë‹¨ ìœ„ì¹˜)
                        overlay_expr = f"{current_input}[{idx}:v]overlay=(W-w)/2:{y_pixel}"
                        filter_parts.append(f"{overlay_expr}[v{idx}]")
                        current_input = f"[v{idx}]"

                # 2.5. title/subtitle drawtextë¥¼ PNG ì´í›„ì— ì ìš© (PNG ìœ„ì— í‘œì‹œ)
                if overlay_drawtext_filters:
                    overlay_drawtext_chain = ",".join(overlay_drawtext_filters)
                    # í˜„ì¬ ì…ë ¥ì— overlay drawtext ì¶”ê°€ (ì¶œë ¥ì€ ì„ì‹œ ë ˆì´ë¸”)
                    filter_parts.append(f"{current_input}{overlay_drawtext_chain}[v_after_overlay]")
                    current_input = "[v_after_overlay]"
                    logging.info(f"ğŸ¨ title/subtitle drawtext {len(overlay_drawtext_filters)}ê°œë¥¼ PNG ì´í›„ì— ì ìš©")

                # 3. drawtext ìë§‰ í•„í„°ë¥¼ ë§ˆì§€ë§‰ì— ì ìš© (ëª¨ë“  overlay ìœ„ì— í‘œì‹œ)
                if subtitle_drawtext_filters:
                    # drawtext í•„í„°ë“¤ì„ ì½¤ë§ˆë¡œ ì—°ê²°í•˜ì—¬ ìˆœì°¨ ì ìš©
                    subtitle_chain = ",".join(subtitle_drawtext_filters)
                    filter_parts.append(f"{current_input}{subtitle_chain}[vout]")
                    video_filter_output_label = "[vout]"
                    logging.info(f"ğŸ¬ drawtext ìë§‰ í•„í„° {len(subtitle_drawtext_filters)}ê°œ ì ìš©ë¨")
                else:
                    # ìë§‰ì´ ì—†ìœ¼ë©´ ë§ˆì§€ë§‰ PNG ì˜¤ë²„ë ˆì´ ì¶œë ¥ì„ voutìœ¼ë¡œ ë³€ê²½
                    if png_overlays:
                        last_overlay_idx = len(png_overlays)
                        filter_parts[-1] = filter_parts[-1].replace(f"[v{last_overlay_idx}]", "[vout]")
                    else:
                        # PNGë„ ì—†ê³  ìë§‰ë„ ì—†ìœ¼ë©´ í˜„ì¬ ì…ë ¥ì„ voutìœ¼ë¡œ
                        filter_parts.append(f"{current_input}[vout]")
                    video_filter_output_label = "[vout]"

                video_filter_complex = ";".join(filter_parts)
                logging.info(f"ğŸ¬ Filter Complex êµ¬ì„±: {video_filter_complex}")
                # ì˜¤ë””ì˜¤ í•„í„°ì™€ ê²°í•©í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¼ë‹¨ ì €ì¥ë§Œ
            elif video_filters:
                # PNG ì˜¤ë²„ë ˆì´ê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ëŒ€ë¡œ -vf ì‚¬ìš©
                filter_string = ",".join(video_filters)
                cmd.extend(["-vf", filter_string])

            # ë¹„ë””ì˜¤ì— ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ì´ ìˆëŠ”ì§€ í™•ì¸
            video_has_audio = False
            try:
                probe_cmd = ["/usr/bin/ffprobe", "-v", "error", "-select_streams", "a:0",
                           "-show_entries", "stream=codec_type", "-of", "default=noprint_wrappers=1:nokey=1",
                           str(video_file.absolute())]
                probe_result = subprocess.run(probe_cmd, capture_output=True, text=True, timeout=10)
                video_has_audio = probe_result.returncode == 0 and probe_result.stdout.strip() == "audio"
                logging.info(f"ğŸ”Š ë¹„ë””ì˜¤ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì¡´ì¬: {video_has_audio}")
            except Exception as e:
                logging.warning(f"ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ í™•ì¸ ì‹¤íŒ¨: {e}")
                video_has_audio = False

            # ì˜¤ë””ì˜¤ ë¯¹ì‹±
            video_track_enabled = tracks_data.get("video", {}).get("enabled", True)
            video_muted = tracks_data.get("video", {}).get("muted", False)

            if len(audio_inputs) > 0:
                # ì˜¤ë””ì˜¤ ì…ë ¥ì´ ìˆëŠ” ê²½ìš°
                audio_filter_inputs = []

                # ë¹„ë””ì˜¤ ì›ë³¸ ì˜¤ë””ì˜¤ (ìŒì†Œê±°ë˜ì§€ ì•Šê³  ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ì´ ìˆëŠ” ê²½ìš°)
                if video_track_enabled and not video_muted and video_has_audio:
                    audio_filter_inputs.append("[0:a]")

                # ì¶”ê°€ ì˜¤ë””ì˜¤ íŠ¸ë™ë“¤
                for i in range(len(audio_inputs)):
                    input_idx = audio_input_start_idx + i
                    audio_filter_inputs.append(f"[{input_idx}:a]")

                if len(audio_filter_inputs) > 1:
                    # ì—¬ëŸ¬ ì˜¤ë””ì˜¤ ë¯¹ì‹±
                    audio_filter = f"{''.join(audio_filter_inputs)}amix=inputs={len(audio_filter_inputs)}:duration=first:dropout_transition=2[aout]"

                    # PNG ì˜¤ë²„ë ˆì´ê°€ ìˆìœ¼ë©´ video_filter_complexì™€ ê²°í•©
                    if png_overlays:
                        combined_filter = f"{video_filter_complex};{audio_filter}"
                        cmd.extend(["-filter_complex", combined_filter, "-map", video_filter_output_label, "-map", "[aout]"])
                    else:
                        cmd.extend(["-filter_complex", audio_filter, "-map", "0:v", "-map", "[aout]"])
                elif len(audio_filter_inputs) == 1:
                    # ë‹¨ì¼ ì˜¤ë””ì˜¤
                    if png_overlays:
                        cmd.extend(["-filter_complex", video_filter_complex, "-map", video_filter_output_label, "-map", audio_filter_inputs[0].strip("[]")])
                    else:
                        cmd.extend(["-map", "0:v", "-map", audio_filter_inputs[0].strip("[]")])
                else:
                    # ì˜¤ë””ì˜¤ ì—†ìŒ
                    if png_overlays:
                        cmd.extend(["-filter_complex", video_filter_complex, "-map", video_filter_output_label, "-an"])
                    else:
                        cmd.extend(["-map", "0:v", "-an"])
            else:
                # ì˜¤ë””ì˜¤ ì…ë ¥ì´ ì—†ëŠ” ê²½ìš°
                if png_overlays:
                    # PNG ì˜¤ë²„ë ˆì´ê°€ ìˆìœ¼ë©´ filter_complex ì‚¬ìš©
                    cmd.extend(["-filter_complex", video_filter_complex, "-map", video_filter_output_label])
                    if video_track_enabled and not video_muted and video_has_audio:
                        cmd.extend(["-map", "0:a"])
                    else:
                        cmd.append("-an")
                else:
                    # PNG ì˜¤ë²„ë ˆì´ê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ëŒ€ë¡œ
                    if video_track_enabled and not video_muted and video_has_audio:
                        cmd.extend(["-map", "0:v", "-map", "0:a"])
                    else:
                        cmd.extend(["-map", "0:v", "-an"])

            # ì¶œë ¥ ì˜µì…˜
            cmd.extend([
                "-c:v", "libx264",
                "-preset", "medium",
                "-crf", "23",
                "-c:a", "aac",
                "-b:a", "192k",
                str(output_path)
            ])

            perf_marks['filter_construction'] = time.time() - perf_start

            logging.info(f"ğŸ¬ FFmpeg ëª…ë ¹ì–´: {' '.join(cmd)}")

            # FFmpeg ì‹¤í–‰
            perf_start = time.time()
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=600)
            perf_marks['ffmpeg_execution'] = time.time() - perf_start

            if result.returncode != 0:
                logging.error(f"FFmpeg ì—ëŸ¬: {result.stderr}")
                raise HTTPException(status_code=500, detail=f"ìµœì¢… ì˜ìƒ ìƒì„± ì‹¤íŒ¨: {result.stderr}")

            # ì¶œë ¥ íŒŒì¼ í™•ì¸
            if output_path.exists():
                size_mb = output_path.stat().st_size / (1024 * 1024)

                # ìë§‰ íŒŒì¼ë“¤ì„ output_dirì— ë³µì‚¬
                saved_subtitles = {}
                if subtitle_files:
                    logging.info("ğŸ“¥ ìë§‰ íŒŒì¼ë“¤ì„ ì˜êµ¬ ë””ë ‰í† ë¦¬ì— ë³µì‚¬ ì¤‘...")
                    for sub_type, sub_path in subtitle_files:
                        src_file = Path(sub_path)
                        if src_file.exists():
                            # ì¶œë ¥ íŒŒì¼ëª…: {video_stem}_{sub_type}.srt
                            dest_filename = f"{video_file.stem}_{sub_type}.srt"
                            dest_path = output_dir / dest_filename

                            # íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ê³ ìœ í•œ ì´ë¦„ ìƒì„±
                            dest_counter = 1
                            while dest_path.exists():
                                dest_filename = f"{video_file.stem}_{sub_type}_{dest_counter}.srt"
                                dest_path = output_dir / dest_filename
                                dest_counter += 1

                            shutil.copy2(src_file, dest_path)
                            saved_subtitles[sub_type] = str(dest_path)
                            logging.info(f"   âœ“ {sub_type} ìë§‰: {dest_path}")

                # ì´ ì‹œê°„ ê³„ì‚°
                perf_marks['total_time'] = time.time() - perf_start_total

                # ì„±ëŠ¥ ë¡œê·¸ ì¶œë ¥
                logging.info(f"âœ… ìµœì¢… ì˜ìƒ ìƒì„± ì™„ë£Œ: {output_path} ({size_mb:.2f}MB)")
                logging.info("â±ï¸ ì„±ëŠ¥ ì¸¡ì • ê²°ê³¼:")
                logging.info(f"   - JSON íŒŒì‹±: {perf_marks.get('json_parsing', 0):.3f}s")
                logging.info(f"   - ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±: {perf_marks.get('temp_dir_creation', 0):.3f}s")
                logging.info(f"   - ì˜¤ë””ì˜¤ íŒŒì¼ ì €ì¥: {perf_marks.get('audio_file_save', 0):.3f}s")
                logging.info(f"   - í•„í„° êµ¬ì„±: {perf_marks.get('filter_construction', 0):.3f}s")
                logging.info(f"   - FFmpeg ì‹¤í–‰: {perf_marks.get('ffmpeg_execution', 0):.3f}s")
                logging.info(f"   - ì „ì²´ ì‹œê°„: {perf_marks.get('total_time', 0):.3f}s")

                return {
                    "success": True,
                    "output_path": str(output_path),
                    "file_name": output_name,
                    "size_mb": round(size_mb, 2),
                    "subtitles": saved_subtitles,  # ìë§‰ íŒŒì¼ ê²½ë¡œ ì¶”ê°€
                    "performance": {
                        "json_parsing": round(perf_marks.get('json_parsing', 0), 3),
                        "temp_dir_creation": round(perf_marks.get('temp_dir_creation', 0), 3),
                        "audio_file_save": round(perf_marks.get('audio_file_save', 0), 3),
                        "filter_construction": round(perf_marks.get('filter_construction', 0), 3),
                        "ffmpeg_execution": round(perf_marks.get('ffmpeg_execution', 0), 3),
                        "total_time": round(perf_marks.get('total_time', 0), 3)
                    }
                }
            else:
                raise HTTPException(status_code=500, detail="ìµœì¢… ì˜ìƒ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            for temp_file in temp_audio_files:
                try:
                    if temp_file.exists():
                        temp_file.unlink()
                except Exception as e:
                    logging.warning(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {temp_file}, {e}")
            try:
                if temp_dir.exists():
                    shutil.rmtree(temp_dir)
            except Exception as e:
                logging.warning(f"ì„ì‹œ ë””ë ‰í† ë¦¬ ì‚­ì œ ì‹¤íŒ¨: {temp_dir}, {e}")

    except HTTPException:
        raise
    except subprocess.TimeoutExpired:
        raise HTTPException(status_code=500, detail="ìµœì¢… ì˜ìƒ ìƒì„± ì‹œê°„ ì´ˆê³¼ (10ë¶„)")
    except Exception as e:
        logging.exception("ìµœì¢… ì˜ìƒ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
        raise HTTPException(status_code=500, detail=f"ìµœì¢… ì˜ìƒ ìƒì„± ì‹¤íŒ¨: {str(e)}")


@app.post("/api/video-analyzer/analyze-frames-with-ai")
async def api_analyze_frames_with_ai(payload: Dict[str, Any] = Body(...)) -> Dict[str, Any]:
    """AIë¥¼ ì‚¬ìš©í•œ í”„ë ˆì„ ë¶„ì„"""
    import anthropic
    import openai
    import os

    try:
        frames = payload.get("frames", [])
        model = payload.get("model", "sonnet")
        analysis_type = payload.get("analysis_type", "scene-description")
        video_name = payload.get("video_name", "unknown")
        custom_prompt = payload.get("custom_prompt")
        subtitle_language_primary = payload.get("subtitle_language_primary", "korean")
        subtitle_language_secondary = payload.get("subtitle_language_secondary", "")
        original_titles = payload.get("original_titles", [])

        # ë””ë²„ê¹…: ë°›ì€ ë°ì´í„° ë¡œê¹…
        logging.info(f"ğŸ” AI ë¶„ì„ ìš”ì²­ ë°ì´í„°:")
        logging.info(f"  - í”„ë ˆì„ ìˆ˜: {len(frames)}")
        logging.info(f"  - ì›ë³¸ ì œëª© ìˆ˜: {len(original_titles)}")
        logging.info(f"  - ì›ë³¸ ì œëª© ëª©ë¡: {original_titles}")
        if frames:
            logging.info(f"  - ì²« ë²ˆì§¸ í”„ë ˆì„ ì‹œê°„: {frames[0].get('time', 'N/A')}ì´ˆ")

        # ì–¸ì–´ë³„ ì§€ì‹œì‚¬í•­
        language_instructions = {
            "korean": "í•œêµ­ì–´",
            "english": "English",
            "japanese": "æ—¥æœ¬èª"
        }

        # ìë§‰ ì–¸ì–´ ì§€ì‹œë¬¸ ìƒì„±
        primary_lang = language_instructions.get(subtitle_language_primary, "í•œêµ­ì–´")

        if subtitle_language_secondary and subtitle_language_secondary != "":
            secondary_lang = language_instructions.get(subtitle_language_secondary, "")
            lang_instruction = f"""**ğŸš¨ í•„ìˆ˜ - ìë§‰ ì–¸ì–´ ì§€ì‹œì‚¬í•­ ğŸš¨**:

ê° í”„ë ˆì„ë§ˆë‹¤ ë°˜ë“œì‹œ ë‹¤ìŒ **3ê°œì˜ ìë§‰ì„ ëª¨ë‘ ì‘ì„±**í•´ì•¼ í•©ë‹ˆë‹¤:

1ï¸âƒ£ **2-0. í•œê¸€ ìë§‰ (ê¸°ë³¸ ìë§‰)** â† í•„ìˆ˜!
   - ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±
   - ëª¨ë“  í•œêµ­ ì‚¬ìš©ìë¥¼ ìœ„í•œ ê¸°ë³¸ ìë§‰
   - ì˜ˆì‹œ: "ê°œì™€ ê³ ì–‘ì´ì˜ ëŒ€ê²°!"

2ï¸âƒ£ **2-1. ì£¼ ìë§‰ ({primary_lang})** â† í•„ìˆ˜!
   - ë°˜ë“œì‹œ {primary_lang}ë¡œ ì‘ì„±
   - í™”ë©´ ìƒë‹¨ ë°°ì¹˜ìš©
   - ì˜ˆì‹œ: "Epic Pet Battle!"

3ï¸âƒ£ **2-2. ë³´ì¡° ìë§‰ ({secondary_lang})** â† í•„ìˆ˜!
   - ë°˜ë“œì‹œ {secondary_lang}ë¡œ ì‘ì„±
   - í™”ë©´ í•˜ë‹¨ ë°°ì¹˜ìš©
   - ì˜ˆì‹œ: "ãƒšãƒƒãƒˆå¤§æ±ºé—˜ï¼"

âš ï¸ **ì¤‘ìš”**: í•˜ë‚˜ë¼ë„ ë¹ ëœ¨ë¦¬ì§€ ë§ê³  ë°˜ë“œì‹œ ì„¸ ê°œ ëª¨ë‘ ì‘ì„±í•˜ì„¸ìš”!"""
        else:
            lang_instruction = f"**ì¤‘ìš”**: ëª¨ë“  ìë§‰ê³¼ ë‚˜ë ˆì´ì…˜ì€ **{primary_lang}**ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."

        if not frames:
            raise HTTPException(status_code=400, detail="ë¶„ì„í•  í”„ë ˆì„ì´ ì—†ìŠµë‹ˆë‹¤.")

        # ê° í”„ë ˆì„ì— í•´ë‹¹í•˜ëŠ” ì œëª© ë§¤ì¹­
        def match_title_for_frame(frame_time, titles):
            """í”„ë ˆì„ ì‹œê°„ì— í•´ë‹¹í•˜ëŠ” ìë§‰ ì œëª© ì°¾ê¸°"""
            for title in titles:
                start_time = title.get('startTime', 0)
                end_time = title.get('endTime', 0)
                if start_time <= frame_time < end_time:
                    return title.get('title')
            return None

        # ê° í”„ë ˆì„ì— ì œëª© ì •ë³´ ì¶”ê°€
        for i, frame in enumerate(frames, 1):
            frame_time = frame.get('time', 0)
            matched_title = match_title_for_frame(frame_time, original_titles)
            frame['matched_title'] = matched_title
            logging.info(f"  - í”„ë ˆì„ {i} ({frame_time}ì´ˆ): ë§¤ì¹­ëœ ì œëª© = '{matched_title}'")

        # ì›ë³¸ ì œëª© ì •ë³´ êµ¬ì„±
        original_titles_text = ""
        if original_titles and len(original_titles) > 0:
            titles_list = "\n".join([f"- [{t.get('frame')}] {t.get('title')} ({t.get('startTime')}ì´ˆ~{t.get('endTime')}ì´ˆ)" for t in original_titles])
            original_titles_text = f"""
**ì¤‘ìš” - ì›ë³¸ ì˜ìƒ ì œëª© ì°¸ê³ **:
ì´ ì˜ìƒë“¤ì˜ ì›ë³¸ ì œëª©ê³¼ ì‹œê°„ êµ¬ê°„ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
{titles_list}

**ê° í”„ë ˆì„ì€ í•´ë‹¹ ì‹œê°„ êµ¬ê°„ì˜ ì œëª©ì„ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„í•´ì•¼ í•©ë‹ˆë‹¤!**

ì›ë³¸ ì œëª©ì˜ í‚¤ì›Œë“œì™€ ë¶„ìœ„ê¸°ë¥¼ **ë°˜ë“œì‹œ** ë°˜ì˜í•˜ì—¬ ë¶„ì„í•´ì£¼ì„¸ìš”.
ì˜ˆë¥¼ ë“¤ì–´ "fight", "battle", "vs"ê°€ í¬í•¨ë˜ì–´ ìˆë‹¤ë©´ ì‹¸ì›€ì´ë‚˜ ëŒ€ë¦½ ìƒí™©ìœ¼ë¡œ,
"friendship", "love", "together"ê°€ í¬í•¨ë˜ì–´ ìˆë‹¤ë©´ ìš°ì •ì´ë‚˜ ì¹œë°€í•¨ìœ¼ë¡œ í•´ì„í•´ì£¼ì„¸ìš”.
ì›ë³¸ ì œëª©ì„ ë¬´ì‹œí•˜ê³  ì„ì˜ë¡œ í•´ì„í•˜ì§€ ë§ˆì„¸ìš”.

"""

        # ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ê°€ ì œê³µë˜ë©´ ì‚¬ìš©, ì•„ë‹ˆë©´ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        if analysis_type == "custom" and custom_prompt:
            prompt = original_titles_text + custom_prompt
        else:
            # ìë§‰ ì„¹ì…˜ êµ¬ì„±
            if subtitle_language_secondary and subtitle_language_secondary != "":
                subtitle_section = f"""### 2-0. í•œê¸€ ìë§‰ (ê¸°ë³¸ ìë§‰) âœ… í•„ìˆ˜
- í•œêµ­ì–´ë¡œ ì‘ì„± (20ì ì´ë‚´)
- ì›ë³¸ ì˜ìƒ ì œëª©ì˜ í‚¤ì›Œë“œë¥¼ ë°˜ì˜í•œ ì„íŒ©íŠ¸ ìˆëŠ” í•œê¸€ ë¬¸êµ¬
- ì˜ˆ: "ê°œì™€ ê³ ì–‘ì´ ê²©íˆ¬!", "ìš°ì •ì˜ ì‹œì‘"

### 2-1. ì£¼ ìë§‰ ({primary_lang}) - ìƒë‹¨ ë°°ì¹˜ìš© âœ… í•„ìˆ˜
- {primary_lang}ë¡œ ì‘ì„± (20ì ì´ë‚´)
- í™”ë©´ ìƒë‹¨ì— í‘œì‹œë  ì„íŒ©íŠ¸ ìˆëŠ” í…ìŠ¤íŠ¸
- ì›ë³¸ ì˜ìƒ ì œëª©ì˜ í‚¤ì›Œë“œë¥¼ ë°˜ì˜

### 2-2. ë³´ì¡° ìë§‰ ({secondary_lang}) - í•˜ë‹¨ ë°°ì¹˜ìš© âœ… í•„ìˆ˜
- {secondary_lang}ë¡œ ì‘ì„± (20ì ì´ë‚´)
- í™”ë©´ í•˜ë‹¨ì— í‘œì‹œë  ì„íŒ©íŠ¸ ìˆëŠ” í…ìŠ¤íŠ¸
- ì›ë³¸ ì˜ìƒ ì œëª©ì˜ í‚¤ì›Œë“œë¥¼ ë°˜ì˜"""
            else:
                subtitle_section = """### 2. í™”ë©´ í…ìŠ¤íŠ¸ (ìë§‰/ìº¡ì…˜)
- í™”ë©´ì— í‘œì‹œë  ì„íŒ©íŠ¸ ìˆëŠ” í…ìŠ¤íŠ¸ (20ì ì´ë‚´)
- ê°•ì¡°í•  í‚¤ì›Œë“œë‚˜ ë¬¸êµ¬"""

            # ê° í”„ë ˆì„ë³„ ì œëª© ì •ë³´ êµ¬ì„±
            frame_title_info = "\n\n**ê° í”„ë ˆì„ì˜ ì›ë³¸ ì˜ìƒ ì œëª© (ë°˜ë“œì‹œ ì´ ì œëª©ì— ë§ì¶° ë¶„ì„í•˜ì„¸ìš”!)**:\n"
            for i, frame in enumerate(frames, 1):
                matched_title = frame.get('matched_title')
                frame_time = frame.get('time', 0)
                if matched_title:
                    frame_title_info += f"- í”„ë ˆì„ {i} ({frame_time}ì´ˆ): \"{matched_title}\" â† ì´ ì œëª© ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„!\n"
                else:
                    frame_title_info += f"- í”„ë ˆì„ {i} ({frame_time}ì´ˆ): ì œëª© ì—†ìŒ\n"

            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompts = {
                "shorts-production": f"""{original_titles_text}{frame_title_info}

ë‹¤ìŒì€ '{video_name}' ì˜ìƒì—ì„œ ì¶”ì¶œí•œ {len(frames)}ê°œì˜ í”„ë ˆì„ì…ë‹ˆë‹¤.
ì´ í”„ë ˆì„ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ YouTube ì‡¼ì¸ (Shorts) ì œì‘ì„ ìœ„í•œ ì¢…í•© ë¶„ì„ì„ í•´ì£¼ì„¸ìš”.

{lang_instruction}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ê° í”„ë ˆì„ë³„ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:

## í”„ë ˆì„ [ë²ˆí˜¸]: [íƒ€ì„ìŠ¤íƒ¬í”„] - ì›ë³¸ ì œëª©: "[í•´ë‹¹ ì œëª©]"

âš ï¸ **ì¤‘ìš”**: ìœ„ì— ëª…ì‹œëœ ê° í”„ë ˆì„ì˜ ì›ë³¸ ì œëª©ì„ **ë°˜ë“œì‹œ** ì°¸ê³ í•˜ì—¬ ê·¸ ì œëª©ì˜ ë‚´ìš©ê³¼ ë¶„ìœ„ê¸°ì— ë§ê²Œ ë¶„ì„í•˜ì„¸ìš”!

### 1. ì‹œì²­ììš© ì½˜í…ì¸  ì„¤ëª…
- ì´ ì¥ë©´ì—ì„œ ì „ë‹¬í•˜ë ¤ëŠ” í•µì‹¬ ë©”ì‹œì§€
- ì‹œì²­ìê°€ ëŠê»´ì•¼ í•  ê°ì •ì´ë‚˜ ë°˜ì‘
- ì£¼ëª©í•´ì•¼ í•  í¬ì¸íŠ¸

**ì¤‘ìš”: ë‹¤ìŒ ìë§‰ ì„¹ì…˜ì€ ëª¨ë‘ í•„ìˆ˜ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ ëª¨ë“  ì„¹ì…˜ì„ ì‘ì„±í•˜ì„¸ìš”!**

{subtitle_section}

**ìë§‰ ì‘ì„± ì˜ˆì‹œ:**
### 2-0. í•œê¸€ ìë§‰ (ê¸°ë³¸ ìë§‰)
- "ê°œì™€ ê³ ì–‘ì´ì˜ ëŒ€ê²°!"

### 2-1. ì£¼ ìë§‰ (English) - ìƒë‹¨ ë°°ì¹˜ìš©
- "Epic Pet Battle!"

### 2-2. ë³´ì¡° ìë§‰ (æ—¥æœ¬èª) - í•˜ë‹¨ ë°°ì¹˜ìš©
- "ãƒšãƒƒãƒˆå¤§æ±ºé—˜ï¼"

### 3. ë‚˜ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸
- ë³´ì´ìŠ¤ì˜¤ë²„ë¡œ ì½ì„ ëŒ€ì‚¬ (ìì—°ìŠ¤ëŸ½ê³  êµ¬ì–´ì²´ë¡œ)
- ì˜ˆìƒ ì½ê¸° ì‹œê°„: 3-5ì´ˆ

### 4. AI ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸ (**ë°˜ë“œì‹œ ì˜ì–´ë¡œë§Œ ì‘ì„±**)
- **IMPORTANT: Write ONLY in English!**
- DALL-E/Midjourneyìš© ìƒì„¸ ë¬˜ì‚¬
- ìŠ¤íƒ€ì¼, ì¡°ëª…, êµ¬ë„, ìƒ‰ê° í¬í•¨
- ì˜ˆì‹œ: "A vibrant YouTube shorts thumbnail, close-up shot, warm lighting, person expressing excitement, modern minimalist background, high contrast, 9:16 aspect ratio"

### 5. AI ì˜ìƒ ìƒì„± í”„ë¡¬í”„íŠ¸ (**ë°˜ë“œì‹œ ì˜ì–´ë¡œë§Œ ì‘ì„±**)
- **IMPORTANT: Write ONLY in English!**
- Sora/Kling/Runwayìš© ë™ì  ì¥ë©´ ë¬˜ì‚¬
- ì¹´ë©”ë¼ ì›€ì§ì„, ì•¡ì…˜, ëª¨ì…˜ í¬í•¨
- ì‹œê°„ì  íë¦„ê³¼ ë³€í™” ì„¤ëª…
- ì˜ˆì‹œ: "Slow zoom in on person's excited face, hands gesturing enthusiastically, natural head movements, soft camera shake for realism, warm lighting gradually brightening, 9:16 vertical format, 3-5 seconds duration"

### 6. í¸ì§‘ ë…¸íŠ¸
- ì „í™˜ íš¨ê³¼ ì œì•ˆ
- BGM ë¶„ìœ„ê¸° ì œì•ˆ
- ì¶”ê°€ ì‹œê° íš¨ê³¼ ì•„ì´ë””ì–´

ì „ì²´ ì‡¼ì¸ ëŠ” 60ì´ˆ ì´ë‚´ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ê° í”„ë ˆì„ì´ ì „ì²´ ìŠ¤í† ë¦¬ íë¦„ì—ì„œ ì–´ë–¤ ì—­í• ì„ í•˜ëŠ”ì§€ ê³ ë ¤í•´ì£¼ì„¸ìš”.""",

                "scene-description": f"""{original_titles_text}ë‹¤ìŒì€ '{video_name}' ì˜ìƒì—ì„œ ì¶”ì¶œí•œ {len(frames)}ê°œì˜ í”„ë ˆì„ì…ë‹ˆë‹¤.
ê° í”„ë ˆì„ì„ ìƒì„¸íˆ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:

1. ê° í”„ë ˆì„ì˜ ì¥ë©´ ì„¤ëª… (ë¬´ì—‡ì´ ë³´ì´ëŠ”ì§€, ì–´ë–¤ ìƒí™©ì¸ì§€)
2. ì£¼ìš” ê°ì²´ë‚˜ ì¸ë¬¼
3. ë°°ê²½ê³¼ ë¶„ìœ„ê¸°
4. ì „ì²´ì ì¸ ìŠ¤í† ë¦¬ íë¦„

í”„ë ˆì„ ìˆœì„œëŒ€ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”.""",

                "object-detection": f"""{original_titles_text}'{video_name}' ì˜ìƒì˜ {len(frames)}ê°œ í”„ë ˆì„ì—ì„œ ë³´ì´ëŠ” ëª¨ë“  ê°ì²´ë¥¼ ì¸ì‹í•˜ê³  ë‚˜ì—´í•´ì£¼ì„¸ìš”.

ê° í”„ë ˆì„ë³„ë¡œ:
- ì‚¬ëŒ (ìˆ˜, ì„±ë³„, ì—°ë ¹ëŒ€ ë“±)
- ë¬¼ì²´ (ì¢…ë¥˜, ìœ„ì¹˜)
- í…ìŠ¤íŠ¸ (ìˆë‹¤ë©´)
- ë¸Œëœë“œë‚˜ ë¡œê³  (ìˆë‹¤ë©´)

ì„ ìì„¸íˆ ë¶„ì„í•´ì£¼ì„¸ìš”.""",

                "text-extraction": f"""{original_titles_text}'{video_name}' ì˜ìƒì˜ {len(frames)}ê°œ í”„ë ˆì„ì—ì„œ ë³´ì´ëŠ” ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ê° í”„ë ˆì„ë³„ë¡œ:
- í™”ë©´ì— í‘œì‹œëœ ëª¨ë“  í…ìŠ¤íŠ¸
- í…ìŠ¤íŠ¸ì˜ ìœ„ì¹˜ì™€ í¬ê¸°
- í…ìŠ¤íŠ¸ì˜ ì¤‘ìš”ë„
- ìë§‰, ì œëª©, ì„¤ëª… ë“± êµ¬ë¶„

ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.""",

                "story-flow": f"""{original_titles_text}'{video_name}' ì˜ìƒì˜ {len(frames)}ê°œ í”„ë ˆì„ì„ ì‹œê°„ìˆœìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ìŠ¤í† ë¦¬ì˜ íë¦„ì„ íŒŒì•…í•´ì£¼ì„¸ìš”.

ë‹¤ìŒì„ í¬í•¨í•´ì£¼ì„¸ìš”:
- ì „ì²´ ìŠ¤í† ë¦¬ ìš”ì•½
- ì‹œì‘-ì „ê°œ-ì ˆì •-ê²°ë§ êµ¬ì¡°
- ê° ì¥ë©´ ì „í™˜ì˜ ì˜ë¯¸
- í•µì‹¬ ë©”ì‹œì§€ë‚˜ ì£¼ì œ

ì˜ìƒì˜ ë‚´ëŸ¬í‹°ë¸Œë¥¼ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë¶„ì„í•´ì£¼ì„¸ìš”.""",

                "thumbnail-suggest": f"""{original_titles_text}'{video_name}' ì˜ìƒì˜ {len(frames)}ê°œ í”„ë ˆì„ ì¤‘ì—ì„œ ì¸ë„¤ì¼ë¡œ ì‚¬ìš©í•˜ê¸° ê°€ì¥ ì¢‹ì€ í”„ë ˆì„ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”:
- ì‹œê°ì  ì„íŒ©íŠ¸
- í´ë¦­ì„ ìœ ë„í•˜ëŠ” ìš”ì†Œ
- ì˜ìƒ ë‚´ìš© ëŒ€í‘œì„±
- ê°ì •ì  ì–´í•„

ê° í”„ë ˆì„ì„ í‰ê°€í•˜ê³  ìˆœìœ„ë¥¼ ë§¤ê²¨ì£¼ì„¸ìš”."""
            }

            prompt = prompts.get(analysis_type, prompts["scene-description"])

        # AI ëª¨ë¸ë³„ ì²˜ë¦¬
        if model in ["sonnet", "haiku"]:
            # Claude API ì‚¬ìš©
            client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))

            model_map = {
                "sonnet": "claude-3-5-sonnet-20241022",
                "haiku": "claude-3-5-haiku-20241022"
            }

            # ì´ë¯¸ì§€ ë©”ì‹œì§€ êµ¬ì„±
            content = [{"type": "text", "text": prompt}]

            for frame in frames:
                content.append({
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": "image/jpeg",
                        "data": frame["image_data"]
                    }
                })

            message = client.messages.create(
                model=model_map[model],
                max_tokens=4000,
                messages=[{"role": "user", "content": content}]
            )

            analysis_result = message.content[0].text

        else:
            # OpenAI API ì‚¬ìš©
            client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

            model_map = {
                "gpt-4o-mini": "gpt-4o-mini",
                "gpt-4o": "gpt-4o"
            }

            # ì´ë¯¸ì§€ ë©”ì‹œì§€ êµ¬ì„±
            content = [{"type": "text", "text": prompt}]

            for frame in frames:
                content.append({
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{frame['image_data']}"
                    }
                })

            response = client.chat.completions.create(
                model=model_map.get(model, "gpt-4o-mini"),
                messages=[{"role": "user", "content": content}],
                max_tokens=4000
            )

            analysis_result = response.choices[0].message.content

        logging.info(f"AI í”„ë ˆì„ ë¶„ì„ ì™„ë£Œ: {len(frames)}ê°œ í”„ë ˆì„, ëª¨ë¸: {model}, íƒ€ì…: {analysis_type}")

        return {
            "success": True,
            "analysis_result": analysis_result,
            "frames_count": len(frames),
            "model": model,
            "analysis_type": analysis_type
        }

    except HTTPException:
        raise
    except Exception as e:
        logging.exception("AI í”„ë ˆì„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
        raise HTTPException(status_code=500, detail=f"AI í”„ë ˆì„ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")


@app.put("/api/video-analyzer/rename-file")
async def api_rename_video_file(payload: Dict[str, Any] = Body(...)) -> Dict[str, Any]:
    """ì˜ìƒ íŒŒì¼ ì´ë¦„ ë³€ê²½"""
    try:
        old_path = payload.get("old_path")
        new_name = payload.get("new_name")

        if not old_path or not new_name:
            raise HTTPException(status_code=400, detail="old_pathì™€ new_nameì´ í•„ìš”í•©ë‹ˆë‹¤.")

        old_path_obj = Path(old_path)

        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not old_path_obj.exists():
            raise HTTPException(status_code=404, detail=f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {old_path}")

        # ë””ë ‰í† ë¦¬ì¸ ê²½ìš° ê±°ë¶€
        if old_path_obj.is_dir():
            raise HTTPException(status_code=400, detail="ë””ë ‰í† ë¦¬ëŠ” ì´ë¦„ì„ ë³€ê²½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # ìƒˆ ê²½ë¡œ ìƒì„±
        new_path_obj = old_path_obj.parent / new_name

        # ì´ë¯¸ ê°™ì€ ì´ë¦„ì˜ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
        if new_path_obj.exists():
            raise HTTPException(status_code=400, detail=f"ì´ë¯¸ ê°™ì€ ì´ë¦„ì˜ íŒŒì¼ì´ ì¡´ì¬í•©ë‹ˆë‹¤: {new_name}")

        # íŒŒì¼ ì´ë¦„ ë³€ê²½
        old_path_obj.rename(new_path_obj)

        logging.info(f"íŒŒì¼ ì´ë¦„ ë³€ê²½ ì™„ë£Œ: {old_path} -> {new_path_obj}")

        return {
            "success": True,
            "message": "íŒŒì¼ ì´ë¦„ì´ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "old_path": str(old_path),
            "new_path": str(new_path_obj),
            "new_name": new_name
        }

    except HTTPException:
        raise
    except PermissionError:
        raise HTTPException(status_code=403, detail="íŒŒì¼ ì´ë¦„ ë³€ê²½ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        logging.exception("íŒŒì¼ ì´ë¦„ ë³€ê²½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì´ë¦„ ë³€ê²½ ì‹¤íŒ¨: {str(e)}")


@app.delete("/api/video-analyzer/delete-file")
async def api_delete_video_file(payload: Dict[str, Any] = Body(...)) -> Dict[str, Any]:
    """ì˜ìƒ íŒŒì¼ ì‚­ì œ"""
    import os

    try:
        file_path = payload.get("file_path")

        if not file_path:
            raise HTTPException(status_code=400, detail="file_pathê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        file_path_obj = Path(file_path)

        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not file_path_obj.exists():
            raise HTTPException(status_code=404, detail=f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")

        # ë””ë ‰í† ë¦¬ì¸ ê²½ìš° ê±°ë¶€
        if file_path_obj.is_dir():
            raise HTTPException(status_code=400, detail="ë””ë ‰í† ë¦¬ëŠ” ì‚­ì œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # íŒŒì¼ ì‚­ì œ
        file_size = file_path_obj.stat().st_size
        file_path_obj.unlink()

        logging.info(f"íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {file_path} ({file_size} bytes)")

        return {
            "success": True,
            "message": "íŒŒì¼ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "deleted_file": str(file_path),
            "size_bytes": file_size
        }

    except HTTPException:
        raise
    except PermissionError:
        raise HTTPException(status_code=403, detail="íŒŒì¼ ì‚­ì œ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        logging.exception("íŒŒì¼ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")


@app.post("/api/video-analyzer/translate-subtitles")
async def api_translate_subtitles(payload: Dict[str, Any] = Body(...)) -> Dict[str, Any]:
    """ìë§‰ ë²ˆì—­ API"""
    import os

    try:
        subtitles = payload.get("subtitles", [])
        target_lang = payload.get("target_lang", "en")

        if not subtitles:
            raise HTTPException(status_code=400, detail="ë²ˆì—­í•  ìë§‰ì´ ì—†ìŠµë‹ˆë‹¤.")

        logging.info(f"ìë§‰ ë²ˆì—­ ì‹œì‘: {len(subtitles)}ê°œ ìë§‰ì„ {target_lang}ë¡œ ë²ˆì—­")

        # ì–¸ì–´ ì½”ë“œë¥¼ ì–¸ì–´ ì´ë¦„ìœ¼ë¡œ ë³€í™˜
        lang_names = {
            'en': 'English',
            'ja': 'Japanese',
            'zh': 'Chinese',
            'es': 'Spanish',
            'fr': 'French',
            'de': 'German',
            'vi': 'Vietnamese',
            'th': 'Thai'
        }
        target_lang_name = lang_names.get(target_lang, target_lang)

        # Anthropic API ì‚¬ìš© (Claude)
        try:
            import anthropic

            api_key = os.getenv("ANTHROPIC_API_KEY")
            if not api_key:
                raise ValueError("ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            client = anthropic.Anthropic(api_key=api_key)

            # ìë§‰ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
            subtitle_texts = [sub.get('text', '') for sub in subtitles]

            # ë²ˆì—­ ìš”ì²­
            prompt = f"""Translate the following subtitles to {target_lang_name}.
Keep the same number of lines and maintain the timing structure.
Only return the translated text for each subtitle, one per line, without any additional formatting or explanations.

Original subtitles:
{chr(10).join(subtitle_texts)}"""

            message = client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=8000,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )

            # ë²ˆì—­ ê²°ê³¼ íŒŒì‹±
            translated_text = message.content[0].text
            translated_lines = translated_text.strip().split('\n')

            # ë²ˆì—­ëœ í…ìŠ¤íŠ¸ë¥¼ ì›ë³¸ ìë§‰ êµ¬ì¡°ì— ë§¤í•‘
            translated_subtitles = []
            for i, subtitle in enumerate(subtitles):
                if i < len(translated_lines):
                    translated_subtitles.append({
                        'start': subtitle.get('start'),
                        'end': subtitle.get('end'),
                        'text': translated_lines[i].strip()
                    })
                else:
                    # ë²ˆì—­ ê²°ê³¼ê°€ ë¶€ì¡±í•œ ê²½ìš° ì›ë³¸ ìœ ì§€
                    translated_subtitles.append(subtitle)

            logging.info(f"ë²ˆì—­ ì™„ë£Œ: {len(translated_subtitles)}ê°œ ìë§‰")

            return {
                "success": True,
                "translated_subtitles": translated_subtitles,
                "target_lang": target_lang,
                "count": len(translated_subtitles)
            }

        except ImportError:
            # Anthropicì´ ì—†ìœ¼ë©´ OpenAI ì‹œë„
            try:
                import openai

                api_key = os.getenv("OPENAI_API_KEY")
                if not api_key:
                    raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

                client = openai.OpenAI(api_key=api_key)

                # ìë§‰ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                subtitle_texts = [sub.get('text', '') for sub in subtitles]

                # ë²ˆì—­ ìš”ì²­
                prompt = f"""Translate the following subtitles to {target_lang_name}.
Keep the same number of lines and maintain the timing structure.
Only return the translated text for each subtitle, one per line, without any additional formatting or explanations.

Original subtitles:
{chr(10).join(subtitle_texts)}"""

                response = client.chat.completions.create(
                    model="gpt-4",
                    messages=[
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=8000
                )

                # ë²ˆì—­ ê²°ê³¼ íŒŒì‹±
                translated_text = response.choices[0].message.content
                translated_lines = translated_text.strip().split('\n')

                # ë²ˆì—­ëœ í…ìŠ¤íŠ¸ë¥¼ ì›ë³¸ ìë§‰ êµ¬ì¡°ì— ë§¤í•‘
                translated_subtitles = []
                for i, subtitle in enumerate(subtitles):
                    if i < len(translated_lines):
                        translated_subtitles.append({
                            'start': subtitle.get('start'),
                            'end': subtitle.get('end'),
                            'text': translated_lines[i].strip()
                        })
                    else:
                        # ë²ˆì—­ ê²°ê³¼ê°€ ë¶€ì¡±í•œ ê²½ìš° ì›ë³¸ ìœ ì§€
                        translated_subtitles.append(subtitle)

                logging.info(f"ë²ˆì—­ ì™„ë£Œ: {len(translated_subtitles)}ê°œ ìë§‰")

                return {
                    "success": True,
                    "translated_subtitles": translated_subtitles,
                    "target_lang": target_lang,
                    "count": len(translated_subtitles)
                }

            except ImportError:
                raise HTTPException(
                    status_code=500,
                    detail="ë²ˆì—­ API ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. anthropic ë˜ëŠ” openai íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”."
                )

    except HTTPException:
        raise
    except Exception as e:
        logging.exception("ìë§‰ ë²ˆì—­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
        raise HTTPException(status_code=500, detail=f"ìë§‰ ë²ˆì—­ ì‹¤íŒ¨: {str(e)}")


@app.post("/api/video-analyzer/analyze-subtitles-with-ai")
async def api_analyze_subtitles_with_ai(payload: Dict[str, Any] = Body(...)) -> Dict[str, Any]:
    """AIë¥¼ ì‚¬ìš©í•œ ìë§‰ ë¶„ì„ - ì‡¼ì¸  ìµœì í™”, ì˜ìƒ ìš”ì•½, êµìœ¡ ì½˜í…ì¸  ì œì‘"""
    import anthropic
    import openai
    import os

    try:
        subtitle_content = payload.get("subtitle_content", "")
        model = payload.get("model", "sonnet")
        analysis_type = payload.get("analysis_type", "enhanced-shorts")

        if not subtitle_content:
            raise HTTPException(status_code=400, detail="ë¶„ì„í•  ìë§‰ì´ ì—†ìŠµë‹ˆë‹¤.")

        logging.info(f"AI ìë§‰ ë¶„ì„ ì‹œì‘: ëª¨ë¸={model}, íƒ€ì…={analysis_type}")

        # ë¶„ì„ íƒ€ì…ë³„ í”„ë¡¬í”„íŠ¸
        type_prompts = {
            "enhanced-shorts": """ì´ ìë§‰ì„ 60ì´ˆ ì´ë‚´ YouTube ì‡¼ì¸ ë¡œ í¸ì§‘í•˜ê¸° ìœ„í•´ ë¶„ì„í•´ì£¼ì„¸ìš”.
- í•µì‹¬ í•˜ì´ë¼ì´íŠ¸ êµ¬ê°„ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” ì‚­ì œ
- 60-70% ì •ë„ì˜ ìë§‰ì„ ì‚­ì œí•˜ì—¬ ë¹ ë¥¸ í…œí¬ ìœ ì§€
- ì„íŒ©íŠ¸ ìˆëŠ” í…ìŠ¤íŠ¸ ì˜¤ë²„ë ˆì´ ì¶”ê°€ (ì˜ˆ: "ğŸ’¥ ì£¼ëª©!", "ğŸ”¥ í•µì‹¬")
- ì‹œì²­ì í›„í‚¹ì„ ìœ„í•œ ì§§ì€ ë‚˜ë ˆì´ì…˜ ì¶”ê°€""",

            "enhanced-summary": """ì´ ìë§‰ì„ 3-5ë¶„ ìš”ì•½ ì˜ìƒìœ¼ë¡œ í¸ì§‘í•˜ê¸° ìœ„í•´ ë¶„ì„í•´ì£¼ì„¸ìš”.
- í•µì‹¬ ë‚´ìš©ë§Œ ë‚¨ê¸°ê³  ë°˜ë³µ/ë¶€ì—°ì„¤ëª… ì‚­ì œ
- 40-50% ì •ë„ì˜ ìë§‰ ì‚­ì œ
- ì±•í„° êµ¬ë¶„ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì¶”ê°€ (ì˜ˆ: "[ì±•í„° 1]", "[í•µì‹¬ ìš”ì•½]")
- ìš”ì•½ ì„¤ëª…ì„ ìœ„í•œ ë‚˜ë ˆì´ì…˜ ì¶”ê°€""",

            "enhanced-education": """ì´ ìë§‰ì„ êµìœ¡ ì½˜í…ì¸ ë¡œ í¸ì§‘í•˜ê¸° ìœ„í•´ ë¶„ì„í•´ì£¼ì„¸ìš”.
- í•µì‹¬ êµìœ¡ ë‚´ìš©ì€ ìµœëŒ€í•œ ìœ ì§€
- 20-30% ì •ë„ë§Œ ì‚­ì œ (ë¶ˆí•„ìš”í•œ ë¶€ë¶„ë§Œ)
- ê°œë… ê°•ì¡°ë¥¼ ìœ„í•œ í…ìŠ¤íŠ¸ ì¶”ê°€ (ì˜ˆ: "ğŸ“Œ ê°œë…", "ğŸ’¡ ë³µìŠµ í¬ì¸íŠ¸")
- ì„¤ëª…ì„ ë³´ì™„í•˜ëŠ” ë‚˜ë ˆì´ì…˜ ì¶”ê°€"""
        }

        prompt = f"""{type_prompts.get(analysis_type, type_prompts["enhanced-shorts"])}

ìë§‰ ë‚´ìš©:
{subtitle_content}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ë¶„ì„ ê²°ê³¼ë¥¼ ë°˜í™˜í•´ì£¼ì„¸ìš”:
{{
    "video_type": "{analysis_type}",
    "kept_originals": [
        {{
            "index": ë²ˆí˜¸,
            "time": "ì‹œì‘ --> ë",
            "text": "ìë§‰ í…ìŠ¤íŠ¸",
            "reason": "ìœ ì§€ ì´ìœ ",
            "importance": "high/medium/low"
        }}
    ],
    "deletions": [
        {{
            "index": ë²ˆí˜¸,
            "time": "ì‹œì‘ --> ë",
            "text": "ìë§‰ í…ìŠ¤íŠ¸",
            "reason": "ì‚­ì œ ì´ìœ ",
            "category": "ë°˜ë³µ/ë¶€ì—°/ë¶ˆí•„ìš”"
        }}
    ],
    "text_additions": [
        {{
            "insert_after": ë²ˆí˜¸,
            "estimated_time": "00:00:10",
            "text": "ì¶”ê°€í•  í…ìŠ¤íŠ¸",
            "type": "í›„í‚¹/ì±•í„°ì œëª©/ê°œë…ì •ì˜",
            "position": "top/center/bottom"
        }}
    ],
    "narration_additions": [
        {{
            "insert_after": ë²ˆí˜¸,
            "estimated_time": "00:00:10",
            "narration": "ë‚˜ë ˆì´ì…˜ í…ìŠ¤íŠ¸",
            "type": "ë„ì…/ì „í™˜/ìš”ì•½",
            "tone": "í¥ë¯¸ì§„ì§„í•œ/ì°¨ë¶„í•œ/ì§„ì§€í•œ"
        }}
    ],
    "statistics": {{
        "original_count": ì „ì²´ ìë§‰ ìˆ˜,
        "kept_count": ìœ ì§€ ìë§‰ ìˆ˜,
        "delete_count": ì‚­ì œ ìë§‰ ìˆ˜,
        "text_add_count": í…ìŠ¤íŠ¸ ì¶”ê°€ ìˆ˜,
        "narration_add_count": ë‚˜ë ˆì´ì…˜ ì¶”ê°€ ìˆ˜
    }}
}}"""

        # API í˜¸ì¶œ
        if model in ["sonnet", "haiku"]:
            # Anthropic API ì‚¬ìš©
            api_key = os.getenv("ANTHROPIC_API_KEY")
            if not api_key:
                raise HTTPException(status_code=400, detail="ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            client = anthropic.Anthropic(api_key=api_key)

            model_name = "claude-3-5-sonnet-20241022" if model == "sonnet" else "claude-3-5-haiku-20241022"

            message = client.messages.create(
                model=model_name,
                max_tokens=8000,
                messages=[{"role": "user", "content": prompt}]
            )

            response_text = message.content[0].text.strip()

        else:
            # OpenAI API ì‚¬ìš©
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise HTTPException(status_code=400, detail="OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            client = openai.OpenAI(api_key=api_key)

            model_name = "gpt-4o" if model == "gpt-4o" else "gpt-4o-mini"

            completion = client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=8000
            )

            response_text = completion.choices[0].message.content.strip()

        # JSON íŒŒì‹±
        import json
        import re

        # JSON ì¶”ì¶œ (ì½”ë“œ ë¸”ë¡ ì œê±°)
        json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', response_text, re.DOTALL)
        if json_match:
            response_text = json_match.group(1)

        result = json.loads(response_text)

        logging.info(f"AI ìë§‰ ë¶„ì„ ì™„ë£Œ: {result.get('statistics', {})}")

        return {
            "success": True,
            "result": result
        }

    except json.JSONDecodeError as e:
        logging.error(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}\nì‘ë‹µ: {response_text}")
        raise HTTPException(status_code=500, detail=f"AI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
    except HTTPException:
        raise
    except Exception as e:
        logging.exception("AI ìë§‰ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
        raise HTTPException(status_code=500, detail=f"AI ìë§‰ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")


@app.post("/api/video-analyzer/generate-title-subtitle")
async def api_generate_title_subtitle(payload: Dict[str, Any] = Body(...)) -> Dict[str, Any]:
    """AIë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ìƒ ì œëª©ê³¼ ë¶€ì œëª© ìƒì„±"""
    import os

    try:
        video_path_str = payload.get("video_path", "")
        video_info = payload.get("video_info", {})
        selected_model = payload.get("model", "claude")  # ê¸°ë³¸ê°’ì€ claude

        if not video_path_str:
            raise HTTPException(status_code=400, detail="ì˜ìƒ ê²½ë¡œê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        video_path = Path(video_path_str)
        if not video_path.exists():
            raise HTTPException(status_code=404, detail="ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        logging.info(f"AI ì œëª©/ë¶€ì œëª© ìƒì„± ì‹œì‘: {video_path.name} (ëª¨ë¸: {selected_model})")

        # ì˜ìƒ ì •ë³´ ì¶”ì¶œ
        video_name = video_path.stem
        video_ext = video_path.suffix

        # ìë§‰ íŒŒì¼ ì°¾ê¸° (ê°™ì€ í´ë”ì—ì„œ)
        subtitle_text = ""
        for ext in ['.ko.srt', '.srt', '.en.srt']:
            subtitle_path = video_path.parent / f"{video_path.stem}{ext}"
            if subtitle_path.exists():
                try:
                    with open(subtitle_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        # ìë§‰ ë²ˆí˜¸ì™€ íƒ€ì„ì½”ë“œ ì œê±°, í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                        import re
                        subtitle_blocks = re.split(r'\n\d+\n', content)
                        text_lines = []
                        for block in subtitle_blocks:
                            lines = block.strip().split('\n')
                            # íƒ€ì„ì½”ë“œ ë¼ì¸ ì œê±° (00:00:00,000 --> 00:00:00,000 í˜•ì‹)
                            text_content = [line for line in lines if '-->' not in line]
                            text_lines.extend(text_content)
                        subtitle_text = ' '.join(text_lines[:50])  # ì²˜ìŒ 50ì¤„ë§Œ ì‚¬ìš©
                        break
                except Exception as e:
                    logging.warning(f"ìë§‰ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")

        # AI í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""ì˜ìƒ ì œëª©ê³¼ ë¶€ì œëª©ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

ì˜ìƒ íŒŒì¼ëª…: {video_name}
{'ìë§‰ ë‚´ìš©: ' + subtitle_text if subtitle_text else '(ìë§‰ ì—†ìŒ)'}

ìš”êµ¬ì‚¬í•­:
1. ì˜ìƒ íŒŒì¼ëª…ê³¼ ìë§‰ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ë§¤ë ¥ì ì´ê³  í´ë¦­ì„ ìœ ë„í•˜ëŠ” ì œëª©ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
2. **ì œëª© ì•ì— ê´€ë ¨ìˆëŠ” ì´ëª¨ì§€ ì•„ì´ì½˜ì„ ë°˜ë“œì‹œ ì¶”ê°€í•´ì£¼ì„¸ìš”** (ì˜ˆ: ğŸ”¥, ğŸ’°, ğŸ¯, âš¡, ğŸ’¡, ğŸš€, âœ¨ ë“±)
3. ë¶€ì œëª©ì€ ì œëª©ì„ ë³´ì™„í•˜ê³  ì¶”ê°€ ì •ë³´ë¥¼ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.
4. ì œëª©ì€ ì´ëª¨ì§€ í¬í•¨ í•œê¸€ë¡œ 35ì ì´ë‚´, ë¶€ì œëª©ì€ 20ì ì´ë‚´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
5. ì‘ë‹µì€ ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”:
{{"title": "ğŸ”¥ ì œëª©", "subtitle": "ë¶€ì œëª©"}}

ì¶”ê°€ ì„¤ëª…ì´ë‚˜ ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ì—†ì´ JSONë§Œ ë°˜í™˜í•´ì£¼ì„¸ìš”."""

        # ëª¨ë¸ ì„ íƒì— ë”°ë¼ API í˜¸ì¶œ
        import json
        import re

        if selected_model == "claude":
            # Anthropic API ì‚¬ìš© (Claude)
            try:
                import anthropic

                api_key = os.getenv("ANTHROPIC_API_KEY")
                if not api_key:
                    raise HTTPException(
                        status_code=400,
                        detail="ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. GPT ëª¨ë¸ì„ ì„ íƒí•˜ê±°ë‚˜ Anthropic API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”."
                    )

                client = anthropic.Anthropic(api_key=api_key)

                message = client.messages.create(
                    model="claude-3-5-sonnet-20241022",
                    max_tokens=1000,
                    messages=[
                        {"role": "user", "content": prompt}
                    ]
                )

                # ì‘ë‹µ íŒŒì‹±
                response_text = message.content[0].text.strip()

            except ImportError:
                raise HTTPException(
                    status_code=500,
                    detail="Anthropic ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install anthropicì„ ì‹¤í–‰í•˜ê±°ë‚˜ GPT ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”."
                )
            except HTTPException:
                raise
            except Exception as e:
                raise HTTPException(
                    status_code=500,
                    detail=f"Claude API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}"
                )

        elif selected_model == "gpt":
            # OpenAI API ì‚¬ìš© (GPT)
            try:
                import openai

                api_key = os.getenv("OPENAI_API_KEY")
                if not api_key:
                    raise HTTPException(
                        status_code=400,
                        detail="OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                    )

                client = openai.OpenAI(api_key=api_key)

                response = client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=1000
                )

                response_text = response.choices[0].message.content.strip()

            except ImportError:
                raise HTTPException(
                    status_code=500,
                    detail="OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install openaiì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”."
                )
            except HTTPException:
                raise
            except Exception as e:
                raise HTTPException(
                    status_code=500,
                    detail=f"GPT API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}"
                )
        else:
            raise HTTPException(status_code=400, detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì…ë‹ˆë‹¤: {selected_model}")

        # JSON ì¶”ì¶œ (markdown ì½”ë“œ ë¸”ë¡ì´ ìˆì„ ìˆ˜ ìˆìŒ)
        json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', response_text, re.DOTALL)
        if json_match:
            json_text = json_match.group(1)
        else:
            # ê·¸ëƒ¥ JSON ì°¾ê¸°
            json_match = re.search(r'\{.*?\}', response_text, re.DOTALL)
            if json_match:
                json_text = json_match.group(0)
            else:
                json_text = response_text

        result = json.loads(json_text)

        title = result.get("title", "")
        subtitle = result.get("subtitle", "")

        if not title:
            raise ValueError("ì œëª©ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        logging.info(f"AI ì œëª©/ë¶€ì œëª© ìƒì„± ì™„ë£Œ ({selected_model}): {title} / {subtitle}")

        return {
            "success": True,
            "title": title,
            "subtitle": subtitle,
            "model": selected_model
        }

    except HTTPException:
        raise
    except Exception as e:
        logging.exception("AI ì œëª©/ë¶€ì œëª© ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
        raise HTTPException(status_code=500, detail=f"AI ì œëª©/ë¶€ì œëª© ìƒì„± ì‹¤íŒ¨: {str(e)}")


@app.post("/api/ytdl/start")
async def api_start_download(payload: Dict[str, Any] = Body(...)) -> Dict[str, Any]:
    """ë°±ê·¸ë¼ìš´ë“œë¡œ ë‹¤ìš´ë¡œë“œ ì‹œì‘"""
    import asyncio
    import threading
    from uuid import uuid4 as generate_uuid

    task_id = str(generate_uuid())
    urls_str = payload.get("urls", "")
    output_dir = payload.get("output_dir", "")
    sub_langs = payload.get("sub_langs", "ko")
    sub_format = payload.get("sub_format", "srt/best")
    download_subs_enabled = payload.get("download_subs", True)
    auto_subs_enabled = payload.get("auto_subs", True)
    dry_run_enabled = payload.get("dry_run", False)
    extract_vocal = payload.get("extract_vocal", False)
    extract_instruments = payload.get("extract_instruments", False)

    parsed_urls = _split_urls(urls_str)

    if not parsed_urls:
        raise HTTPException(status_code=400, detail="ìµœì†Œ í•˜ë‚˜ì˜ ìœ íš¨í•œ URLì„ ì…ë ¥í•˜ì„¸ìš”.")

    # ì´ˆê¸° ìƒíƒœ ì„¤ì •
    download_tasks[task_id] = {
        "status": "starting",
        "progress": 0,
        "total": len(parsed_urls),
        "current": 0,
        "message": "ë‹¤ìš´ë¡œë“œ ì¤€ë¹„ ì¤‘...",
        "files": [],
        "error": None,
        "completed": False,
    }

    def progress_callback(d):
        """yt-dlp progress hook"""
        if d["status"] == "downloading":
            # ì§„í–‰ìœ¨ ê³„ì‚°
            downloaded = d.get("downloaded_bytes", 0)
            total = d.get("total_bytes") or d.get("total_bytes_estimate", 0)

            if total > 0:
                percent = (downloaded / total) * 100
                download_tasks[task_id]["progress"] = round(percent, 1)
                download_tasks[task_id]["message"] = f"ë‹¤ìš´ë¡œë“œ ì¤‘... {percent:.1f}%"
        elif d["status"] == "finished":
            download_tasks[task_id]["message"] = "ë‹¤ìš´ë¡œë“œ ì™„ë£Œ, ì²˜ë¦¬ ì¤‘..."

    def run_download():
        """ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ ë‹¤ìš´ë¡œë“œ ì‹¤í–‰"""
        try:
            download_tasks[task_id]["status"] = "downloading"
            download_tasks[task_id]["message"] = "ë‹¤ìš´ë¡œë“œ ì¤‘..."

            files = download_with_options(
                parsed_urls,
                output_dir or None,
                skip_download=dry_run_enabled,
                download_subs=download_subs_enabled,
                auto_subs=auto_subs_enabled,
                sub_langs=sub_langs,
                sub_format=sub_format,
                progress_hook=progress_callback,
            )

            # ì˜¤ë””ì˜¤ ë¶„ë¦¬ ì²˜ë¦¬ (dry_runì´ ì•„ë‹ ë•Œ)
            audio_files = []
            if not dry_run_enabled and (extract_vocal or extract_instruments):
                download_tasks[task_id]["message"] = "ì˜¤ë””ì˜¤ ë¶„ë¦¬ ì¤‘..."
                audio_files = extract_audio_sources(
                    files, extract_vocal, extract_instruments
                )
                files.extend(audio_files)

            # ë‹¤ìš´ë¡œë“œ ê¸°ë¡ì— ì¶”ê°€
            if not dry_run_enabled and files:
                form_values = {
                    "output_dir": output_dir,
                    "sub_langs": sub_langs,
                    "sub_format": sub_format,
                    "download_subs": download_subs_enabled,
                    "auto_subs": auto_subs_enabled,
                    "dry_run": dry_run_enabled,
                    "extract_vocal": extract_vocal,
                    "extract_instruments": extract_instruments,
                }
                add_to_download_history(parsed_urls, files, form_values)

            download_tasks[task_id].update({
                "status": "completed",
                "progress": 100,
                "message": "ë‹¤ìš´ë¡œë“œ ì™„ë£Œ",
                "files": [str(f) for f in files],
                "completed": True,
            })
        except Exception as e:
            logging.exception("ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
            download_tasks[task_id].update({
                "status": "error",
                "message": f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "error": str(e),
                "completed": True,
            })

    # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œë¡œ ë‹¤ìš´ë¡œë“œ ì‹œì‘
    thread = threading.Thread(target=run_download, daemon=True)
    thread.start()

    return {
        "task_id": task_id,
        "message": "ë‹¤ìš´ë¡œë“œê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
    }


@app.get("/api/ytdl/status/{task_id}")
async def api_get_download_status(task_id: str) -> Dict[str, Any]:
    """ë‹¤ìš´ë¡œë“œ ì§„í–‰ ìƒíƒœ ì¡°íšŒ"""
    if task_id not in download_tasks:
        raise HTTPException(status_code=404, detail="ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    return download_tasks[task_id]


@app.get("/api/ytdl/history")
async def api_get_download_history() -> List[Dict[str, Any]]:
    """Get download history."""
    return load_download_history()


@app.delete("/api/ytdl/files")
async def api_delete_files(file_paths: List[str] = Body(...)) -> Dict[str, Any]:
    """Delete downloaded files."""
    return delete_download_files(file_paths)


@app.post("/api/ytdl/extract-audio")
async def api_extract_audio(payload: Dict[str, Any] = Body(...)) -> Dict[str, Any]:
    """Extract vocal and/or instruments from already downloaded video files as background task."""
    import threading
    from uuid import uuid4 as generate_uuid

    file_paths_str = payload.get("file_paths", [])
    extract_vocal = payload.get("extract_vocal", False)
    extract_instruments = payload.get("extract_instruments", False)

    if not file_paths_str:
        raise HTTPException(status_code=400, detail="file_pathsê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    if not extract_vocal and not extract_instruments:
        raise HTTPException(status_code=400, detail="ìµœì†Œ í•˜ë‚˜ì˜ ì¶”ì¶œ ì˜µì…˜ì„ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.")

    # ë¬¸ìì—´ ê²½ë¡œë¥¼ Path ê°ì²´ë¡œ ë³€í™˜
    file_paths = [Path(fp) for fp in file_paths_str]

    # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    existing_files = [fp for fp in file_paths if fp.exists()]

    if not existing_files:
        raise HTTPException(status_code=404, detail="ìœ íš¨í•œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    # Task ID ìƒì„±
    task_id = str(generate_uuid())

    # ì´ˆê¸° ìƒíƒœ ì„¤ì •
    extraction_tasks[task_id] = {
        "status": "starting",
        "progress": 0,
        "total": len(existing_files),
        "current": 0,
        "message": "ì˜¤ë””ì˜¤ ì¶”ì¶œ ì¤€ë¹„ ì¤‘...",
        "extracted_files": [],
        "error": None,
        "completed": False,
    }

    def progress_callback(current: int, total: int, message: str):
        """Progress callback for extraction"""
        progress_percent = (current / total * 100) if total > 0 else 0
        extraction_tasks[task_id]["current"] = current
        extraction_tasks[task_id]["progress"] = progress_percent
        extraction_tasks[task_id]["message"] = message

    def run_extraction():
        """ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ ì˜¤ë””ì˜¤ ì¶”ì¶œ ì‹¤í–‰"""
        try:
            extraction_tasks[task_id]["status"] = "extracting"
            extraction_tasks[task_id]["message"] = "ì˜¤ë””ì˜¤ ì¶”ì¶œ ì¤‘..."

            extracted_files = extract_audio_sources(
                existing_files,
                extract_vocal,
                extract_instruments,
                progress_callback
            )

            extraction_tasks[task_id].update({
                "status": "completed",
                "progress": 100,
                "message": "ì˜¤ë””ì˜¤ ì¶”ì¶œ ì™„ë£Œ",
                "extracted_files": [str(f) for f in extracted_files],
                "completed": True,
            })
        except Exception as e:
            logging.exception("ì˜¤ë””ì˜¤ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
            extraction_tasks[task_id].update({
                "status": "error",
                "message": f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "error": str(e),
                "completed": True,
            })

    # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œë¡œ ì¶”ì¶œ ì‹œì‘
    thread = threading.Thread(target=run_extraction, daemon=True)
    thread.start()

    return {
        "task_id": task_id,
        "message": "ì˜¤ë””ì˜¤ ì¶”ì¶œì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
    }


@app.get("/api/ytdl/extract-audio/status/{task_id}")
async def api_get_extraction_status(task_id: str) -> Dict[str, Any]:
    """ì˜¤ë””ì˜¤ ì¶”ì¶œ ì§„í–‰ ìƒíƒœ ì¡°íšŒ"""
    if task_id not in extraction_tasks:
        raise HTTPException(status_code=404, detail="ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    return extraction_tasks[task_id]


@app.get("/api/ytdl/load-from-ytdl-server")
async def api_load_from_ytdl_server() -> Dict[str, Any]:
    """8001 í¬íŠ¸ì˜ ytdl ì„œë²„ì—ì„œ ë‹¤ìš´ë¡œë“œ ê¸°ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    import httpx

    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get("http://127.0.0.1:8001/api/ytdl/history")
            response.raise_for_status()
            history = response.json()

            # ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ë“¤ì„ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜
            files = []
            for record in history:
                if "files" in record:
                    for file_path in record["files"]:
                        path_obj = Path(file_path)
                        if path_obj.exists():
                            files.append({
                                "path": str(path_obj),
                                "name": path_obj.name,
                                "timestamp": record.get("timestamp", ""),
                                "size": path_obj.stat().st_size,
                            })

            return {
                "success": True,
                "files": files,
                "history": history,
            }
    except httpx.HTTPError as e:
        logger.error(f"8001 í¬íŠ¸ ì—°ê²° ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=503,
            detail=f"ë‹¤ìš´ë¡œë“œ ì„œë²„(8001)ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}"
        )
    except Exception as e:
        logger.error(f"ë‹¤ìš´ë¡œë“œ ê¸°ë¡ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ë‹¤ìš´ë¡œë“œ ê¸°ë¡ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}"
        )


@app.post("/api/ytdl/upload-file")
async def api_upload_file(file: UploadFile = File(...)) -> Dict[str, Any]:
    """
    ì‚¬ìš©ìê°€ ì„ íƒí•œ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ vocal/instruments ë¶„ë¦¬ì— ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
    ì—…ë¡œë“œëœ íŒŒì¼ì€ ë‹¤ìš´ë¡œë“œ ê¸°ë¡ì— ì¶”ê°€ë©ë‹ˆë‹¤.
    """
    import shutil
    from datetime import datetime

    # íŒŒì¼ í™•ì¥ì ê²€ì¦
    allowed_extensions = {".mp4", ".webm", ".mkv", ".avi", ".mov", ".wav", ".mp3", ".m4a", ".ogg", ".flac"}
    file_ext = Path(file.filename or "").suffix.lower()

    if file_ext not in allowed_extensions:
        raise HTTPException(
            status_code=400,
            detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. í—ˆìš©ëœ í˜•ì‹: {', '.join(allowed_extensions)}"
        )

    try:
        # ê³ ìœ í•œ íŒŒì¼ëª… ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ + ì›ë³¸ íŒŒì¼ëª…)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_filename = file.filename.replace(" ", "_")
        unique_filename = f"{timestamp}_{safe_filename}"
        file_path = YTDL_UPLOAD_DIR / unique_filename

        # íŒŒì¼ ì €ì¥
        with file_path.open("wb") as buffer:
            shutil.copyfileobj(file.file, buffer)

        logger.info(f"íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ: {file_path}")

        # ë‹¤ìš´ë¡œë“œ ê¸°ë¡ì— ì¶”ê°€
        history = load_download_history()
        upload_record = {
            "timestamp": datetime.now().isoformat(),
            "urls": ["Uploaded File"],
            "files": [str(file_path)],
            "settings": {
                "output_dir": str(YTDL_UPLOAD_DIR),
                "sub_langs": "",
                "download_subs": False,
                "auto_subs": False,
            }
        }
        history.insert(0, upload_record)
        history = history[:100]  # Keep only last 100 records
        save_download_history(history)

        return {
            "success": True,
            "file_path": str(file_path),
            "file_name": unique_filename,
            "message": "íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤."
        }

    except Exception as e:
        logger.error(f"íŒŒì¼ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"íŒŒì¼ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@app.get("/api/copyright/settings")
def api_get_copyright_settings() -> Dict[str, Any]:
    return load_copyright_settings()


@app.post("/api/copyright/settings")
def api_save_copyright_settings(payload: Dict[str, Any] = Body(...)) -> Dict[str, Any]:
    try:
        save_copyright_settings(payload)
    except OSError as exc:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì„¤ì •ì„ ì €ì¥í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {exc}",
        ) from exc
    return {"message": "ì €ì‘ê¶Œ ê²€ì‚¬ ê¸°ë³¸ ì„¤ì •ì„ ì €ì¥í–ˆìŠµë‹ˆë‹¤."}



@app.get("/copyright", response_class=HTMLResponse)
async def copyright_index(request: Request) -> HTMLResponse:
    context = {
        "request": request,
        "form_values": default_copyright_form(),
        "result_payload": None,
        "error": None,
        "default_report_dir": str(COPYRIGHT_REPORT_DIR),
        "nav_active": "copyright",
    }
    return templates.TemplateResponse("copyright.html", context)


@app.post("/copyright", response_class=HTMLResponse)
async def copyright_check(
    request: Request,
    video_file: UploadFile = File(...),
    reference_dir: Optional[str] = Form(None),
    fps: str = Form("1.0"),
    hash_alg: str = Form("phash"),
    resize: str = Form("256x256"),
    max_frames: Optional[str] = Form(""),
    hamming_th: str = Form("12"),
    high_th: str = Form("0.30"),
    med_th: str = Form("0.15"),
    audio_only: Optional[str] = Form(None),
    report_dir: Optional[str] = Form(""),
) -> HTMLResponse:
    form_values = default_copyright_form()
    result_payload: Optional[Dict[str, Any]] = None
    error: Optional[str] = None

    reference_dir_clean = (reference_dir or "").strip()
    fps_clean = (fps or "").strip() or form_values["fps"]
    hash_clean = (hash_alg or "").strip().lower() or form_values["hash"]
    resize_clean = (resize or "").strip() or form_values["resize"]
    max_frames_clean = (max_frames or "").strip()
    hamming_th_clean = (hamming_th or "").strip() or form_values["hamming_th"]
    high_th_clean = (high_th or "").strip() or form_values["high_th"]
    med_th_clean = (med_th or "").strip() or form_values["med_th"]
    report_dir_clean = (report_dir or "").strip()
    audio_only_flag = audio_only is not None

    form_values.update(
        {
            "reference_dir": reference_dir_clean,
            "fps": fps_clean,
            "hash": hash_clean,
            "resize": resize_clean,
            "max_frames": max_frames_clean,
            "hamming_th": hamming_th_clean,
            "high_th": high_th_clean,
            "med_th": med_th_clean,
            "audio_only": audio_only_flag,
            "report_dir": report_dir_clean,
        }
    )

    saved_path: Optional[Path] = None
    reference_dir_path: Optional[Path] = None
    report_dir_path = COPYRIGHT_REPORT_DIR

    try:
        if not video_file or not video_file.filename:
            error = "ê²€ì‚¬í•  ì˜ìƒì„ ì—…ë¡œë“œí•˜ì„¸ìš”."

        fps_value: Optional[float] = None
        if error is None:
            try:
                fps_value = float(fps_clean)
                if fps_value <= 0:
                    raise ValueError
            except ValueError:
                error = "FPSëŠ” 0ë³´ë‹¤ í° ìˆ«ìë¡œ ì…ë ¥í•˜ì„¸ìš”."

        resize_tuple: Optional[tuple[int, int]] = None
        if error is None:
            resize_tuple = parse_resize_form(resize_clean)
            if resize_tuple is None:
                error = "ë¦¬ì‚¬ì´ì¦ˆëŠ” 'ê°€ë¡œxì„¸ë¡œ' í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•˜ì„¸ìš”."

        max_frames_value: Optional[int] = None
        if error is None and max_frames_clean:
            try:
                max_frames_value = int(max_frames_clean)
                if max_frames_value <= 0:
                    raise ValueError
            except ValueError:
                error = "ìµœëŒ€ í”„ë ˆì„ ìˆ˜ëŠ” 1 ì´ìƒì˜ ì •ìˆ˜ë¡œ ì…ë ¥í•˜ì„¸ìš”."

        hamming_th_value: Optional[int] = None
        if error is None:
            try:
                hamming_th_value = int(hamming_th_clean)
                if hamming_th_value < 0:
                    raise ValueError
            except ValueError:
                error = "í•´ë° ì„ê³„ê°’ì€ 0 ì´ìƒì˜ ì •ìˆ˜ë¡œ ì…ë ¥í•˜ì„¸ìš”."

        high_th_value: Optional[float] = None
        if error is None:
            try:
                high_th_value = float(high_th_clean)
                if high_th_value < 0:
                    raise ValueError
            except ValueError:
                error = "HIGH ê¸°ì¤€ì€ 0 ì´ìƒì˜ ìˆ«ìë¡œ ì…ë ¥í•˜ì„¸ìš”."

        med_th_value: Optional[float] = None
        if error is None:
            try:
                med_th_value = float(med_th_clean)
                if med_th_value < 0:
                    raise ValueError
            except ValueError:
                error = "MED ê¸°ì¤€ì€ 0 ì´ìƒì˜ ìˆ«ìë¡œ ì…ë ¥í•˜ì„¸ìš”."

        if error is None:
            if hash_clean not in {"phash", "ahash", "dhash"}:
                error = "ì§€ì›í•˜ì§€ ì•ŠëŠ” í•´ì‹œ ë°©ì‹ì…ë‹ˆë‹¤."

        if error is None and reference_dir_clean:
            candidate = Path(reference_dir_clean).expanduser().resolve()
            if not candidate.exists() or not candidate.is_dir():
                error = f"ë¹„êµ ê¸°ì¤€ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {candidate}"
            else:
                reference_dir_path = candidate

        if error is None:
            if report_dir_clean:
                report_candidate = Path(report_dir_clean).expanduser()
            else:
                report_candidate = COPYRIGHT_REPORT_DIR
            try:
                ensure_directory(report_candidate)
                report_dir_path = report_candidate.resolve()
            except OSError as exc:  # pragma: no cover - filesystem error
                logger.exception("Failed to prepare copyright report directory: %s", exc)
                error = f"ë¦¬í¬íŠ¸ ì €ì¥ í´ë”ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {exc}"

        if error is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            original_suffix = Path(video_file.filename).suffix if video_file.filename else ""
            if not original_suffix:
                original_suffix = ".mp4"
            filename = f"{timestamp}_{uuid4().hex[:8]}{original_suffix}"
            saved_path = COPYRIGHT_UPLOAD_DIR / filename
            try:
                with saved_path.open("wb") as buffer:
                    while True:
                        chunk = await video_file.read(1024 * 1024)
                        if not chunk:
                            break
                        buffer.write(chunk)
            except Exception as exc:  # pragma: no cover - I/O error path
                logger.exception("Failed to save uploaded video for copyright check: %s", exc)
                error = f"ì—…ë¡œë“œ ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {exc}"
                if saved_path.exists():
                    try:
                        saved_path.unlink()
                    except OSError:
                        pass

        if error is None and saved_path is not None:
            try:
                result_payload = await run_in_threadpool(
                    run_copyright_precheck,
                    saved_path,
                    reference_dir_path,
                    fps=fps_value or 1.0,
                    hash_name=hash_clean,
                    hamming_th=hamming_th_value or int(form_values["hamming_th"]),
                    resize=resize_tuple,
                    max_frames=max_frames_value,
                    high_th=high_th_value or float(form_values["high_th"]),
                    med_th=med_th_value or float(form_values["med_th"]),
                    audio_only=audio_only_flag,
                    report_dir=report_dir_path,
                    show_progress=False,
                )
                if result_payload is not None:
                    result_payload["uploaded_filename"] = video_file.filename or saved_path.name
                    result_payload["saved_video_path"] = str(saved_path)
            except Exception as exc:  # pragma: no cover - runtime failure
                logger.exception("Copyright precheck failed: %s", exc)
                error = f"ì €ì‘ê¶Œ ê²€ì‚¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {exc}"
    finally:
        await video_file.close()

    context = {
        "request": request,
        "form_values": form_values,
        "result_payload": result_payload,
        "error": error,
        "default_report_dir": str(COPYRIGHT_REPORT_DIR),
        "nav_active": "copyright",
    }
    return templates.TemplateResponse("copyright.html", context)


@app.post("/ytdl", response_class=HTMLResponse)
async def ytdl_download(
    request: Request,
    urls: str = Form(""),
    output_dir: str = Form(""),
    sub_langs: str = Form("ko"),
    sub_format: str = Form("srt/best"),
    download_subs: Optional[str] = Form("on"),
    auto_subs: Optional[str] = Form("on"),
    dry_run: Optional[str] = Form(None),
    save_settings: Optional[str] = Form(None),
) -> HTMLResponse:
    download_subs_enabled = download_subs is not None
    auto_subs_enabled = auto_subs is not None
    dry_run_enabled = dry_run is not None

    form_values = {
        "urls": urls,
        "output_dir": output_dir,
        "sub_langs": sub_langs,
        "sub_format": sub_format,
        "download_subs": download_subs_enabled,
        "auto_subs": auto_subs_enabled,
        "dry_run": dry_run_enabled,
    }

    settings_saved = False
    if save_settings is not None:
        settings_payload = {key: value for key, value in form_values.items() if key != "urls"}
        try:
            save_ytdl_settings(settings_payload)
            settings_saved = True
        except Exception as exc:  # pylint: disable=broad-except
            logger.exception("Failed to save YTDL settings: %s", exc)

    parsed_urls = _split_urls(urls)
    error = None
    result: Optional[Dict[str, Any]] = None

    if parsed_urls:
        try:
            selected_langs = parse_sub_langs(sub_langs)
            files = await run_in_threadpool(
                download_with_options,
                parsed_urls,
                output_dir or None,
                skip_download=dry_run_enabled,
                download_subs=download_subs_enabled,
                auto_subs=auto_subs_enabled,
                sub_langs=sub_langs,
                sub_format=sub_format,
            )

            # Add to download history if not dry run
            if not dry_run_enabled and files:
                add_to_download_history(parsed_urls, files, form_values)

            result = {
                "files": [str(path) for path in files],
                "count": len(files),
                "langs": selected_langs,
                "dry_run": dry_run_enabled,
            }
        except Exception as exc:  # pylint: disable=broad-except
            logger.exception("YT download error: %s", exc)
            error = str(exc)
    elif not settings_saved:
        error = "ìµœì†Œ í•˜ë‚˜ì˜ ìœ íš¨í•œ URLì„ ì…ë ¥í•˜ì„¸ìš”."

    context = {
        "request": request,
        "form_values": form_values,
        "error": error,
        "result": result,
        "settings_saved": settings_saved,
        "nav_active": "ytdl",
    }
    return templates.TemplateResponse("ytdl.html", context)







@app.get("/api/similarity/settings")
def api_get_similarity_settings() -> Dict[str, Any]:
    return load_similarity_settings()


@app.post("/api/similarity/settings")
def api_save_similarity_settings(payload: Dict[str, Any] = Body(...)) -> Dict[str, Any]:
    try:
        save_similarity_settings(payload)
    except OSError as exc:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì„¤ì •ì„ ì €ì¥í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {exc}",
        ) from exc
    return {"message": "ì˜ìƒ ìœ ì‚¬ë„ ê¸°ë³¸ ì„¤ì •ì„ ì €ì¥í–ˆìŠµë‹ˆë‹¤."}



@app.get("/similarity", response_class=HTMLResponse)
async def similarity_index(request: Request) -> HTMLResponse:
    context = {
        "request": request,
        "form_values": default_similarity_form(),
        "result": None,
        "error": None,
        "nav_active": "similarity",
    }
    return templates.TemplateResponse("similarity.html", context)


@app.post("/similarity", response_class=HTMLResponse)
async def similarity_check(
    request: Request,
    video_a: str = Form(""),
    video_b: Optional[str] = Form(None),
    ref_dir: Optional[str] = Form(None),
    fps: str = Form("1.0"),
    resize: str = Form("320x320"),
    max_frames: Optional[str] = Form("200"),
    phash_th: str = Form("12"),
    weight_phash: str = Form("0.25"),
    weight_orb: str = Form("0.25"),
    weight_hist: str = Form("0.25"),
    weight_psnr: str = Form("0.25"),
    top_n: str = Form("5"),
) -> HTMLResponse:
    video_a_clean = video_a.strip()
    video_b_clean = (video_b or "").strip()
    ref_dir_clean = (ref_dir or "").strip()
    fps_clean = (fps or "").strip() or SIMILARITY_DEFAULTS["fps"]
    resize_clean = (resize or "").strip() or SIMILARITY_DEFAULTS["resize"]
    max_frames_clean = (max_frames or "").strip() or SIMILARITY_DEFAULTS["max_frames"]
    phash_th_clean = (phash_th or "").strip() or SIMILARITY_DEFAULTS["phash_th"]
    weight_phash_clean = (weight_phash or "").strip() or SIMILARITY_DEFAULTS["weight_phash"]
    weight_orb_clean = (weight_orb or "").strip() or SIMILARITY_DEFAULTS["weight_orb"]
    weight_hist_clean = (weight_hist or "").strip() or SIMILARITY_DEFAULTS["weight_hist"]
    weight_psnr_clean = (weight_psnr or "").strip() or SIMILARITY_DEFAULTS["weight_psnr"]
    top_n_clean = (top_n or "").strip() or SIMILARITY_DEFAULTS["top_n"]

    form_values = {
        "video_a": video_a_clean,
        "video_b": video_b_clean,
        "ref_dir": ref_dir_clean,
        "fps": fps_clean,
        "resize": resize_clean,
        "max_frames": max_frames_clean,
        "phash_th": phash_th_clean,
        "weight_phash": weight_phash_clean,
        "weight_orb": weight_orb_clean,
        "weight_hist": weight_hist_clean,
        "weight_psnr": weight_psnr_clean,
        "top_n": top_n_clean,
    }

    error: Optional[str] = None
    result: Optional[Dict[str, Any]] = None

    a_path: Optional[Path] = None
    b_path: Optional[Path] = None
    dir_path: Optional[Path] = None

    if not video_a_clean:
        error = "ê¸°ì¤€ ì˜ìƒ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”."
    else:
        candidate = Path(video_a_clean).expanduser().resolve()
        if not candidate.exists() or not candidate.is_file():
            error = f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {candidate}"
        else:
            a_path = candidate

    fps_value: Optional[float] = None
    if error is None:
        try:
            fps_value = float(fps_clean)
            if fps_value <= 0:
                raise ValueError
        except ValueError:
            error = "FPSëŠ” ìˆ«ìë¡œ ì…ë ¥í•˜ì„¸ìš”."

    resize_tuple: Optional[Tuple[int, int]] = None
    if error is None:
        resize_tuple = parse_resize_form(resize_clean)
        if resize_tuple is None:
            error = "ë¦¬ì‚¬ì´ì¦ˆëŠ” 'ê°€ë¡œxì„¸ë¡œ' í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•˜ì„¸ìš”."

    max_frames_value: Optional[int] = None
    if error is None and max_frames_clean:
        try:
            max_frames_value = int(max_frames_clean)
            if max_frames_value <= 0:
                raise ValueError
        except ValueError:
            error = "ìµœëŒ€ í”„ë ˆì„ ìˆ˜ëŠ” 1 ì´ìƒì˜ ì •ìˆ˜ë¡œ ì…ë ¥í•˜ì„¸ìš”."

    phash_th_value: Optional[int] = None
    if error is None:
        try:
            phash_th_value = int(phash_th_clean)
            if phash_th_value < 0:
                raise ValueError
        except ValueError:
            error = "pHash ì„ê³„ê°’ì€ 0 ì´ìƒì˜ ì •ìˆ˜ë¡œ ì…ë ¥í•˜ì„¸ìš”."

    weights_tuple: Optional[Tuple[float, float, float, float]] = None
    if error is None:
        try:
            weights_tuple = (
                float(weight_phash_clean),
                float(weight_orb_clean),
                float(weight_hist_clean),
                float(weight_psnr_clean),
            )
        except ValueError:
            error = "ê°€ì¤‘ì¹˜ëŠ” ìˆ«ìë¡œ ì…ë ¥í•˜ì„¸ìš”."

    top_n_value = 5
    if error is None:
        try:
            top_n_value = int(top_n_clean)
            if top_n_value <= 0:
                top_n_value = 5
                form_values["top_n"] = str(top_n_value)
        except ValueError:
            error = "ìƒìœ„ ì¶œë ¥ ê°œìˆ˜ëŠ” ì •ìˆ˜ë¡œ ì…ë ¥í•˜ì„¸ìš”."

    mode: Optional[str] = None
    if error is None and a_path is not None:
        if video_b_clean and ref_dir_clean:
            error = "ë¹„êµ ì˜ìƒê³¼ í´ë” ì¤‘ í•˜ë‚˜ë§Œ ì§€ì •í•˜ì„¸ìš”."
        elif video_b_clean:
            candidate = Path(video_b_clean).expanduser().resolve()
            if not candidate.exists() or not candidate.is_file():
                error = f"ë¹„êµ ì˜ìƒì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {candidate}"
            else:
                b_path = candidate
                mode = "pair"
        elif ref_dir_clean:
            candidate = Path(ref_dir_clean).expanduser().resolve()
            if not candidate.exists() or not candidate.is_dir():
                error = f"í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {candidate}"
            else:
                dir_path = candidate
                mode = "dir"
        else:
            error = "ë¹„êµí•  ì˜ìƒ ë˜ëŠ” í´ë”ë¥¼ ì…ë ¥í•˜ì„¸ìš”."

    if error is None and SIMILARITY_IMPORT_ERROR is not None:
        logger.error("Similarity comparison unavailable: %s", SIMILARITY_IMPORT_ERROR)
        error = "ì˜ìƒ ìœ ì‚¬ë„ ë¹„êµ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ OpenCV (cv2) íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤."

    if error is None and mode == "pair" and a_path and b_path:
        try:
            scores = await run_in_threadpool(
                compare_two,
                a_path,
                b_path,
                fps_value or 1.0,
                resize_tuple,
                max_frames_value,
                phash_th_value or 12,
                weights_tuple or (0.25, 0.25, 0.25, 0.25),
            )
            result = {
                "mode": "pair",
                "a_path": str(a_path),
                "b_path": str(b_path),
                "scores": scores,
            }
        except Exception as exc:  # pylint: disable=broad-except
            logger.exception("Video similarity check failed (pair): %s", exc)
            error = str(exc)

    if error is None and mode == "dir" and a_path and dir_path:
        try:
            results = await run_in_threadpool(
                compare_dir,
                a_path,
                dir_path,
                fps_value or 1.0,
                resize_tuple,
                max_frames_value,
                phash_th_value or 12,
                weights_tuple or (0.25, 0.25, 0.25, 0.25),
            )
            entries: List[Dict[str, Any]] = []
            for rank, (name, data) in enumerate(results[:top_n_value], start=1):
                if isinstance(data, dict) and "error" in data:
                    entries.append({
                        "rank": rank,
                        "name": name,
                        "error": data.get("error"),
                    })
                else:
                    entries.append({
                        "rank": rank,
                        "name": name,
                        "scores": data,
                    })
            result = {
                "mode": "dir",
                "a_path": str(a_path),
                "dir_path": str(dir_path),
                "entries": entries,
                "total": len(results),
                "top_n": top_n_value,
            }
        except Exception as exc:  # pylint: disable=broad-except
            logger.exception("Video similarity check failed (directory): %s", exc)
            error = str(exc)

    context = {
        "request": request,
        "form_values": form_values,
        "result": result,
        "error": error,
        "nav_active": "similarity",
    }
    return templates.TemplateResponse("similarity.html", context)


@app.get("/api/dashboard/projects", response_model=List[DashboardProject])
async def api_dashboard_projects(query: Optional[str] = None) -> List[DashboardProject]:
    shorts_summaries = list_projects(OUTPUT_DIR)
    all_project_data = aggregate_dashboard_projects(shorts_summaries)

    try:
        all_projects = [DashboardProject(**p) for p in all_project_data]
    except Exception as e:
        print(f"Error validating dashboard projects: {e}")
        print(f"Data: {all_project_data}")
        all_projects = []


    if query:
        q = query.strip().lower()
        if q:
            all_projects = [
                project
                for project in all_projects
                if q in project.id.lower()
                or q in project.title.lower()
                or (project.topic and q in project.topic.lower())
                or (project.language and q in project.language.lower())
            ]

    all_projects.sort(key=lambda p: p.updated_at or "", reverse=True)
    return all_projects


@app.get("/", response_class=HTMLResponse)
async def dashboard(request: Request):
    shorts_summaries = list_projects(OUTPUT_DIR)
    all_projects = aggregate_dashboard_projects(shorts_summaries)
    all_projects.sort(key=lambda p: p.get("updated_at"), reverse=True)

    context = {
        "request": request,
        "projects": all_projects,
    }
    return templates.TemplateResponse("dashboard.html", context)


@app.get("/translator", response_class=HTMLResponse)
async def translator_page(request: Request):
    context = {"request": request}
    return templates.TemplateResponse("translator.html", context)


@app.get("/test-simple", response_class=HTMLResponse)
async def test_simple_page(request: Request):
    context = {"request": request}
    return templates.TemplateResponse("test_simple.html", context)


@app.get("/analysis")
async def analysis_redirect():
    """ë¶„ì„ í˜ì´ì§€ë¡œ ë¦¬ë””ë ‰ì…˜"""
    from fastapi.responses import RedirectResponse
    return RedirectResponse(url="http://127.0.0.1:8002/", status_code=302)


@app.get("/favicon.ico")
async def favicon():
    """Favicon ìš”ì²­ ì²˜ë¦¬"""
    from fastapi.responses import Response
    return Response(status_code=204)


@app.get("/shorts", response_class=HTMLResponse)
async def index(request: Request):
    selected_base = request.query_params.get("existing")
    project_summaries = list_projects(OUTPUT_DIR)
    version_history: List[ProjectVersionInfo] = []
    version_history_json: List[Dict[str, Any]] = []
    result = None
    error = None

    if selected_base:
        try:
            project = load_project(selected_base, OUTPUT_DIR)
            result = build_result_payload(project)
            version_history = list_versions(selected_base)
            version_history_json = [item.model_dump(exclude_none=True) for item in version_history]
        except FileNotFoundError:
            error = f"'{selected_base}' í”„ë¡œì íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    context = {
        "request": request,
        "form_values": default_form_values(),
        "result": result,
        "error": error,
        "project_summaries": project_summaries,
        "selected_project": selected_base,
        "version_history_json": version_history_json,
        "lang_options": LANG_OPTIONS,
    }
    return templates.TemplateResponse("index.html", context)


@app.post("/generate", response_class=HTMLResponse)
async def generate(
    request: Request,
    topic: str = Form(...),
    style: str = Form("ì •ë³´/ìš”ì•½"),
    duration: int = Form(30),
    lang: str = Form("ko"),
    voice: str = Form("alloy"),
    fps: int = Form(24),
    music: Optional[str] = Form(None),
    music_volume: float = Form(0.12),
    ducking: float = Form(0.35),
    burn_subs: Optional[str] = Form(None),
    dry_run: Optional[str] = Form(None),
    save_json: Optional[str] = Form(None),
    output_name: Optional[str] = Form(None),
    script_model: str = Form("gpt-4o-mini"),
    tts_model: str = Form("gpt-4o-mini-tts"),
):
    lang = sanitize_lang(lang)

    form_values = {
        "topic": topic,
        "style": style,
        "duration": duration,
        "lang": lang,
        "voice": voice,
        "fps": fps,
        "music": music,
        "music_volume": music_volume,
        "ducking": ducking,
        "burn_subs": burn_subs,
        "dry_run": dry_run,
        "save_json": save_json,
        "output_name": output_name,
        "script_model": script_model,
        "tts_model": tts_model,
    }

    version_history: List[ProjectVersionInfo] = []
    version_history_json: List[Dict[str, Any]] = []
    error = None

    try:
        options = GenerationOptions(
            topic=topic,
            style=style,
            duration=duration,
            lang=lang,
            fps=fps,
            voice=voice,
            music=music is not None,
            music_volume=music_volume,
            ducking=ducking,
            burn_subs=burn_subs is not None,
            dry_run=dry_run is not None,
            save_json=save_json is not None,
            script_model=script_model,
            tts_model=tts_model,
            output_name=output_name or None,
        )

        generation_result = await run_in_threadpool(generate_short, options)
        base_name = generation_result.get("base_name")
        if base_name:
            project = load_project(base_name, OUTPUT_DIR)
            result = build_result_payload(project)
            version_history = list_versions(base_name)
            version_history_json = [item.model_dump(exclude_none=True) for item in version_history]
        else:
            result = build_result_payload_dict(generation_result)
            error = None
    except Exception as exc:  # pylint: disable=broad-except
        logger.exception("Generation error: %s", exc)
        result = None
        error = str(exc)

    selected_project = None
    if result and isinstance(result.get("metadata"), dict):
        selected_project = result["metadata"].get("base_name")

    context = {
        "request": request,
        "form_values": form_values,
        "result": result,
        "error": error,
        "project_summaries": list_projects(OUTPUT_DIR),
        "selected_project": selected_project,
        "version_history_json": version_history_json,
        "lang_options": LANG_OPTIONS,
    }
    return templates.TemplateResponse("index.html", context)


def default_form_values() -> Dict[str, Any]:
    return {
        "topic": "ë¬´ì„œìš´ ì°",
        "style": "ê³µí¬/ë¯¸ìŠ¤í„°ë¦¬",
        "duration": 30,
        "lang": sanitize_lang("ko"),
        "voice": "alloy",
        "fps": 24,
        "music": "on",
        "music_volume": 0.12,
        "ducking": 0.35,
        "burn_subs": None,
        "dry_run": None,
        "save_json": None,
        "output_name": "",
        "script_model": "gpt-4o-mini",
        "tts_model": "gpt-4o-mini-tts",
    }


def build_result_payload(project: ProjectMetadata) -> Dict[str, Any]:
    metadata = project.model_dump(exclude_none=False)

    def relative_output(path_str: Optional[str]) -> Optional[str]:
        if not path_str:
            return None
        path = Path(path_str)
        try:
            return f"/outputs/{path.name}"
        except ValueError:
            return None

    metadata_json = json.dumps(metadata, ensure_ascii=False, indent=2, default=str)

    return {
        "metadata": metadata,
        "metadata_json": metadata_json,
        "video_url": relative_output(project.video_path),
        "audio_url": relative_output(project.audio_path),
        "srt_url": relative_output(project.subtitles_path),
        "json_url": relative_output(str(metadata_path(project.base_name, OUTPUT_DIR))),
    }


def build_result_payload_dict(metadata: Dict[str, Any]) -> Dict[str, Any]:
    def relative_output(path_str: Optional[str]) -> Optional[str]:
        if not path_str:
            return None
        path = Path(path_str)
        try:
            return f"/outputs/{path.name}"
        except ValueError:
            return None

    metadata_json = json.dumps(metadata, ensure_ascii=False, indent=2, default=str)
    return {
        "metadata": metadata,
        "metadata_json": metadata_json,
        "video_url": relative_output(metadata.get("video_path")),
        "audio_url": relative_output(metadata.get("audio_path")),
        "srt_url": relative_output(metadata.get("subtitles_path")),
        "json_url": relative_output(metadata.get("metadata_path")),
    }
