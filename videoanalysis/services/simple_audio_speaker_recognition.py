"""
ê°„ë‹¨í•œ ìŒì„±-ìë§‰ í•˜ì´ë¸Œë¦¬ë“œ í™”ì ì¸ì‹ ì„œë¹„ìŠ¤
scikit-learn ì—†ì´ ê¸°ë³¸ì ì¸ ì˜¤ë””ì˜¤ íŠ¹ì„±ìœ¼ë¡œ í™”ì êµ¬ë¶„
"""
import logging
import numpy as np
import librosa
from typing import List, Dict, Any, Tuple
from pathlib import Path
import os

logger = logging.getLogger(__name__)


class SimpleAudioSpeakerRecognition:
    """ê°„ë‹¨í•œ ìŒì„± ê¸°ë°˜ í™”ì ì¸ì‹ í´ë˜ìŠ¤"""

    def __init__(self):
        self.sample_rate = 16000  # 16kHzë¡œ í†µì¼
        self.window_size = 1.0    # 1ì´ˆ ìœˆë„ìš°
        self.hop_size = 0.5       # 0.5ì´ˆ ê²¹ì¹¨

    def extract_simple_features(self, audio_path: str) -> Dict[str, np.ndarray]:
        """ê°„ë‹¨í•œ ì˜¤ë””ì˜¤ íŠ¹ì„± ì¶”ì¶œ (scikit-learn ì—†ì´)"""
        try:
            logger.info(f"ğŸµ ê°„ë‹¨í•œ ì˜¤ë””ì˜¤ íŠ¹ì„± ì¶”ì¶œ ì‹œì‘: {audio_path}")

            # librosaë¡œ ì˜¤ë””ì˜¤ ë¡œë“œ
            y, sr = librosa.load(audio_path, sr=self.sample_rate)

            # ìœˆë„ìš° ë‹¨ìœ„ë¡œ íŠ¹ì„± ì¶”ì¶œ
            window_samples = int(self.window_size * sr)
            hop_samples = int(self.hop_size * sr)

            features = {
                'pitch': [],          # í”¼ì¹˜ (ìŒë†’ì´)
                'energy': [],         # ì—ë„ˆì§€ (ìŒëŸ‰)
                'spectral_centroid': [], # ìŠ¤í™íŠ¸ëŸ¼ ì¤‘ì‹¬ (ìŒìƒ‰)
                'zero_crossing_rate': [], # ì˜ì  êµì°¨ìœ¨ (ìŒì„± íŠ¹ì„±)
                'timestamps': []      # ì‹œê°„ ì •ë³´
            }

            # ìœˆë„ìš°ë³„ íŠ¹ì„± ì¶”ì¶œ
            for i in range(0, len(y) - window_samples, hop_samples):
                window = y[i:i + window_samples]
                timestamp = i / sr

                # í”¼ì¹˜ ì¶”ì¶œ (ìŒë†’ì´)
                pitches, magnitudes = librosa.piptrack(y=window, sr=sr)
                pitch = np.mean(pitches[pitches > 0]) if len(pitches[pitches > 0]) > 0 else 0
                features['pitch'].append(pitch)

                # ì—ë„ˆì§€ ê³„ì‚° (ìŒëŸ‰)
                energy = np.sum(window ** 2) / len(window)
                features['energy'].append(energy)

                # ìŠ¤í™íŠ¸ëŸ¼ ì¤‘ì‹¬ (ìŒìƒ‰)
                spectral_centroid = librosa.feature.spectral_centroid(y=window, sr=sr)
                features['spectral_centroid'].append(np.mean(spectral_centroid))

                # ì˜ì  êµì°¨ìœ¨ (ìŒì„± íŠ¹ì„±)
                zcr = librosa.feature.zero_crossing_rate(window)
                features['zero_crossing_rate'].append(np.mean(zcr))

                features['timestamps'].append(timestamp)

            # ë°°ì—´ë¡œ ë³€í™˜
            for key in features:
                if key != 'timestamps':
                    features[key] = np.array(features[key])

            logger.info(f"ğŸµ ê°„ë‹¨í•œ íŠ¹ì„± ì¶”ì¶œ ì™„ë£Œ: {len(features['timestamps'])}ê°œ ìœˆë„ìš°")
            return features

        except Exception as e:
            logger.error(f"âŒ ê°„ë‹¨í•œ íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {}

    def simple_speaker_clustering(self, features: Dict[str, np.ndarray], n_speakers: int = 2) -> np.ndarray:
        """ê°„ë‹¨í•œ í™”ì í´ëŸ¬ìŠ¤í„°ë§ (k-means ëŒ€ì‹  ì„ê³„ê°’ ê¸°ë°˜)"""
        try:
            logger.info(f"ğŸ­ ê°„ë‹¨í•œ í™”ì í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘: {n_speakers}ëª… í™”ì")

            if len(features['timestamps']) == 0:
                return np.array([])

            # ì£¼ìš” íŠ¹ì„±ë“¤ì„ ì •ê·œí™”
            pitch = features['pitch']
            energy = features['energy']
            spectral_centroid = features['spectral_centroid']

            # ì •ê·œí™” (0-1 ë²”ìœ„ë¡œ)
            def normalize(arr):
                arr_min, arr_max = np.min(arr), np.max(arr)
                if arr_max - arr_min > 0:
                    return (arr - arr_min) / (arr_max - arr_min)
                return np.zeros_like(arr)

            norm_pitch = normalize(pitch)
            norm_energy = normalize(energy)
            norm_spectral = normalize(spectral_centroid)

            # ê°„ë‹¨í•œ 2ëª… í™”ì êµ¬ë¶„ (í”¼ì¹˜ ê¸°ë°˜)
            if n_speakers == 2:
                # í”¼ì¹˜ì˜ ì¤‘ê°„ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ êµ¬ë¶„
                pitch_median = np.median(norm_pitch[norm_pitch > 0])
                speaker_labels = np.where(norm_pitch > pitch_median, 1, 0)

            # 3ëª… ì´ìƒì˜ í™”ì êµ¬ë¶„ (í”¼ì¹˜ + ì—ë„ˆì§€ ì¡°í•©)
            else:
                speaker_labels = np.zeros(len(features['timestamps']), dtype=int)

                # í”¼ì¹˜ì™€ ì—ë„ˆì§€ ì¡°í•©ìœ¼ë¡œ êµ¬ë¶„
                for i in range(len(features['timestamps'])):
                    # ë†’ì€ í”¼ì¹˜ + ë†’ì€ ì—ë„ˆì§€ = í™”ì 0
                    if norm_pitch[i] > 0.7 and norm_energy[i] > 0.6:
                        speaker_labels[i] = 0
                    # ë‚®ì€ í”¼ì¹˜ + ë†’ì€ ì—ë„ˆì§€ = í™”ì 1
                    elif norm_pitch[i] < 0.3 and norm_energy[i] > 0.4:
                        speaker_labels[i] = 1
                    # ì¤‘ê°„ í”¼ì¹˜ = í™”ì 2
                    else:
                        speaker_labels[i] = 2 if n_speakers > 2 else 0

            unique_speakers = len(np.unique(speaker_labels))
            logger.info(f"ğŸ­ ê°„ë‹¨í•œ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {unique_speakers}ëª… í™”ì ê°ì§€")

            return speaker_labels

        except Exception as e:
            logger.error(f"âŒ ê°„ë‹¨í•œ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {e}")
            return np.zeros(len(features['timestamps']))

    def map_speakers_to_subtitles(self, speaker_labels: np.ndarray,
                                features: Dict[str, np.ndarray],
                                subtitles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """í™”ì ë¼ë²¨ì„ ìë§‰ì— ë§¤í•‘"""
        try:
            logger.info(f"ğŸ­ í™”ì-ìë§‰ ë§¤í•‘ ì‹œì‘")

            timestamps = features['timestamps']

            # ê° ìë§‰ì— ëŒ€í•´ í•´ë‹¹ ì‹œê°„ëŒ€ì˜ í™”ì ê²°ì •
            for subtitle in subtitles:
                start_time = subtitle.get('start_time', 0)
                end_time = subtitle.get('end_time', 0)

                # ìë§‰ ì‹œê°„ëŒ€ì™€ ê²¹ì¹˜ëŠ” ì˜¤ë””ì˜¤ ìœˆë„ìš°ë“¤ ì°¾ê¸°
                overlapping_windows = []
                for i, ts in enumerate(timestamps):
                    window_end = ts + self.window_size

                    # ê²¹ì¹˜ëŠ” êµ¬ê°„ì´ ìˆëŠ”ì§€ í™•ì¸
                    if not (end_time <= ts or start_time >= window_end):
                        overlapping_windows.append(i)

                if overlapping_windows:
                    # ê°€ì¥ ë§ì´ ë‚˜íƒ€ë‚˜ëŠ” í™”ìë¥¼ ì„ íƒ
                    speaker_votes = [speaker_labels[i] for i in overlapping_windows]
                    speaker_id = max(set(speaker_votes), key=speaker_votes.count)
                    speaker_name = f"í™”ì{speaker_id + 1}"

                    if speaker_name == 'í™”ì4':
                        subtitle['original_speaker_name'] = 'í™”ì4'
                        subtitle['speaker_id'] = -1
                        subtitle['speaker_name'] = 'ë¯¸ë¶„ë¥˜'
                    else:
                        subtitle['speaker_id'] = int(speaker_id)
                        subtitle['speaker_name'] = speaker_name
                else:
                    # ê²¹ì¹˜ëŠ” ìœˆë„ìš°ê°€ ì—†ìœ¼ë©´ ê°€ì¥ ê°€ê¹Œìš´ ìœˆë„ìš°ì˜ í™”ì ì‚¬ìš©
                    if len(timestamps) > 0:
                        closest_window = np.argmin([abs(ts - start_time) for ts in timestamps])
                        closest_speaker_id = int(speaker_labels[closest_window])
                        speaker_name = f"í™”ì{closest_speaker_id + 1}"

                        if speaker_name == 'í™”ì4':
                            subtitle['original_speaker_name'] = 'í™”ì4'
                            subtitle['speaker_id'] = -1
                            subtitle['speaker_name'] = 'ë¯¸ë¶„ë¥˜'
                        else:
                            subtitle['speaker_id'] = closest_speaker_id
                            subtitle['speaker_name'] = speaker_name
                    else:
                        subtitle['speaker_id'] = 0
                        subtitle['speaker_name'] = "í™”ì1"

            # í™”ìë³„ í†µê³„
            speaker_stats = {}
            for speaker_id in np.unique(speaker_labels):
                speaker_subtitles = [s for s in subtitles if s.get('speaker_id') == speaker_id]
                speaker_name = f"í™”ì{speaker_id + 1}"

                if speaker_name == 'í™”ì4':
                    speaker_stats[speaker_name] = {
                        "subtitle_count": 0,
                        "total_duration": 0,
                        "sample_texts": []
                    }
                    continue

                speaker_stats[speaker_name] = {
                    "subtitle_count": len(speaker_subtitles),
                    "total_duration": sum(s.get('duration', 0) for s in speaker_subtitles),
                    "sample_texts": [s.get('text', '')[:50] for s in speaker_subtitles[:3]]
                }

            logger.info(f"ğŸ­ í™”ì-ìë§‰ ë§¤í•‘ ì™„ë£Œ: {len(speaker_stats)}ëª… í™”ì")
            for speaker, stats in speaker_stats.items():
                logger.info(f"  {speaker}: {stats['subtitle_count']}ê°œ ìë§‰")

            return subtitles

        except Exception as e:
            logger.error(f"âŒ í™”ì-ìë§‰ ë§¤í•‘ ì‹¤íŒ¨: {e}")
            return subtitles

    def recognize_speakers_from_audio(self, audio_path: str,
                                    subtitles: List[Dict[str, Any]] = None,
                                    n_speakers: int = None) -> Dict[str, Any]:
        """ê°„ë‹¨í•œ ì˜¤ë””ì˜¤ ê¸°ë°˜ í™”ì ì¸ì‹ ìˆ˜í–‰"""
        try:
            logger.info(f"ğŸµ ê°„ë‹¨í•œ ìŒì„± ê¸°ë°˜ í™”ì ì¸ì‹ ì‹œì‘: {audio_path}")

            # 1. ê°„ë‹¨í•œ ì˜¤ë””ì˜¤ íŠ¹ì„± ì¶”ì¶œ
            features = self.extract_simple_features(audio_path)
            if not features:
                raise Exception("ì˜¤ë””ì˜¤ íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨")

            # 2. ê°„ë‹¨í•œ í™”ì í´ëŸ¬ìŠ¤í„°ë§ (ê¸°ë³¸ê°’: 2ëª…)
            if n_speakers is None:
                n_speakers = 2

            speaker_labels = self.simple_speaker_clustering(features, n_speakers)

            # 3. ìë§‰ì´ ìˆìœ¼ë©´ ë§¤í•‘
            if subtitles:
                subtitles = self.map_speakers_to_subtitles(speaker_labels, features, subtitles)

            # 4. ê²°ê³¼ ë°˜í™˜
            unique_speakers = np.unique(speaker_labels)
            speakers = {}

            for speaker_id in unique_speakers:
                speaker_windows = speaker_labels == speaker_id
                speaker_features = {
                    'avg_pitch': float(np.mean(features['pitch'][speaker_windows])),
                    'avg_energy': float(np.mean(features['energy'][speaker_windows])),
                    'avg_spectral_centroid': float(np.mean(features['spectral_centroid'][speaker_windows])),
                    'window_count': int(np.sum(speaker_windows)),
                    'total_duration': float(np.sum(speaker_windows) * self.hop_size)
                }

                speaker_name = f"í™”ì{speaker_id + 1}"

                if speaker_name == 'í™”ì4':
                    speaker_features['window_count'] = 0
                    speaker_features['total_duration'] = 0
                    speaker_features['avg_pitch'] = 0
                    speaker_features['avg_energy'] = 0
                    speaker_features['avg_spectral_centroid'] = 0

                if subtitles:
                    speaker_subtitles = [s for s in subtitles if s.get('speaker_id') == speaker_id]
                    speaker_features.update({
                        'subtitle_count': len(speaker_subtitles),
                        'subtitle_indices': [i for i, s in enumerate(subtitles) if s.get('speaker_id') == speaker_id],
                        'sample_texts': [s.get('text', '')[:50] + '...' for s in speaker_subtitles[:3]]
                    })

                    if speaker_name == 'í™”ì4':
                        speaker_features['subtitle_count'] = 0
                        speaker_features['subtitle_indices'] = []
                        speaker_features['sample_texts'] = []

                speakers[speaker_name] = speaker_features

            result = {
                "speakers": speakers,
                "total_speakers": len(unique_speakers),
                "analysis_method": "simple_audio_based",
                "audio_duration": float(len(features['timestamps']) * self.hop_size),
                "features_extracted": len(features['timestamps'])
            }

            if subtitles:
                result["classified_subtitles"] = subtitles

            logger.info(f"ğŸµ ê°„ë‹¨í•œ ìŒì„± ê¸°ë°˜ í™”ì ì¸ì‹ ì™„ë£Œ: {len(speakers)}ëª… í™”ì")
            return result

        except Exception as e:
            logger.error(f"âŒ ê°„ë‹¨í•œ ìŒì„± ê¸°ë°˜ í™”ì ì¸ì‹ ì‹¤íŒ¨: {e}")
            return {"speakers": {}, "total_speakers": 0, "error": str(e)}
