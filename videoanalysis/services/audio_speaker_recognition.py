"""
ìŒì„± ê¸°ë°˜ í™”ì ì¸ì‹ ë° ë¶„ë¦¬ ì„œë¹„ìŠ¤
"""
import logging
import numpy as np
import librosa
from typing import List, Dict, Any, Tuple
from pathlib import Path
import subprocess
import tempfile
import os

logger = logging.getLogger(__name__)


class AudioSpeakerRecognition:
    """ìŒì„± ê¸°ë°˜ í™”ì ì¸ì‹ ë° ë¶„ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self):
        self.sample_rate = 16000  # 16kHzë¡œ í†µì¼
        self.window_size = 2.0    # 2ì´ˆ ìœˆë„ìš°
        self.hop_size = 0.5       # 0.5ì´ˆ ê²¹ì¹¨

    def extract_audio_features(self, audio_path: str) -> Dict[str, np.ndarray]:
        """ì˜¤ë””ì˜¤ì—ì„œ í™”ì êµ¬ë¶„ìš© íŠ¹ì„± ì¶”ì¶œ"""
        try:
            # librosaë¡œ ì˜¤ë””ì˜¤ ë¡œë“œ
            y, sr = librosa.load(audio_path, sr=self.sample_rate)

            # ìœˆë„ìš° ë‹¨ìœ„ë¡œ íŠ¹ì„± ì¶”ì¶œ
            window_samples = int(self.window_size * sr)
            hop_samples = int(self.hop_size * sr)

            features = {
                'mfcc': [],           # Mel-frequency cepstral coefficients
                'chroma': [],         # í¬ë¡œë§ˆ íŠ¹ì„±
                'spectral_centroid': [],  # ìŠ¤í™íŠ¸ëŸ¼ ì¤‘ì‹¬
                'spectral_rolloff': [],   # ìŠ¤í™íŠ¸ëŸ¼ ë¡¤ì˜¤í”„
                'zero_crossing_rate': [], # ì˜ì  êµì°¨ìœ¨
                'tempo': [],          # í…œí¬
                'pitch': [],          # í”¼ì¹˜
                'energy': [],         # ì—ë„ˆì§€
                'timestamps': []      # ì‹œê°„ ì •ë³´
            }

            # ìœˆë„ìš°ë³„ íŠ¹ì„± ì¶”ì¶œ
            for i in range(0, len(y) - window_samples, hop_samples):
                window = y[i:i + window_samples]
                timestamp = i / sr

                # MFCC (í™”ì íŠ¹ì„±ì— ì¤‘ìš”)
                mfcc = librosa.feature.mfcc(y=window, sr=sr, n_mfcc=13)
                features['mfcc'].append(np.mean(mfcc, axis=1))

                # í¬ë¡œë§ˆ íŠ¹ì„±
                chroma = librosa.feature.chroma_stft(y=window, sr=sr)
                features['chroma'].append(np.mean(chroma, axis=1))

                # ìŠ¤í™íŠ¸ëŸ¼ íŠ¹ì„±
                spectral_centroid = librosa.feature.spectral_centroid(y=window, sr=sr)
                features['spectral_centroid'].append(np.mean(spectral_centroid))

                spectral_rolloff = librosa.feature.spectral_rolloff(y=window, sr=sr)
                features['spectral_rolloff'].append(np.mean(spectral_rolloff))

                # ì˜ì  êµì°¨ìœ¨
                zcr = librosa.feature.zero_crossing_rate(window)
                features['zero_crossing_rate'].append(np.mean(zcr))

                # í”¼ì¹˜ ì¶”ì¶œ
                pitches, magnitudes = librosa.piptrack(y=window, sr=sr)
                pitch = np.mean(pitches[pitches > 0]) if len(pitches[pitches > 0]) > 0 else 0
                features['pitch'].append(pitch)

                # ì—ë„ˆì§€ ê³„ì‚°
                energy = np.sum(window ** 2) / len(window)
                features['energy'].append(energy)

                features['timestamps'].append(timestamp)

            # ë°°ì—´ë¡œ ë³€í™˜
            for key in features:
                if key != 'timestamps':
                    features[key] = np.array(features[key])

            logger.info(f"ğŸµ ì˜¤ë””ì˜¤ íŠ¹ì„± ì¶”ì¶œ ì™„ë£Œ: {len(features['timestamps'])}ê°œ ìœˆë„ìš°")
            return features

        except Exception as e:
            logger.error(f"âŒ ì˜¤ë””ì˜¤ íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {}

    def cluster_speakers(self, features: Dict[str, np.ndarray], n_speakers: int = None) -> np.ndarray:
        """íŠ¹ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ í™”ì í´ëŸ¬ìŠ¤í„°ë§"""
        try:
            # scikit-learn ì„í¬íŠ¸ì™€ threadpoolctl ê²½ê³  ë¬´ì‹œ
            import warnings
            warnings.filterwarnings('ignore', category=UserWarning)

            from sklearn.cluster import KMeans
            from sklearn.preprocessing import StandardScaler
            from sklearn.decomposition import PCA

            # íŠ¹ì„± ë²¡í„° êµ¬ì„±
            feature_vectors = []

            for i in range(len(features['timestamps'])):
                vector = np.concatenate([
                    features['mfcc'][i],
                    features['chroma'][i],
                    [features['spectral_centroid'][i]],
                    [features['spectral_rolloff'][i]],
                    [features['zero_crossing_rate'][i]],
                    [features['pitch'][i]],
                    [features['energy'][i]]
                ])
                feature_vectors.append(vector)

            feature_vectors = np.array(feature_vectors)

            # ì •ê·œí™”
            scaler = StandardScaler()
            normalized_features = scaler.fit_transform(feature_vectors)

            # ì°¨ì› ì¶•ì†Œ (ì„ íƒì )
            if normalized_features.shape[1] > 50:
                pca = PCA(n_components=50)
                normalized_features = pca.fit_transform(normalized_features)

            # í™”ì ìˆ˜ ìë™ ê²°ì • (ì—†ìœ¼ë©´ 2-5 ë²”ìœ„ì—ì„œ ìµœì ê°’ ì°¾ê¸°)
            if n_speakers is None:
                n_speakers = self.find_optimal_speakers(normalized_features)

            # K-means í´ëŸ¬ìŠ¤í„°ë§
            kmeans = KMeans(n_clusters=n_speakers, random_state=42, n_init=10)
            speaker_labels = kmeans.fit_predict(normalized_features)

            logger.info(f"ğŸ­ í™”ì í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {n_speakers}ëª… í™”ì")
            return speaker_labels

        except ImportError:
            logger.error("âŒ scikit-learnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install scikit-learn")
            return np.zeros(len(features['timestamps']))
        except Exception as e:
            logger.error(f"âŒ í™”ì í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {e}")
            return np.zeros(len(features['timestamps']))

    def find_optimal_speakers(self, features: np.ndarray, max_speakers: int = 5) -> int:
        """ìµœì  í™”ì ìˆ˜ ìë™ ê²°ì • (Elbow Method)"""
        try:
            import warnings
            warnings.filterwarnings('ignore', category=UserWarning)

            from sklearn.cluster import KMeans

            inertias = []
            k_range = range(2, min(max_speakers + 1, len(features) // 10 + 1))

            for k in k_range:
                kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
                kmeans.fit(features)
                inertias.append(kmeans.inertia_)

            # Elbow í¬ì¸íŠ¸ ì°¾ê¸° (ê°„ë‹¨í•œ ë°©ë²•)
            if len(inertias) < 2:
                return 2

            # ê¸°ìš¸ê¸° ë³€í™”ê°€ ê°€ì¥ í° ì§€ì 
            diffs = np.diff(inertias)
            optimal_k = k_range[np.argmax(diffs)] if len(diffs) > 0 else 2

            logger.info(f"ğŸ¯ ìµœì  í™”ì ìˆ˜: {optimal_k}")
            return optimal_k

        except Exception as e:
            logger.error(f"âŒ ìµœì  í™”ì ìˆ˜ ê²°ì • ì‹¤íŒ¨: {e}")
            return 2

    def map_speakers_to_subtitles(self, speaker_labels: np.ndarray,
                                features: Dict[str, np.ndarray],
                                subtitles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """í™”ì ë¼ë²¨ì„ ìë§‰ì— ë§¤í•‘"""
        try:
            timestamps = features['timestamps']

            # ê° ìë§‰ì— ëŒ€í•´ í•´ë‹¹ ì‹œê°„ëŒ€ì˜ í™”ì ê²°ì •
            for subtitle in subtitles:
                start_time = subtitle.get('start_time', 0)
                end_time = subtitle.get('end_time', 0)

                # ìë§‰ ì‹œê°„ëŒ€ì™€ ê²¹ì¹˜ëŠ” ì˜¤ë””ì˜¤ ìœˆë„ìš°ë“¤ ì°¾ê¸°
                overlapping_windows = []
                for i, ts in enumerate(timestamps):
                    window_end = ts + self.window_size

                    # ê²¹ì¹˜ëŠ” êµ¬ê°„ì´ ìˆëŠ”ì§€ í™•ì¸
                    if not (end_time <= ts or start_time >= window_end):
                        overlapping_windows.append(i)

                if overlapping_windows:
                    # ê°€ì¥ ë§ì´ ë‚˜íƒ€ë‚˜ëŠ” í™”ìë¥¼ ì„ íƒ
                    speaker_votes = [speaker_labels[i] for i in overlapping_windows]
                    speaker_id = max(set(speaker_votes), key=speaker_votes.count)
                    speaker_name = f"í™”ì{speaker_id + 1}"

                    if speaker_name == 'í™”ì4':
                        subtitle['original_speaker_name'] = 'í™”ì4'
                        subtitle['speaker_id'] = -1
                        subtitle['speaker_name'] = 'ë¯¸ë¶„ë¥˜'
                    else:
                        subtitle['speaker_id'] = int(speaker_id)
                        subtitle['speaker_name'] = speaker_name
                else:
                    # ê²¹ì¹˜ëŠ” ìœˆë„ìš°ê°€ ì—†ìœ¼ë©´ ê°€ì¥ ê°€ê¹Œìš´ ìœˆë„ìš°ì˜ í™”ì ì‚¬ìš©
                    closest_window = np.argmin([abs(ts - start_time) for ts in timestamps])
                    closest_speaker_id = int(speaker_labels[closest_window])
                    speaker_name = f"í™”ì{closest_speaker_id + 1}"

                    if speaker_name == 'í™”ì4':
                        subtitle['original_speaker_name'] = 'í™”ì4'
                        subtitle['speaker_id'] = -1
                        subtitle['speaker_name'] = 'ë¯¸ë¶„ë¥˜'
                    else:
                        subtitle['speaker_id'] = closest_speaker_id
                        subtitle['speaker_name'] = speaker_name

            # í™”ìë³„ í†µê³„
            speaker_stats = {}
            for speaker_id in np.unique(speaker_labels):
                speaker_subtitles = [s for s in subtitles if s.get('speaker_id') == speaker_id]
                speaker_name = f"í™”ì{speaker_id + 1}"

                if speaker_name == 'í™”ì4':
                    speaker_stats[speaker_name] = {
                        "subtitle_count": 0,
                        "total_duration": 0,
                        "sample_texts": []
                    }
                    continue

                speaker_stats[speaker_name] = {
                    "subtitle_count": len(speaker_subtitles),
                    "total_duration": sum(s.get('duration', 0) for s in speaker_subtitles),
                    "sample_texts": [s.get('text', '')[:50] for s in speaker_subtitles[:3]]
                }

            logger.info(f"ğŸ­ í™”ì-ìë§‰ ë§¤í•‘ ì™„ë£Œ: {len(speaker_stats)}ëª… í™”ì")
            for speaker, stats in speaker_stats.items():
                logger.info(f"  {speaker}: {stats['subtitle_count']}ê°œ ìë§‰")

            return subtitles

        except Exception as e:
            logger.error(f"âŒ í™”ì-ìë§‰ ë§¤í•‘ ì‹¤íŒ¨: {e}")
            return subtitles

    def recognize_speakers_from_audio(self, audio_path: str,
                                    subtitles: List[Dict[str, Any]] = None,
                                    n_speakers: int = None) -> Dict[str, Any]:
        """ì˜¤ë””ì˜¤ì—ì„œ í™”ì ì¸ì‹ ìˆ˜í–‰"""
        try:
            logger.info(f"ğŸµ ìŒì„± ê¸°ë°˜ í™”ì ì¸ì‹ ì‹œì‘: {audio_path}")

            # 1. ì˜¤ë””ì˜¤ íŠ¹ì„± ì¶”ì¶œ
            features = self.extract_audio_features(audio_path)
            if not features:
                raise Exception("ì˜¤ë””ì˜¤ íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨")

            # 2. í™”ì í´ëŸ¬ìŠ¤í„°ë§
            speaker_labels = self.cluster_speakers(features, n_speakers)

            # 3. ìë§‰ì´ ìˆìœ¼ë©´ ë§¤í•‘
            if subtitles:
                subtitles = self.map_speakers_to_subtitles(speaker_labels, features, subtitles)

            # 4. ê²°ê³¼ ë°˜í™˜
            unique_speakers = np.unique(speaker_labels)
            speakers = {}

            for speaker_id in unique_speakers:
                speaker_windows = speaker_labels == speaker_id
                speaker_features = {
                    'avg_pitch': np.mean(features['pitch'][speaker_windows]),
                    'avg_energy': np.mean(features['energy'][speaker_windows]),
                    'avg_spectral_centroid': np.mean(features['spectral_centroid'][speaker_windows]),
                    'window_count': np.sum(speaker_windows),
                    'total_duration': np.sum(speaker_windows) * self.hop_size
                }

                speaker_name = f"í™”ì{speaker_id + 1}"

                if speaker_name == 'í™”ì4':
                    speaker_features['window_count'] = 0
                    speaker_features['total_duration'] = 0
                    speaker_features['avg_pitch'] = 0
                    speaker_features['avg_energy'] = 0
                    speaker_features['avg_spectral_centroid'] = 0

                if subtitles:
                    speaker_subtitles = [s for s in subtitles if s.get('speaker_id') == speaker_id]
                    speaker_features.update({
                        'subtitle_count': len(speaker_subtitles),
                        'subtitle_indices': [i for i, s in enumerate(subtitles) if s.get('speaker_id') == speaker_id],
                        'sample_texts': [s.get('text', '')[:50] + '...' for s in speaker_subtitles[:3]]
                    })

                    if speaker_name == 'í™”ì4':
                        speaker_features['subtitle_count'] = 0
                        speaker_features['subtitle_indices'] = []
                        speaker_features['sample_texts'] = []

                speakers[speaker_name] = speaker_features

            result = {
                "speakers": speakers,
                "total_speakers": len(unique_speakers),
                "analysis_method": "audio_based",
                "audio_duration": len(features['timestamps']) * self.hop_size,
                "features_extracted": len(features['timestamps'])
            }

            if subtitles:
                result["classified_subtitles"] = subtitles

            logger.info(f"ğŸµ ìŒì„± ê¸°ë°˜ í™”ì ì¸ì‹ ì™„ë£Œ: {len(speakers)}ëª… í™”ì")
            return result

        except Exception as e:
            logger.error(f"âŒ ìŒì„± ê¸°ë°˜ í™”ì ì¸ì‹ ì‹¤íŒ¨: {e}")
            return {"speakers": {}, "total_speakers": 0, "error": str(e)}

    def extract_speaker_segments(self, audio_path: str, speaker_labels: np.ndarray,
                                features: Dict[str, np.ndarray], output_dir: str = None) -> Dict[str, str]:
        """í™”ìë³„ë¡œ ì˜¤ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ ë¶„ë¦¬í•˜ì—¬ ì €ì¥"""
        try:
            if output_dir is None:
                output_dir = os.path.dirname(audio_path)

            y, sr = librosa.load(audio_path, sr=self.sample_rate)
            timestamps = features['timestamps']
            hop_samples = int(self.hop_size * sr)

            speaker_files = {}

            for speaker_id in np.unique(speaker_labels):
                # í•´ë‹¹ í™”ìì˜ ì„¸ê·¸ë¨¼íŠ¸ë“¤ ì¶”ì¶œ
                speaker_segments = []

                for i, label in enumerate(speaker_labels):
                    if label == speaker_id:
                        start_sample = i * hop_samples
                        end_sample = min(start_sample + int(self.window_size * sr), len(y))
                        speaker_segments.append(y[start_sample:end_sample])

                if speaker_segments:
                    # ì„¸ê·¸ë¨¼íŠ¸ë“¤ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
                    combined_audio = np.concatenate(speaker_segments)

                    # íŒŒì¼ ì €ì¥
                    base_name = Path(audio_path).stem
                    speaker_filename = f"{base_name}_speaker{speaker_id + 1}.wav"
                    speaker_path = os.path.join(output_dir, speaker_filename)

                    # librosaë¡œ ì €ì¥
                    import soundfile as sf
                    sf.write(speaker_path, combined_audio, sr)

                    speaker_files[f"í™”ì{speaker_id + 1}"] = speaker_path
                    logger.info(f"ğŸµ í™”ì{speaker_id + 1} ì˜¤ë””ì˜¤ ì €ì¥: {speaker_path}")

            return speaker_files

        except Exception as e:
            logger.error(f"âŒ í™”ìë³„ ì˜¤ë””ì˜¤ ë¶„ë¦¬ ì‹¤íŒ¨: {e}")
            return {}
