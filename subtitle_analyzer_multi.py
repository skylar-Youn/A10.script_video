#!/usr/bin/env python3
"""
ë©€í‹° ëª¨ë¸ ìë§‰ ë¶„ì„ ë„êµ¬
OpenAI GPTì™€ Anthropic Claude ëª¨ë¸ì„ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤.
"""

import json
import os
from pathlib import Path
from typing import Dict, Any, Optional
import openai

try:
    from anthropic import Anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False
    Anthropic = None


class MultiModelSubtitleAnalyzer:
    """ì—¬ëŸ¬ AI ëª¨ë¸ì„ ì§€ì›í•˜ëŠ” ìë§‰ ë¶„ì„ í´ë˜ìŠ¤"""

    # ì§€ì› ëª¨ë¸ ëª©ë¡
    SUPPORTED_MODELS = {
        # OpenAI ëª¨ë¸
        "gpt-4o": {"provider": "openai", "cost_per_1k_input": 0.0025, "cost_per_1k_output": 0.010},
        "gpt-4o-mini": {"provider": "openai", "cost_per_1k_input": 0.00015, "cost_per_1k_output": 0.0006},

        # Anthropic Claude ëª¨ë¸
        "claude-3-5-sonnet-20241022": {"provider": "anthropic", "cost_per_1k_input": 0.003, "cost_per_1k_output": 0.015},
        "claude-3-haiku-20240307": {"provider": "anthropic", "cost_per_1k_input": 0.00025, "cost_per_1k_output": 0.00125},
        "claude-3-opus-20240229": {"provider": "anthropic", "cost_per_1k_input": 0.015, "cost_per_1k_output": 0.075},
    }

    # ë³„ì¹­ ì§€ì›
    MODEL_ALIASES = {
        "gpt4o": "gpt-4o",
        "gpt4o-mini": "gpt-4o-mini",
        "sonnet": "claude-3-5-sonnet-20241022",
        "claude-sonnet": "claude-3-5-sonnet-20241022",
        "haiku": "claude-3-haiku-20240307",
        "claude-haiku": "claude-3-haiku-20240307",
        "opus": "claude-3-opus-20240229",
        "claude-opus": "claude-3-opus-20240229",
    }

    def __init__(self, openai_api_key: str = None, anthropic_api_key: str = None):
        """
        ì´ˆê¸°í™”

        Args:
            openai_api_key: OpenAI API í‚¤
            anthropic_api_key: Anthropic API í‚¤
        """
        self.openai_api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        self.anthropic_api_key = anthropic_api_key or os.getenv("ANTHROPIC_API_KEY")

        if self.openai_api_key:
            openai.api_key = self.openai_api_key

        if self.anthropic_api_key and ANTHROPIC_AVAILABLE:
            self.anthropic_client = Anthropic(api_key=self.anthropic_api_key)
        else:
            self.anthropic_client = None

    def resolve_model_name(self, model: str) -> str:
        """ëª¨ë¸ ë³„ì¹­ì„ ì‹¤ì œ ëª¨ë¸ëª…ìœ¼ë¡œ ë³€í™˜"""
        return self.MODEL_ALIASES.get(model.lower(), model)

    def get_model_info(self, model: str) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        model = self.resolve_model_name(model)
        if model not in self.SUPPORTED_MODELS:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model}\nì§€ì› ëª¨ë¸: {list(self.SUPPORTED_MODELS.keys())}")
        return self.SUPPORTED_MODELS[model]

    def read_srt(self, srt_path: str) -> str:
        """SRT íŒŒì¼ ì½ê¸°"""
        with open(srt_path, "r", encoding="utf-8") as f:
            return f.read()

    def get_system_prompt(self, prompt_type: str = "shorts") -> str:
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë°˜í™˜"""
        prompts = {
            "shorts": """ë‹¹ì‹ ì€ ìœ íŠœë¸Œ ì‡¼ì¸  ì „ë¬¸ í¸ì§‘ìì…ë‹ˆë‹¤.
ìë§‰ì„ ë¶„ì„í•˜ì—¬ 60ì´ˆ ì´ë‚´ ì‡¼ì¸  ì˜ìƒì— ìµœì í™”í•˜ì„¸ìš”.
ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”.""",

            "basic": """ë‹¹ì‹ ì€ ìœ íŠœë¸Œ ì˜ìƒ ìë§‰ í¸ì§‘ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ìë§‰ì„ ë¶„ì„í•˜ì—¬ ì‚­ì œí•  ë¶€ë¶„ê³¼ ì¶”ê°€í•  ì„¤ëª…ì„ ì œì•ˆí•˜ì„¸ìš”.
ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”.""",

            "detailed": """ë‹¹ì‹ ì€ ìë§‰ í¸ì§‘ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì˜ìƒì˜ êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ê³  ìƒì„¸í•œ í¸ì§‘ ì œì•ˆì„ í•˜ì„¸ìš”.
ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”.""",

            "enhanced": """ë‹¹ì‹ ì€ ìœ íŠœë¸Œ ì˜ìƒ ìë§‰ ë° ë‚˜ë ˆì´ì…˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ìë§‰ì„ 3ê°€ì§€ íƒ€ì…ìœ¼ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”:
1. kept_originals: ì›ë³¸ ëŒ€ì‚¬ ìœ ì§€
2. text_additions: í™”ë©´ í…ìŠ¤íŠ¸ë§Œ (ì†Œë¦¬ ì—†ìŒ)
3. narration_additions: AI ìŒì„± ë‚˜ë ˆì´ì…˜ (ì†Œë¦¬ ìˆìŒ)
ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”.""",

            "enhanced-shorts": """ë‹¹ì‹ ì€ ìœ íŠœë¸Œ ì‡¼ì¸  ì „ë¬¸ í¸ì§‘ìì…ë‹ˆë‹¤.
ì´ ì˜ìƒì„ 60ì´ˆ ì´ë‚´ ì‡¼ì¸ ë¡œ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤.
ê³¼ê°í•œ ì‚­ì œ(60-70%), ë¹ ë¥¸ ì „í™˜, ê°•ë ¬í•œ í›„í‚¹ì— ì§‘ì¤‘í•˜ì„¸ìš”.
ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”.""",

            "enhanced-summary": """ë‹¹ì‹ ì€ ì˜ìƒ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ê¸´ ì˜ìƒì„ í•µì‹¬ë§Œ ì¶”ë ¤ì„œ 3-5ë¶„ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.
êµ¬ì¡°í™”, ì±•í„° êµ¬ë¶„, íë¦„ ì—°ê²°ì— ì§‘ì¤‘í•˜ì„¸ìš”.
ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”.""",

            "enhanced-education": """ë‹¹ì‹ ì€ êµìœ¡ ì½˜í…ì¸  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
í•™ìŠµ íš¨ê³¼ë¥¼ ê·¹ëŒ€í™”í•˜ëŠ” êµìœ¡ ì˜ìƒì„ ë§Œë“œì„¸ìš”.
ëª…í™•í•œ ê°œë… ì„¤ëª…, ë‹¨ê³„ë³„ í•™ìŠµ, ë°˜ë³µ ë³µìŠµì— ì§‘ì¤‘í•˜ì„¸ìš”.
ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”."""
        }
        return prompts.get(prompt_type, prompts["basic"])

    def get_user_prompt(self, subtitle_content: str, prompt_type: str = "shorts") -> str:
        """ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        if prompt_type == "shorts":
            template = """
ì•„ë˜ ìë§‰ì„ ë¶„ì„í•˜ì—¬ 60ì´ˆ ì‡¼ì¸ ì— ìµœì í™”í•˜ì„¸ìš”.

**í¸ì§‘ ì›ì¹™:**
1. â±ï¸ ì‹œê°„ ì ˆì•½: ë¶ˆí•„ìš”í•œ ë‚´ìš© ê³¼ê°íˆ ì‚­ì œ
2. ğŸ“Œ í•µì‹¬ ê°•ì¡°: ì¤‘ìš”í•œ ë©”ì‹œì§€ë§Œ ë‚¨ê¸°ê¸°
3. ğŸ¯ ì‹œì²­ì ëª°ì…: ì„¤ëª… ìë§‰ìœ¼ë¡œ ë§¥ë½ ì œê³µ

**ì¶œë ¥ í˜•ì‹ (JSON):**
{{
  "title_suggestion": "ì¶”ì²œ ì œëª©",
  "theme": "ì˜ìƒ ì£¼ì œ",
  "edit_summary": {{
    "original_count": ì›ë³¸ê°œìˆ˜,
    "delete_count": ì‚­ì œê°œìˆ˜,
    "add_count": ì¶”ê°€ê°œìˆ˜,
    "final_count": ìµœì¢…ê°œìˆ˜
  }},
  "deletions": [
    {{"index": ë²ˆí˜¸, "time": "ì‹œê°„", "text": "í…ìŠ¤íŠ¸", "reason": "ì´ìœ "}}
  ],
  "additions": [
    {{"after": ë²ˆí˜¸, "time": "ì‹œê°„", "text": "í…ìŠ¤íŠ¸", "type": "ì„¤ëª…|ê°•ì¡°|ì „í™˜"}}
  ],
  "highlights": [
    {{"index": ë²ˆí˜¸, "text": "ë‚´ìš©", "hook": "ì™œ ì£¼ëª©í• ë§Œí•œê°€"}}
  ]
}}

**ìë§‰:**
{subtitle_content}
"""
        elif prompt_type == "enhanced":
            template = """
ì•„ë˜ ìë§‰ì„ 3ê°€ì§€ íƒ€ì…ìœ¼ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”:

**ë¶„ë¥˜ ê¸°ì¤€:**
1. **kept_originals**: ì›ë³¸ ëŒ€ì‚¬ ìœ ì§€ (ì˜ìƒ ì›ë³¸ ìŒì„±)
2. **text_additions**: í™”ë©´ í…ìŠ¤íŠ¸ë§Œ (ì†Œë¦¬ ì—†ìŒ, ì˜ˆ: [í™”ë©´ ì „í™˜], ğŸ’¡ íŒ)
3. **narration_additions**: AI ìŒì„± ë‚˜ë ˆì´ì…˜ (TTSë¡œ ì½ì„ ë‚´ìš©)
4. **deletions**: ì‚­ì œí•  ìë§‰ (ì¶”ì„ìƒˆ, ì¤‘ë³µ, ì¡ë‹´)

**ì¶œë ¥ í˜•ì‹ (JSON):**
{{
  "video_info": {{
    "title_suggestion": "ì¶”ì²œ ì œëª©",
    "theme": "ì˜ìƒ ì£¼ì œ",
    "original_count": ì›ë³¸ê°œìˆ˜,
    "final_count": ìµœì¢…ê°œìˆ˜
  }},
  "kept_originals": [
    {{
      "index": ë²ˆí˜¸,
      "time": "ì‹œì‘ --> ì¢…ë£Œ",
      "text": "ì›ë³¸ ëŒ€ì‚¬",
      "reason": "ìœ ì§€ ì´ìœ ",
      "importance": "high|medium|low"
    }}
  ],
  "deletions": [
    {{
      "index": ë²ˆí˜¸,
      "time": "ì‹œì‘ --> ì¢…ë£Œ",
      "text": "ì‚­ì œí•  í…ìŠ¤íŠ¸",
      "reason": "ì‚­ì œ ì´ìœ ",
      "category": "ì¶”ì„ìƒˆ|ì¤‘ë³µ|ë¬´ì˜ë¯¸|ì¡ë‹´"
    }}
  ],
  "text_additions": [
    {{
      "insert_after": ì‚½ì…ìœ„ì¹˜,
      "estimated_time": "ì˜ˆìƒì‹œê°„",
      "text": "[í™”ë©´ í…ìŠ¤íŠ¸]",
      "type": "í™”ë©´ì „í™˜|ê°•ì¡°|ê°€ì´ë“œ",
      "position": "top|center|bottom",
      "duration": í‘œì‹œì‹œê°„(ì´ˆ)
    }}
  ],
  "narration_additions": [
    {{
      "insert_after": ì‚½ì…ìœ„ì¹˜,
      "estimated_time": "ì˜ˆìƒì‹œê°„",
      "narration_text": "AI ìŒì„±ìœ¼ë¡œ ì½ì„ í…ìŠ¤íŠ¸",
      "subtitle_text": "ìŒì„±ê³¼ í•¨ê»˜ í‘œì‹œí•  ìë§‰",
      "purpose": "ë§¥ë½ì„¤ëª…|ìš”ì•½|ì „í™˜",
      "tone": "ì¹œê·¼|ì „ë¬¸ì |ì—´ì •ì "
    }}
  ],
  "editing_summary": {{
    "kept_count": ìœ ì§€ê°œìˆ˜,
    "deleted_count": ì‚­ì œê°œìˆ˜,
    "text_added_count": í…ìŠ¤íŠ¸ìë§‰ê°œìˆ˜,
    "narration_added_count": ë‚˜ë ˆì´ì…˜ê°œìˆ˜
  }}
}}

**ìë§‰:**
{subtitle_content}
"""
        elif prompt_type == "enhanced-shorts":
            template = """
**ëª©í‘œ:** 60ì´ˆ ì‡¼ì¸ ë¡œ ë§Œë“¤ê¸°

**ì „ëµ:** ê³¼ê°í•œ ì‚­ì œ(60-70%), ë¹ ë¥¸ ì „í™˜, ê°•ë ¬í•œ í›„í‚¹

**ì¶œë ¥ (JSON):**
{{
  "video_type": "shorts",
  "target_duration": 60,
  "kept_originals": [{{"index": ë²ˆí˜¸, "text": "ëŒ€ì‚¬", "importance": "high", "impact": "í›„í‚¹|í•µì‹¬|ë°˜ì „"}}],
  "deletions": [{{"index": ë²ˆí˜¸, "reason": "ì´ìœ ", "category": "ì¸ì‚¬|ë°˜ë³µ|ì¶”ì„ìƒˆ"}}],
  "text_additions": [{{"insert_after": ë²ˆí˜¸, "text": "í…ìŠ¤íŠ¸(ì´ëª¨ì§€í¬í•¨)", "type": "í›„í‚¹|ê°•ì¡°|ìˆ«ì", "animation": "fade|bounce|zoom"}}],
  "narration_additions": [{{"insert_after": ë²ˆí˜¸, "narration_text": "ë‚˜ë ˆì´ì…˜", "purpose": "ì˜¤í”„ë‹|ì „í™˜|í´ë¡œì§•", "tone": "ì—´ì •ì ", "speed": "fast"}}],
  "shorts_optimization": {{"opening_hook": "ì²« 3ì´ˆ ì „ëµ", "key_moments": [{{"time": "ì‹œê°„", "moment": "ìˆœê°„"}}]}}
}}

**ìë§‰:**
{subtitle_content}
"""
        elif prompt_type == "enhanced-summary":
            template = """
**ëª©í‘œ:** ê¸´ ì˜ìƒì„ 3-5ë¶„ìœ¼ë¡œ ìš”ì•½

**ì „ëµ:** êµ¬ì¡°í™”, ì±•í„° êµ¬ë¶„, í•µì‹¬ ìœ ì§€

**ì¶œë ¥ (JSON):**
{{
  "video_type": "summary",
  "structure": [{{"chapter": "ì±•í„°ëª…", "summary": "ìš”ì•½", "key_points": ["í•µì‹¬1", "í•µì‹¬2"]}}],
  "kept_originals": [{{"index": ë²ˆí˜¸, "text": "ëŒ€ì‚¬", "chapter": "ì±•í„°", "key_point": "í•µì‹¬í¬ì¸íŠ¸"}}],
  "deletions": [{{"index": ë²ˆí˜¸, "reason": "ì´ìœ ", "category": "ì¡ë‹´|ë°˜ë³µ|ì˜ˆì‹œì¤‘ë³µ"}}],
  "text_additions": [{{"insert_after": ë²ˆí˜¸, "text": "í…ìŠ¤íŠ¸", "type": "ì±•í„°ì œëª©|ìš”ì•½|í•µì‹¬"}}],
  "narration_additions": [{{"insert_after": ë²ˆí˜¸, "narration_text": "ë‚˜ë ˆì´ì…˜", "purpose": "ì±•í„°ì „í™˜|ì„¹ì…˜ìš”ì•½", "tone": "ì „ë¬¸ì "}}]
}}

**ìë§‰:**
{subtitle_content}
"""
        elif prompt_type == "enhanced-education":
            template = """
**ëª©í‘œ:** í•™ìŠµ íš¨ê³¼ ê·¹ëŒ€í™”

**ì „ëµ:** ëª…í™•í•œ ê°œë… ì„¤ëª…, ë‹¨ê³„ë³„ í•™ìŠµ, ë°˜ë³µ ë³µìŠµ

**ì¶œë ¥ (JSON):**
{{
  "video_type": "education",
  "learning_structure": [{{"step": ë²ˆí˜¸, "title": "ë‹¨ê³„", "concepts": ["ê°œë…1", "ê°œë…2"]}}],
  "kept_originals": [{{"index": ë²ˆí˜¸, "text": "ëŒ€ì‚¬", "category": "ê°œë…|ì˜ˆì‹œ|ì„¤ëª…", "key_concept": "í•µì‹¬ê°œë…"}}],
  "deletions": [{{"index": ë²ˆí˜¸, "reason": "ì´ìœ ", "category": "ë§ë”ë“¬|ë°˜ë³µ(ë¶ˆí•„ìš”)"}}],
  "text_additions": [{{"insert_after": ë²ˆí˜¸, "text": "í…ìŠ¤íŠ¸", "type": "ê°œë…ì •ì˜|ê³µì‹|Step|ìš”ì•½", "importance": "critical|important"}}],
  "narration_additions": [{{"insert_after": ë²ˆí˜¸, "narration_text": "ë‚˜ë ˆì´ì…˜", "purpose": "ê°œë…ì„¤ëª…|ì˜ˆì‹œì¶”ê°€|ë³µìŠµ", "tone": "ì°¨ë¶„", "speed": "slow"}}],
  "educational_elements": {{
    "key_concepts": [{{"concept": "ê°œë…", "definition": "ì •ì˜"}}],
    "formulas": [{{"formula": "ê³µì‹", "explanation": "ì„¤ëª…"}}],
    "review_checkpoints": [{{"checkpoint": "ë³µìŠµí¬ì¸íŠ¸"}}]
  }}
}}

**ìë§‰:**
{subtitle_content}
"""
        else:
            template = """
ì•„ë˜ ìë§‰ì„ ë¶„ì„í•˜ê³  í¸ì§‘ ì œì•ˆì„ í•´ì£¼ì„¸ìš”.

**ì¶œë ¥ í˜•ì‹ (JSON):**
{{
  "summary": "ì˜ìƒ ìš”ì•½",
  "deletions": [{{"index": ë²ˆí˜¸, "reason": "ì´ìœ "}}],
  "additions": [{{"after": ë²ˆí˜¸, "text": "í…ìŠ¤íŠ¸"}}],
  "key_moments": [{{"index": ë²ˆí˜¸, "text": "ë‚´ìš©"}}]
}}

**ìë§‰:**
{subtitle_content}
"""

        return template.format(subtitle_content=subtitle_content)

    def analyze_with_openai(
        self,
        subtitle_content: str,
        model: str,
        prompt_type: str,
        temperature: float
    ) -> Dict[str, Any]:
        """OpenAI ëª¨ë¸ë¡œ ë¶„ì„"""
        if not self.openai_api_key:
            raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        system_msg = self.get_system_prompt(prompt_type)
        user_msg = self.get_user_prompt(subtitle_content, prompt_type)

        response = openai.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_msg}
            ],
            temperature=temperature,
            response_format={"type": "json_object"}
        )

        result = json.loads(response.choices[0].message.content)
        usage = response.usage

        return {
            "result": result,
            "usage": {
                "prompt_tokens": usage.prompt_tokens,
                "completion_tokens": usage.completion_tokens,
                "total_tokens": usage.total_tokens
            }
        }

    def analyze_with_anthropic(
        self,
        subtitle_content: str,
        model: str,
        prompt_type: str,
        temperature: float
    ) -> Dict[str, Any]:
        """Anthropic Claude ëª¨ë¸ë¡œ ë¶„ì„"""
        if not ANTHROPIC_AVAILABLE:
            raise ValueError("anthropic íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install anthropic ì‹¤í–‰í•˜ì„¸ìš”.")
        if not self.anthropic_api_key:
            raise ValueError("ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        system_msg = self.get_system_prompt(prompt_type)
        user_msg = self.get_user_prompt(subtitle_content, prompt_type)

        # ClaudeëŠ” JSON ëª¨ë“œê°€ ì—†ìœ¼ë¯€ë¡œ í”„ë¡¬í”„íŠ¸ì— ê°•ì œ
        user_msg += "\n\në°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”."

        response = self.anthropic_client.messages.create(
            model=model,
            max_tokens=4096,
            temperature=temperature,
            system=system_msg,
            messages=[
                {"role": "user", "content": user_msg}
            ]
        )

        # Claude ì‘ë‹µì—ì„œ JSON ì¶”ì¶œ
        content = response.content[0].text

        # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (```json ... ``` í˜•íƒœì¼ ìˆ˜ ìˆìŒ)
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.split("```")[1].split("```")[0].strip()

        result = json.loads(content)

        return {
            "result": result,
            "usage": {
                "prompt_tokens": response.usage.input_tokens,
                "completion_tokens": response.usage.output_tokens,
                "total_tokens": response.usage.input_tokens + response.usage.output_tokens
            }
        }

    def calculate_cost(self, model: str, usage: Dict[str, int]) -> float:
        """ë¹„ìš© ê³„ì‚°"""
        model_info = self.get_model_info(model)

        input_cost = (usage["prompt_tokens"] / 1000) * model_info["cost_per_1k_input"]
        output_cost = (usage["completion_tokens"] / 1000) * model_info["cost_per_1k_output"]

        return input_cost + output_cost

    def analyze(
        self,
        subtitle_content: str,
        model: str = "gpt-4o-mini",
        prompt_type: str = "shorts",
        temperature: float = 0.7
    ) -> Dict[str, Any]:
        """
        ìë§‰ ë¶„ì„

        Args:
            subtitle_content: ìë§‰ ë‚´ìš©
            model: ëª¨ë¸ëª… (gpt-4o, gpt-4o-mini, sonnet, haiku, opus)
            prompt_type: í”„ë¡¬í”„íŠ¸ íƒ€ì…
            temperature: ì°½ì˜ì„± ìˆ˜ì¤€

        Returns:
            ë¶„ì„ ê²°ê³¼
        """
        model = self.resolve_model_name(model)
        model_info = self.get_model_info(model)
        provider = model_info["provider"]

        print(f"ğŸ¤– ëª¨ë¸: {model}")
        print(f"ğŸ“Š ìë§‰ ê¸¸ì´: {len(subtitle_content):,} ì")

        # ì œê³µì‚¬ë³„ ë¶„ì„
        if provider == "openai":
            response_data = self.analyze_with_openai(subtitle_content, model, prompt_type, temperature)
        elif provider == "anthropic":
            response_data = self.analyze_with_anthropic(subtitle_content, model, prompt_type, temperature)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì œê³µì‚¬: {provider}")

        # ë¹„ìš© ê³„ì‚°
        cost = self.calculate_cost(model, response_data["usage"])

        print(f"âœ… ë¶„ì„ ì™„ë£Œ!")
        print(f"ğŸ’° í† í°: ì…ë ¥={response_data['usage']['prompt_tokens']:,}, "
              f"ì¶œë ¥={response_data['usage']['completion_tokens']:,}, "
              f"ì´={response_data['usage']['total_tokens']:,}")
        print(f"ğŸ’µ ì˜ˆìƒ ë¹„ìš©: ${cost:.4f} (ì•½ {cost * 1400:.0f}ì›)")

        return {
            "result": response_data["result"],
            "model": model,
            "provider": provider,
            "usage": response_data["usage"],
            "cost": cost
        }

    def analyze_file(
        self,
        srt_path: str,
        output_path: str = None,
        model: str = "gpt-4o-mini",
        prompt_type: str = "shorts"
    ) -> Dict[str, Any]:
        """SRT íŒŒì¼ ë¶„ì„"""
        subtitle_content = self.read_srt(srt_path)
        analysis = self.analyze(subtitle_content, model, prompt_type)

        # ê²°ê³¼ ì €ì¥
        if output_path is None:
            srt_file = Path(srt_path)
            output_path = srt_file.parent / f"{srt_file.stem}_analysis_{model.replace('/', '-')}.json"

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")

        return analysis

    def compare_models(
        self,
        srt_path: str,
        models: list = None
    ) -> Dict[str, Dict[str, Any]]:
        """ì—¬ëŸ¬ ëª¨ë¸ë¡œ ë¶„ì„í•˜ì—¬ ë¹„êµ"""
        if models is None:
            models = ["gpt-4o-mini", "sonnet", "haiku"]

        subtitle_content = self.read_srt(srt_path)
        results = {}

        print(f"\nğŸ”„ {len(models)}ê°œ ëª¨ë¸ë¡œ ë¹„êµ ë¶„ì„ ì‹œì‘...\n")

        for model in models:
            print(f"\n{'='*60}")
            print(f"ë¶„ì„ ì¤‘: {model}")
            print(f"{'='*60}")

            try:
                result = self.analyze(subtitle_content, model=model)
                results[model] = result
            except Exception as e:
                print(f"âŒ {model} ë¶„ì„ ì‹¤íŒ¨: {e}")
                results[model] = {"error": str(e)}

        # ë¹„êµ ê²°ê³¼ ì¶œë ¥
        print(f"\n\n{'='*60}")
        print("ğŸ“Š ëª¨ë¸ ë¹„êµ ê²°ê³¼")
        print(f"{'='*60}\n")

        print(f"{'ëª¨ë¸':<25} {'ë¹„ìš©':>10} {'ì…ë ¥í† í°':>12} {'ì¶œë ¥í† í°':>12}")
        print("-" * 65)

        for model, data in results.items():
            if "error" not in data:
                print(f"{model:<25} ${data['cost']:>9.4f} "
                      f"{data['usage']['prompt_tokens']:>12,} "
                      f"{data['usage']['completion_tokens']:>12,}")

        return results


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import sys

    if len(sys.argv) < 2:
        print("""
ì‚¬ìš©ë²•: python subtitle_analyzer_multi.py <SRTíŒŒì¼> [ëª¨ë¸] [í”„ë¡¬í”„íŠ¸íƒ€ì…]

ëª¨ë¸ ì˜µì…˜:
  - OpenAI: gpt-4o, gpt-4o-mini (ê¸°ë³¸)
  - Claude: sonnet, haiku, opus

í”„ë¡¬í”„íŠ¸íƒ€ì…:
  - shorts (ê¸°ë³¸): ì‡¼ì¸  ìµœì í™”
  - basic: ê¸°ë³¸ í¸ì§‘
  - detailed: ìƒì„¸ ë¶„ì„
  - enhanced: ìë§‰/ë‚˜ë ˆì´ì…˜ êµ¬ë¶„
  - enhanced-shorts: 1ë¶„ ì‡¼ì¸  ìµœì í™” â­ NEW
  - enhanced-summary: ê¸´ ì˜ìƒ ìš”ì•½ (3-5ë¶„) â­ NEW
  - enhanced-education: êµìœ¡ ì½˜í…ì¸  ìµœì í™” â­ NEW

ì˜ˆì‹œ:
  python subtitle_analyzer_multi.py video.srt
  python subtitle_analyzer_multi.py video.srt sonnet
  python subtitle_analyzer_multi.py video.srt haiku shorts
  python subtitle_analyzer_multi.py video.srt sonnet enhanced
  python subtitle_analyzer_multi.py video.srt sonnet enhanced-shorts     # 60ì´ˆ ì‡¼ì¸ 
  python subtitle_analyzer_multi.py video.srt sonnet enhanced-summary    # ê¸´ ì˜ìƒ ìš”ì•½
  python subtitle_analyzer_multi.py video.srt sonnet enhanced-education  # êµìœ¡ ì½˜í…ì¸ 

ë¹„êµ ëª¨ë“œ:
  python subtitle_analyzer_multi.py video.srt --compare
""")
        sys.exit(1)

    srt_path = sys.argv[1]

    # ë¹„êµ ëª¨ë“œ
    if len(sys.argv) > 2 and sys.argv[2] == "--compare":
        models = sys.argv[3:] if len(sys.argv) > 3 else ["gpt-4o-mini", "sonnet", "haiku"]
        analyzer = MultiModelSubtitleAnalyzer()
        analyzer.compare_models(srt_path, models)
        return

    # ì¼ë°˜ ëª¨ë“œ
    model = sys.argv[2] if len(sys.argv) > 2 else "gpt-4o-mini"
    prompt_type = sys.argv[3] if len(sys.argv) > 3 else "shorts"

    analyzer = MultiModelSubtitleAnalyzer()
    result = analyzer.analyze_file(srt_path, model=model, prompt_type=prompt_type)

    # ê²°ê³¼ ìš”ì•½
    if "result" in result and "edit_summary" in result["result"]:
        summary = result["result"]["edit_summary"]
        print(f"\nğŸ“Š í¸ì§‘ ìš”ì•½:")
        print(f"  ì›ë³¸: {summary.get('original_count', 0)}ê°œ")
        print(f"  ì‚­ì œ: {summary.get('delete_count', 0)}ê°œ")
        print(f"  ì¶”ê°€: {summary.get('add_count', 0)}ê°œ")
        print(f"  ìµœì¢…: {summary.get('final_count', 0)}ê°œ")


if __name__ == "__main__":
    main()
